{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "affdbdd6-90a4-4baf-afd6-9b35437767c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Multi-Product Cross-Sell - Explainability & Advisor Outputs\n",
    "\n",
    "## Overview\n",
    "Transform model predictions into actionable, explainable recommendations that advisors can use in client conversations.\n",
    "\n",
    "## Approach\n",
    "1. **Load Scored Clients**: Get high-scoring clients from Notebook 04\n",
    "2. **SHAP Analysis**: Generate feature importance explanations\n",
    "3. **Natural Language Generation**: Convert SHAP values to talking points\n",
    "4. **Create Smart Lists**: Generate advisor-ready prospect lists\n",
    "5. **Multi-Product Strategy**: Identify optimal multi-product approaches\n",
    "\n",
    "## Key Benefits\n",
    "- **WHY**: Explain why each client got their score\n",
    "- **HOW**: Provide talking points for advisors\n",
    "- **WHO**: Prioritize based on score + readiness + wealth\n",
    "- **WHAT**: Recommend optimal product strategy (single vs multi)\n",
    "\n",
    "## Output\n",
    "- `multi_product_explanations`: SHAP-based feature importance per client\n",
    "- `advisor_smart_lists`: Top prospects with talking points\n",
    "- `multi_product_strategy`: Bundle recommendations and pitching order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7288b8bf-1ac3-49c4-912d-81f1154645a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Schema: eda_smartlist.us_wealth_management_smartlist\nTraining Month: 202510\nPrediction Month: 202510\nMin Score Threshold: 0.6\nTop N per Product: 500\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "dbutils.widgets.text(\"target_schema\", \"eda_smartlist.us_wealth_management_smartlist\")\n",
    "dbutils.widgets.text(\"training_month\", \"202510\")\n",
    "dbutils.widgets.text(\"prediction_month\", \"202510\")\n",
    "dbutils.widgets.text(\"min_score_threshold\", \"0.60\")\n",
    "dbutils.widgets.text(\"top_n_per_product\", \"500\")\n",
    "\n",
    "target_schema = dbutils.widgets.get(\"target_schema\")\n",
    "training_month = dbutils.widgets.get(\"training_month\")\n",
    "prediction_month = dbutils.widgets.get(\"prediction_month\")\n",
    "min_score_threshold = float(dbutils.widgets.get(\"min_score_threshold\"))\n",
    "top_n_per_product = int(dbutils.widgets.get(\"top_n_per_product\"))\n",
    "\n",
    "print(f\"Target Schema: {target_schema}\")\n",
    "print(f\"Training Month: {training_month}\")\n",
    "print(f\"Prediction Month: {prediction_month}\")\n",
    "print(f\"Min Score Threshold: {min_score_threshold}\")\n",
    "print(f\"Top N per Product: {top_n_per_product}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28e196fb-3790-4a10-b342-2c7119dfc7aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries and custom modules imported\nConfiguration initialized for schema: eda_smartlist.us_wealth_management_smartlist\nWealth Management Source Schema: dl_tenants_daas.us_wealth_management\nBranch Code Filter: 83\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import shap\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lit, when, concat_ws, min as spark_min, first\n",
    "from pyspark.sql.types import StringType\n",
    "import sys\n",
    "\n",
    "# Import our custom modules for robust architecture\n",
    "sys.path.append('/Workspace/Users/juan.hernandez@equitable.com/multi_product_model')\n",
    "from config import create_default_config\n",
    "\n",
    "print(\"Libraries and custom modules imported\")\n",
    "\n",
    "# Initialize configuration\n",
    "config = create_default_config(target_schema)\n",
    "print(f\"Configuration initialized for schema: {target_schema}\")\n",
    "\n",
    "# Add widget for branch filtering\n",
    "dbutils.widgets.text(\"wm_source_schema\", \"dl_tenants_daas.us_wealth_management\")\n",
    "dbutils.widgets.text(\"branch_code\", \"83\")\n",
    "wm_source_schema = dbutils.widgets.get(\"wm_source_schema\")\n",
    "branch_code = dbutils.widgets.get(\"branch_code\")\n",
    "print(f\"Wealth Management Source Schema: {wm_source_schema}\")\n",
    "print(f\"Branch Code Filter: {branch_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f9abfc-e868-4a79-ad99-ef0a489f2db6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Filter by Branch and Join with Training Data\n",
    "\n",
    "Filter clients by branch 83, join with training data to get current product and demographics, \n",
    "and join with base cross-sale data to get the first product category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1605ffc9-b477-4dbe-8a2e-a18d671736b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nSTEP 1: FILTER BY BRANCH AND ENRICH WITH PRODUCT/DEMOGRAPHIC DATA\n================================================================================\n\nFiltering clients from branch 83...\nGetting first product category for each client...\n  Found first product for 521,005 clients\n\nFiltering scores by branch 83 and joining with training data...\n  Enhanced scores: 4,454 clients from branch 83\n\nSample of enhanced scores:\n+--------------------+------------+------------------+------------------------+----------------------+----------+------------+-------------+\n|axa_party_id        |best_product|best_score        |current_product_category|first_product_category|client_age|acct_val_amt|channel      |\n+--------------------+------------+------------------+------------------------+----------------------+----------+------------+-------------+\n|74BK05RY51AYQXKUXXXX|investment  |0.5682963644985044|INVESTMENT              |INVESTMENT            |59.0      |75887.61    |Retail       |\n|95BK08DDBMZMBGJTXXXX|investment  |0.6559546840539312|INVESTMENT              |INVESTMENT            |59.0      |47822.2     |Retail       |\n|69BK05RY3MCG2ZS7XXXX|investment  |0.5781633682896086|RETIREMENT              |RETIREMENT            |68.0      |108166.67   |Branch Assist|\n|20168ac72304920216a4|investment  |0.9207757856690497|INVESTMENT              |INVESTMENT            |59.0      |34698.12    |Retail       |\n|76BK0AYYX3AIUNX5XXXX|investment  |0.7650064269697655|INVESTMENT              |INVESTMENT            |59.0      |458796.22   |Retail       |\n+--------------------+------------+------------------+------------------------+----------------------+----------+------------+-------------+\nonly showing top 5 rows\n\n✅ Step 1 Complete: Enhanced scores with branch filter and product/demographic data\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter by Branch 83 and Join with Training Data\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: FILTER BY BRANCH AND ENRICH WITH PRODUCT/DEMOGRAPHIC DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nFiltering clients from branch {branch_code}...\")\n",
    "\n",
    "# First, get the first product category for each client from base cross-sale data\n",
    "print(\"Getting first product category for each client...\")\n",
    "first_product_df = spark.sql(f\"\"\"\n",
    "    WITH ranked_products AS (\n",
    "        SELECT \n",
    "            axa_party_id,\n",
    "            product_category,\n",
    "            register_date,\n",
    "            ROW_NUMBER() OVER (PARTITION BY axa_party_id ORDER BY register_date ASC) AS rnk\n",
    "        FROM {target_schema}.multi_product_cross_sale_base\n",
    "        WHERE product_category IS NOT NULL\n",
    "          AND product_category != 'OTHER'\n",
    "    )\n",
    "    SELECT \n",
    "        axa_party_id,\n",
    "        product_category AS first_product_category,\n",
    "        register_date AS first_product_register_date\n",
    "    FROM ranked_products\n",
    "    WHERE rnk = 1\n",
    "\"\"\")\n",
    "\n",
    "# Register as temporary view for SQL join\n",
    "first_product_df.createOrReplaceTempView(\"first_product_temp\")\n",
    "\n",
    "first_product_count = first_product_df.count()\n",
    "print(f\"  Found first product for {first_product_count:,} clients\")\n",
    "\n",
    "# Now filter scores by branch and join with training data and first product\n",
    "print(f\"\\nFiltering scores by branch {branch_code} and joining with training data...\")\n",
    "enhanced_scores = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        csc.*,\n",
    "        \n",
    "        -- Current product and demographics from training data\n",
    "        train.product_category AS current_product_category,\n",
    "        train.client_age,\n",
    "        train.acct_val_amt,\n",
    "        train.channel,\n",
    "        train.client_seg,\n",
    "        train.aum_band,\n",
    "        train.wc_total_assets,\n",
    "        train.wc_assetmix_stocks,\n",
    "        train.wc_assetmix_bonds,\n",
    "        train.client_tenure_years,\n",
    "        train.retirement_planning_trigger,\n",
    "        train.family_protection_trigger,\n",
    "        train.wealth_building_trigger,\n",
    "        \n",
    "        -- First product category\n",
    "        first.first_product_category,\n",
    "        first.first_product_register_date\n",
    "        \n",
    "    FROM {target_schema}.multi_product_client_scores AS csc\n",
    "    \n",
    "    -- Filter by branch 83\n",
    "    INNER JOIN (\n",
    "        SELECT DISTINCT axa_party_id\n",
    "        FROM {wm_source_schema}.wealth_management_client_metrics\n",
    "        WHERE branchoffice_code = '{branch_code}'\n",
    "    ) AS cm\n",
    "    ON csc.axa_party_id = cm.axa_party_id\n",
    "    \n",
    "    -- Join with training data for current product and demographics\n",
    "    LEFT JOIN (\n",
    "        SELECT DISTINCT\n",
    "            axa_party_id,\n",
    "            product_category,\n",
    "            client_age,\n",
    "            acct_val_amt,\n",
    "            channel,\n",
    "            client_seg,\n",
    "            aum_band,\n",
    "            wc_total_assets,\n",
    "            wc_assetmix_stocks,\n",
    "            wc_assetmix_bonds,\n",
    "            client_tenure_years,\n",
    "            retirement_planning_trigger,\n",
    "            family_protection_trigger,\n",
    "            wealth_building_trigger\n",
    "        FROM {target_schema}.multi_product_training_data\n",
    "    ) AS train\n",
    "    ON csc.axa_party_id = train.axa_party_id\n",
    "    \n",
    "    -- Join with first product data (using temporary view)\n",
    "    LEFT JOIN first_product_temp AS first\n",
    "    ON csc.axa_party_id = first.axa_party_id\n",
    "    \n",
    "    WHERE csc.prediction_month = '{prediction_month}'\n",
    "\"\"\")\n",
    "\n",
    "enhanced_count = enhanced_scores.count()\n",
    "print(f\"  Enhanced scores: {enhanced_count:,} clients from branch {branch_code}\")\n",
    "\n",
    "# Show sample of enhanced data\n",
    "print(\"\\nSample of enhanced scores:\")\n",
    "enhanced_scores.select(\n",
    "    'axa_party_id',\n",
    "    'best_product',\n",
    "    'best_score',\n",
    "    'current_product_category',\n",
    "    'first_product_category',\n",
    "    'csc.client_age',\n",
    "    'csc.acct_val_amt',\n",
    "    'csc.channel'\n",
    ").show(5, truncate=False)\n",
    "\n",
    "print(\"\\n✅ Step 1 Complete: Enhanced scores with branch filter and product/demographic data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "769393e0-8b97-4616-ae95-5bbc92833792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Save Enhanced Scores Table\n",
    "\n",
    "Save the enhanced scores table with all product and demographic information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e227525-6a43-4ab0-82ec-7ca8e12d19cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nSTEP 2: SAVING ENHANCED SCORES TABLE\n================================================================================\nWriting enhanced scores to eda_smartlist.us_wealth_management_smartlist.multi_product_client_scores_enhanced...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6604191847310184>, line 15\u001B[0m\n",
       "\u001B[1;32m     10\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDROP TABLE IF EXISTS \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menhanced_scores_table\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWriting enhanced scores to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menhanced_scores_table\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     13\u001B[0m enhanced_scores\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     14\u001B[0m     \u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction_month\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbest_product\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[0;32m---> 15\u001B[0m     \u001B[38;5;241m.\u001B[39msaveAsTable(enhanced_scores_table)\n",
       "\u001B[1;32m     17\u001B[0m final_count \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSELECT COUNT(*) as cnt FROM \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menhanced_scores_table\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcollect()[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcnt\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
       "\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ Saved \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfinal_count\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m enhanced client scores to: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menhanced_scores_table\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1857\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1855\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   1856\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n",
       "\u001B[0;32m-> 1857\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msaveAsTable(name)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:275\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    271\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    273\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    274\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 275\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    277\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [COLUMN_ALREADY_EXISTS] The column `acct_val_amt` already exists. Choose another name or rename the existing column. SQLSTATE: 42711"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[COLUMN_ALREADY_EXISTS] The column `acct_val_amt` already exists. Choose another name or rename the existing column. SQLSTATE: 42711"
       },
       "metadata": {
        "errorSummary": "[COLUMN_ALREADY_EXISTS] The column `acct_val_amt` already exists. Choose another name or rename the existing column. SQLSTATE: 42711"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "COLUMN_ALREADY_EXISTS",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": "42711",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-6604191847310184>, line 15\u001B[0m\n\u001B[1;32m     10\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDROP TABLE IF EXISTS \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menhanced_scores_table\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWriting enhanced scores to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menhanced_scores_table\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     13\u001B[0m enhanced_scores\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprediction_month\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbest_product\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m---> 15\u001B[0m     \u001B[38;5;241m.\u001B[39msaveAsTable(enhanced_scores_table)\n\u001B[1;32m     17\u001B[0m final_count \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSELECT COUNT(*) as cnt FROM \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menhanced_scores_table\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcollect()[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcnt\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ Saved \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfinal_count\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m enhanced client scores to: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menhanced_scores_table\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1857\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1855\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1856\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[0;32m-> 1857\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msaveAsTable(name)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:275\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    271\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    273\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    274\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 275\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [COLUMN_ALREADY_EXISTS] The column `acct_val_amt` already exists. Choose another name or rename the existing column. SQLSTATE: 42711"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Save Enhanced Scores Table\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 2: SAVING ENHANCED SCORES TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "## here is the table we need\n",
    "enhanced_scores_table = f\"{target_schema}.multi_product_client_scores_enhanced\"\n",
    "\n",
    "# Drop table if exists to prevent schema conflicts\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {enhanced_scores_table}\")\n",
    "\n",
    "print(f\"Writing enhanced scores to {enhanced_scores_table}...\")\n",
    "enhanced_scores.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .partitionBy(\"prediction_month\", \"best_product\") \\\n",
    "    .saveAsTable(enhanced_scores_table)\n",
    "\n",
    "final_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {enhanced_scores_table}\").collect()[0]['cnt']\n",
    "print(f\"✅ Saved {final_count:,} enhanced client scores to: {enhanced_scores_table}\")\n",
    "print(f\"   Partitioned by: prediction_month, best_product\")\n",
    "print(f\"   Includes: scores + current product + demographics + first product category\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afc805f7-6820-4870-b5f3-db0a99ec76e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Original Feature Sets from Model Metadata\n",
    "\n",
    "Load the original feature sets used during training for mapping transformed features back to original features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35710e9d-32b3-4b9b-9f13-cc661ec90fec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "branch_code": "83",
        "min_score_threshold": "0.60",
        "prediction_month": "202510",
        "target_schema": "eda_smartlist.us_wealth_management_smartlist",
        "top_n_per_product": "500",
        "training_month": "202510",
        "wm_source_schema": "dl_tenants_daas.us_wealth_management"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Original Feature Sets from Model Metadata\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING FEATURE SETS FROM MODEL METADATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "metadata_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        product,\n",
    "        features,\n",
    "        feature_count\n",
    "    FROM {target_schema}.multi_product_model_metadata\n",
    "    WHERE business_month = '{training_month}'\n",
    "\"\"\").toPandas()\n",
    "\n",
    "if len(metadata_df) == 0:\n",
    "    raise ValueError(f\"No metadata found for training_month={training_month}. Please run Notebook 03 first.\")\n",
    "\n",
    "PRODUCT_FEATURE_SETS = {}\n",
    "for _, row in metadata_df.iterrows():\n",
    "    product = row['product']\n",
    "    feature_list = row['features'].split(',')\n",
    "    PRODUCT_FEATURE_SETS[product] = feature_list\n",
    "    print(f\"  {product}: {row['feature_count']} features\")\n",
    "\n",
    "# Use config.target_products for consistency with Notebooks 03 and 04\n",
    "target_products = config.target_products\n",
    "\n",
    "print(f\"\\nLoaded original feature sets for {len(PRODUCT_FEATURE_SETS)} products\")\n",
    "print(f\"Using target products from config: {target_products}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "089e2a4c-528f-423f-804e-8dd8576d785f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "branch_code": "83",
        "min_score_threshold": "0.60",
        "prediction_month": "202510",
        "target_schema": "eda_smartlist.us_wealth_management_smartlist",
        "top_n_per_product": "500",
        "training_month": "202510",
        "wm_source_schema": "dl_tenants_daas.us_wealth_management"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Models from Unity Catalog\n",
    "print(\"Loading models from Unity Catalog...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "models = {}\n",
    "\n",
    "for product in target_products:\n",
    "    model_uri = f\"models:/eda_smartlist.models.{product}_{training_month}/1\"\n",
    "    print(f\"Loading {product}...\")\n",
    "    try:\n",
    "        models[product] = mlflow.sklearn.load_model(model_uri)\n",
    "        print(f\"  Loaded from {model_uri}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        raise\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(models)} models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acfad58f-1de6-4c21-99c6-b3dc5d706616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Feature Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f11d15a-ff45-416d-9104-2964f271f1dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "branch_code": "83",
        "min_score_threshold": "0.60",
        "prediction_month": "202510",
        "target_schema": "eda_smartlist.us_wealth_management_smartlist",
        "top_n_per_product": "500",
        "training_month": "202510",
        "wm_source_schema": "dl_tenants_daas.us_wealth_management"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load feature metadata from training\n",
    "print(\"Loading feature metadata from training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "feature_metadata = {}\n",
    "\n",
    "try:\n",
    "    metadata_df = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            product,\n",
    "            transformed_features,\n",
    "            transformed_feature_count,\n",
    "            feature_count\n",
    "        FROM {target_schema}.multi_product_model_metadata\n",
    "        WHERE business_month = '{training_month}'\n",
    "    \"\"\").toPandas()\n",
    "    \n",
    "    if len(metadata_df) == 0:\n",
    "        print(f\"WARNING: No metadata found for training_month={training_month}\")\n",
    "        print(\"   Will use fallback feature extraction\")\n",
    "    else:\n",
    "        # Create lookup dictionary\n",
    "        for _, row in metadata_df.iterrows():\n",
    "            product = row['product']\n",
    "            # Check if transformed_features exists and is not None\n",
    "            if pd.notna(row.get('transformed_features')) and row.get('transformed_features'):\n",
    "                try:\n",
    "                    feature_metadata[product] = {\n",
    "                        'transformed_features': row['transformed_features'].split(','),\n",
    "                        'count': int(row['transformed_feature_count']) if pd.notna(row.get('transformed_feature_count')) else None\n",
    "                    }\n",
    "                    print(f\"  {product}: {feature_metadata[product]['count']} transformed features\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  WARNING: {product}: Error parsing metadata - {str(e)[:60]}...\")\n",
    "                    print(f\"    Will use fallback for this product\")\n",
    "            else:\n",
    "                print(f\"  WARNING: {product}: No transformed_features in metadata (will use fallback)\")\n",
    "        \n",
    "        print(f\"\\nLoaded metadata for {len(feature_metadata)} products\")\n",
    "        if len(feature_metadata) < len(metadata_df):\n",
    "            print(f\"  Note: {len(metadata_df) - len(feature_metadata)} products will use fallback extraction\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"WARNING: Could not load feature metadata: {str(e)}\")\n",
    "    print(\"   Will use fallback feature extraction for all products\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46cfb5d5-ed15-4a23-a272-9dc1d7045628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Load High-Scoring Clients from Enhanced Table\n",
    "\n",
    "Load high-scoring clients from the enhanced scores table for SHAP analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6b9662-48a0-45f8-8341-ce4a6e1c43ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "branch_code": "83",
        "min_score_threshold": "0.60",
        "prediction_month": "202510",
        "target_schema": "eda_smartlist.us_wealth_management_smartlist",
        "top_n_per_product": "500",
        "training_month": "202510",
        "wm_source_schema": "dl_tenants_daas.us_wealth_management"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load scored clients from Enhanced Scores Table\n",
    "print(f\"Loading high-scoring clients from enhanced table (score > {min_score_threshold})...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "scored_clients = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {target_schema}.multi_product_client_scores_enhanced\n",
    "    WHERE prediction_month = '{prediction_month}'\n",
    "      AND best_score > {min_score_threshold}\n",
    "\"\"\")\n",
    "\n",
    "client_count = scored_clients.count()\n",
    "print(f\"Loaded {client_count:,} high-scoring clients\")\n",
    "print(f\"   (best_score > {min_score_threshold})\")\n",
    "\n",
    "# Convert to Pandas for SHAP analysis with enhanced column handling (like Notebook 04)\n",
    "print(\"Converting Spark DataFrame to Pandas...\")\n",
    "df_clients = scored_clients.toPandas()\n",
    "print(f\"Initial conversion: {df_clients.shape}\")\n",
    "\n",
    "# CRITICAL: Comprehensive column name and data type fixing (matching Notebook 04)\n",
    "print(\"Fixing column names and data types...\")\n",
    "\n",
    "# 1. Ensure all column names are strings\n",
    "original_columns = df_clients.columns.tolist()\n",
    "string_columns = [str(col).strip() for col in original_columns]  # Also strip whitespace\n",
    "df_clients.columns = string_columns\n",
    "\n",
    "# 2. Check for any problematic column names (spaces, special chars, etc.)\n",
    "problematic_cols = [col for col in df_clients.columns if not col.replace('_', '').replace('.', '').isalnum()]\n",
    "if problematic_cols:\n",
    "    print(f\"Found problematic column names: {problematic_cols[:5]}...\")\n",
    "    # Clean up column names\n",
    "    clean_columns = []\n",
    "    for col in df_clients.columns:\n",
    "        clean_col = ''.join(c if c.isalnum() or c == '_' else '_' for c in str(col))\n",
    "        clean_columns.append(clean_col)\n",
    "    df_clients.columns = clean_columns\n",
    "\n",
    "# 3. Reset index to ensure clean structure\n",
    "df_clients = df_clients.reset_index(drop=True)\n",
    "\n",
    "# 4. Check data types and fix any issues\n",
    "print(\"Checking data types...\")\n",
    "converted_count = 0\n",
    "for col in df_clients.columns:\n",
    "    dtype = df_clients[col].dtype\n",
    "    if dtype == 'object':\n",
    "        # Try to convert object columns to numeric if they should be numeric\n",
    "        try:\n",
    "            numeric_series = pd.to_numeric(df_clients[col], errors='coerce')\n",
    "            if not numeric_series.isna().all():  # If conversion worked for some values\n",
    "                df_clients[col] = numeric_series\n",
    "                converted_count += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if converted_count > 0:\n",
    "    print(f\"  Converted {converted_count} object columns to numeric\")\n",
    "\n",
    "print(f\"Final DataFrame: {df_clients.shape}\")\n",
    "print(f\"Column types: {df_clients.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"Sample columns: {list(df_clients.columns[:10])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd7e3d5c-44e1-4e0d-834e-73a01a8814bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## SHAP Analysis - Generate Explanations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7818f05-dc80-43dd-a2f4-00e28fda7623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "branch_code": "83",
        "min_score_threshold": "0.60",
        "prediction_month": "202510",
        "target_schema": "eda_smartlist.us_wealth_management_smartlist",
        "top_n_per_product": "500",
        "training_month": "202510",
        "wm_source_schema": "dl_tenants_daas.us_wealth_management"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate SHAP explanations for each product\n",
    "print(\"Generating SHAP explanations...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "shap_results = {}\n",
    "\n",
    "for product in target_products:\n",
    "    print(f\"\\nAnalyzing {product}...\")\n",
    "    \n",
    "    product_short = product.replace('_cross_sell', '')\n",
    "    score_col = f'{product_short}_score'\n",
    "    \n",
    "    # Get clients with good scores for this product\n",
    "    product_clients = df_clients[df_clients[score_col] > min_score_threshold].copy()\n",
    "    \n",
    "    if len(product_clients) == 0:\n",
    "        print(f\"  WARNING: No clients above threshold for {product}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Clients to explain: {len(product_clients):,}\")\n",
    "    \n",
    "    # Get features (ensure they're strings, matching Notebook 04)\n",
    "    features = [str(f).strip() for f in PRODUCT_FEATURE_SETS[product]]\n",
    "    \n",
    "    # Check for missing features and clean feature names (matching Notebook 04 approach)\n",
    "    available_features = []\n",
    "    missing_features = []\n",
    "    \n",
    "    for feature in features:\n",
    "        # Try exact match first\n",
    "        if feature in product_clients.columns:\n",
    "            available_features.append(feature)\n",
    "        else:\n",
    "            # Try to find similar column names (in case of minor differences)\n",
    "            similar_cols = [col for col in product_clients.columns if col.lower().replace('_', '') == feature.lower().replace('_', '')]\n",
    "            if similar_cols:\n",
    "                available_features.append(similar_cols[0])\n",
    "                print(f\"  Mapped {feature} -> {similar_cols[0]}\")\n",
    "            else:\n",
    "                missing_features.append(feature)\n",
    "    \n",
    "    if missing_features:\n",
    "        print(f\"  WARNING: Missing features: {missing_features[:5]}...\")\n",
    "    \n",
    "    if not available_features:\n",
    "        print(f\"  ERROR: No features available for {product}\")\n",
    "        continue\n",
    "    \n",
    "    features = available_features\n",
    "    \n",
    "    # Prepare data\n",
    "    X = product_clients[features].copy()\n",
    "    \n",
    "    # Ensure feature DataFrame has clean structure (matching Notebook 04)\n",
    "    X.columns = [str(col).strip() for col in X.columns]\n",
    "    X = X.reset_index(drop=True)\n",
    "    \n",
    "    # Handle NaN values\n",
    "    feature_nans = X.isna().sum().sum()\n",
    "    if feature_nans > 0:\n",
    "        print(f\"  Found {feature_nans} NaN values, filling with 0\")\n",
    "        X = X.fillna(0)\n",
    "    \n",
    "    # Create SHAP explainer with robust transformation (matching Notebook 04 approach)\n",
    "    print(f\"  Creating SHAP explainer...\")\n",
    "    try:\n",
    "        model_pipeline = models[product]\n",
    "        \n",
    "        # Extract preprocessing pipeline (everything except the final classifier)\n",
    "        preprocessing_pipeline = model_pipeline[:-1]\n",
    "        \n",
    "        # Transform data through the preprocessing pipeline with fallback strategies\n",
    "        print(f\"  Transforming through preprocessing pipeline...\")\n",
    "        X_transformed = None\n",
    "        transformation_success = False\n",
    "        \n",
    "        # Approach 1: Direct transformation (simplest, try first)\n",
    "        try:\n",
    "            print(f\"    Trying direct transformation...\")\n",
    "            X_transformed = preprocessing_pipeline.transform(X)\n",
    "            print(f\"    SUCCESS: Direct transformation worked\")\n",
    "            transformation_success = True\n",
    "        except Exception as e1:\n",
    "            print(f\"    Direct transformation failed: {str(e1)[:100]}...\")\n",
    "            \n",
    "            # Approach 2: Convert to numpy array\n",
    "            try:\n",
    "                print(f\"    Trying numpy array transformation...\")\n",
    "                X_transformed = preprocessing_pipeline.transform(X.values)\n",
    "                print(f\"    SUCCESS: Numpy transformation worked\")\n",
    "                transformation_success = True\n",
    "            except Exception as e2:\n",
    "                print(f\"    Numpy transformation failed: {str(e2)[:100]}...\")\n",
    "                \n",
    "                # Approach 3: Try with DataFrame column names as list\n",
    "                try:\n",
    "                    print(f\"    Trying with explicit column list...\")\n",
    "                    X_transformed = preprocessing_pipeline.transform(X[features].values)\n",
    "                    print(f\"    SUCCESS: Column list transformation worked\")\n",
    "                    transformation_success = True\n",
    "                except Exception as e3:\n",
    "                    print(f\"    Column list transformation failed: {str(e3)[:100]}...\")\n",
    "        \n",
    "        if not transformation_success or X_transformed is None:\n",
    "            raise Exception(\"All transformation methods failed\")\n",
    "        \n",
    "        # Convert to dense if sparse\n",
    "        if hasattr(X_transformed, 'toarray'):\n",
    "            X_transformed = X_transformed.toarray()\n",
    "        \n",
    "        # FEATURE NAME EXTRACTION - Use metadata first, then fallback\n",
    "        print(f\"  Getting feature names...\")\n",
    "        feature_names = None\n",
    "        \n",
    "        # PRIORITY 1: Use saved metadata from training (most reliable!)\n",
    "        if product in feature_metadata:\n",
    "            feature_names = feature_metadata[product]['transformed_features']\n",
    "            expected_count = feature_metadata[product]['count']\n",
    "            \n",
    "            if len(feature_names) == X_transformed.shape[1]:\n",
    "                print(f\"  METADATA: Perfect match - {len(feature_names)} features\")\n",
    "            else:\n",
    "                print(f\"  WARNING: METADATA: Count mismatch ({len(feature_names)} vs {X_transformed.shape[1]})\")\n",
    "                # Still use metadata names, they're the ground truth\n",
    "        \n",
    "        # PRIORITY 2: Try extraction if no metadata\n",
    "        if not feature_names:\n",
    "            print(f\"  No metadata found, attempting extraction...\")\n",
    "            try:\n",
    "                feature_names = list(preprocessing_pipeline.get_feature_names_out(features))\n",
    "                print(f\"  EXTRACTION: Got {len(feature_names)} features\")\n",
    "            except Exception as e:\n",
    "                print(f\"  WARNING: Extraction failed: {str(e)[:60]}...\")\n",
    "        \n",
    "        # PRIORITY 3: Manual construction as last resort\n",
    "        if not feature_names or len(feature_names) != X_transformed.shape[1]:\n",
    "            if feature_names:\n",
    "                print(f\"  Feature count mismatch: {len(feature_names)} vs {X_transformed.shape[1]}\")\n",
    "            \n",
    "            print(f\"  FALLBACK: Constructing feature names manually...\")\n",
    "            feature_names = []\n",
    "            categorical_features = ['aum_segment', 'channel', 'product_category']\n",
    "            \n",
    "            for feat in features:\n",
    "                if feat in categorical_features:\n",
    "                    unique_vals = sorted(X[feat].dropna().unique().astype(str))\n",
    "                    for val in unique_vals:\n",
    "                        feature_names.append(f\"{feat}_{val}\")\n",
    "                    print(f\"    {feat}: {len(unique_vals)} categories\")\n",
    "                else:\n",
    "                    feature_names.append(feat)\n",
    "            \n",
    "            # Pad if needed with descriptive names\n",
    "            if len(feature_names) < X_transformed.shape[1]:\n",
    "                extra = X_transformed.shape[1] - len(feature_names)\n",
    "                print(f\"    Padding with {extra} encoded features (from training data)\")\n",
    "                for j in range(extra):\n",
    "                    feature_names.append(f\"encoded_feature_{len(feature_names) + 1}\")\n",
    "            elif len(feature_names) > X_transformed.shape[1]:\n",
    "                feature_names = feature_names[:X_transformed.shape[1]]\n",
    "        \n",
    "        print(f\"  Final: {len(feature_names)} features\")\n",
    "        print(f\"  Sample: {feature_names[:5]}\")\n",
    "        \n",
    "        # Create DataFrame for SHAP\n",
    "        X_for_shap = pd.DataFrame(X_transformed, columns=feature_names)\n",
    "        \n",
    "        # Extract the base model\n",
    "        final_step = model_pipeline[-1]\n",
    "        model_to_explain = final_step.classifier if hasattr(final_step, 'classifier') else final_step\n",
    "        \n",
    "        # Create SHAP explainer\n",
    "        explainer = shap.TreeExplainer(model_to_explain)\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        max_explain = min(1000, len(X_for_shap))\n",
    "        print(f\"  Calculating SHAP values for {max_explain} clients...\")\n",
    "        shap_values = explainer.shap_values(X_for_shap.iloc[:max_explain])\n",
    "        \n",
    "        # Handle binary classification\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1]\n",
    "        \n",
    "        # Store results\n",
    "        shap_results[product] = {\n",
    "            'shap_values': shap_values,\n",
    "            'transformed_features': feature_names,\n",
    "            'original_features': features,\n",
    "            'X_transformed': X_for_shap.iloc[:max_explain],\n",
    "            'X_original': X.iloc[:max_explain],\n",
    "            'client_ids': product_clients.iloc[:max_explain]['axa_party_id'].values,\n",
    "            'scores': product_clients.iloc[:max_explain][score_col].values\n",
    "        }\n",
    "        \n",
    "        print(f\"  SHAP complete: {shap_values.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(f\"\\nSHAP analysis complete for {len(shap_results)} products\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "539e2f38-ff2b-4478-bb67-5bc86f998b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a03c629-e2c2-4d35-a51d-8eaca4d1de50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "branch_code": "83",
        "min_score_threshold": "0.60",
        "prediction_month": "202510",
        "target_schema": "eda_smartlist.us_wealth_management_smartlist",
        "top_n_per_product": "500",
        "training_month": "202510",
        "wm_source_schema": "dl_tenants_daas.us_wealth_management"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate feature importance for each client\n",
    "print(\"Generating feature importance rankings...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def map_feature_to_original(feature_name, original_features):\n",
    "    \"\"\"Maps one-hot encoded features back to original\"\"\"\n",
    "    for orig_feat in original_features:\n",
    "        if feature_name.startswith(f\"{orig_feat}_\"):\n",
    "            category_value = feature_name.replace(f\"{orig_feat}_\", \"\")\n",
    "            return orig_feat, category_value\n",
    "    return feature_name, None\n",
    "\n",
    "def get_original_value_safe(feature_name, category_value, client_idx, X_original, X_transformed):\n",
    "    \"\"\"Get value - ALL values returned as strings for consistent Spark schema\"\"\"\n",
    "    # If category value from one-hot encoding\n",
    "    if category_value:\n",
    "        return str(category_value)\n",
    "    \n",
    "    # Try original data first\n",
    "    for orig_feat in X_original.columns:\n",
    "        if feature_name == orig_feat or feature_name.startswith(f\"{orig_feat}_\"):\n",
    "            val = X_original.iloc[client_idx][orig_feat]\n",
    "            \n",
    "            # Convert EVERYTHING to string for Spark compatibility\n",
    "            if pd.isna(val):\n",
    "                return \"N/A\"\n",
    "            else:\n",
    "                # Convert numeric types to string\n",
    "                if isinstance(val, (np.integer, np.int32, np.int64)):\n",
    "                    return str(int(val))\n",
    "                elif isinstance(val, (np.floating, np.float32, np.float64)):\n",
    "                    return str(float(val))\n",
    "                else:\n",
    "                    return str(val)\n",
    "    \n",
    "    # Fallback to transformed\n",
    "    if feature_name in X_transformed.columns:\n",
    "        val = X_transformed.iloc[client_idx][feature_name]\n",
    "        if pd.isna(val):\n",
    "            return \"N/A\"\n",
    "        else:\n",
    "            if isinstance(val, (np.integer, np.int32, np.int64)):\n",
    "                return str(int(val))\n",
    "            elif isinstance(val, (np.floating, np.float32, np.float64)):\n",
    "                return str(float(val))\n",
    "            else:\n",
    "                return str(val)\n",
    "    \n",
    "    return \"N/A\"\n",
    "\n",
    "explanation_records = []\n",
    "\n",
    "for product, result in shap_results.items():\n",
    "    print(f\"\\nProcessing {product}...\")\n",
    "    \n",
    "    shap_vals = result['shap_values']\n",
    "    transformed_features = result['transformed_features']\n",
    "    original_features = result['original_features']\n",
    "    X_transformed = result['X_transformed']\n",
    "    X_original = result['X_original']\n",
    "    client_ids = result['client_ids']\n",
    "    scores = result['scores']\n",
    "    \n",
    "    product_short = product.replace('_cross_sell', '')\n",
    "    \n",
    "    for i, (client_id, score) in enumerate(zip(client_ids, scores)):\n",
    "        client_shap = shap_vals[i]\n",
    "        abs_shap = np.abs(client_shap)\n",
    "        top_5_idx = np.argsort(abs_shap)[-5:][::-1]\n",
    "        \n",
    "        record = {\n",
    "            'axa_party_id': client_id,\n",
    "            'product': product_short,\n",
    "            'score': float(score),\n",
    "            'prediction_month': prediction_month,\n",
    "            'training_month': training_month,\n",
    "            'explanation_date': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Add top 5 features with type-safe values\n",
    "        for rank, idx in enumerate(top_5_idx, 1):\n",
    "            transformed_feat = transformed_features[idx]\n",
    "            original_feat, category_value = map_feature_to_original(transformed_feat, original_features)\n",
    "            value = get_original_value_safe(transformed_feat, category_value, i, X_original, X_transformed)\n",
    "            \n",
    "            record[f'top_feature_{rank}'] = original_feat\n",
    "            record[f'top_feature_{rank}_shap'] = float(client_shap[idx])\n",
    "            record[f'top_feature_{rank}_value'] = value\n",
    "        \n",
    "        explanation_records.append(record)\n",
    "    \n",
    "    print(f\"  Generated {len(client_ids):,} explanations\")\n",
    "\n",
    "print(f\"\\nTotal explanations: {len(explanation_records):,}\")\n",
    "df_explanations = pd.DataFrame(explanation_records)\n",
    "print(f\"DataFrame shape: {df_explanations.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8486a77-e28c-4cf4-9321-b0438dbd47a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate Natural Language Talking Points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48d12f60-59e8-4ea5-a286-e2b8c96885f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "branch_code": "83",
        "min_score_threshold": "0.60",
        "prediction_month": "202510",
        "target_schema": "eda_smartlist.us_wealth_management_smartlist",
        "top_n_per_product": "500",
        "training_month": "202510",
        "wm_source_schema": "dl_tenants_daas.us_wealth_management"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature name to talking point mapping\n",
    "TALKING_POINT_TEMPLATES = {\n",
    "    'acct_val_amt': \"Account value of ${value:,.0f}\",\n",
    "    'wc_total_assets': \"Total assets of ${value:,.0f}\",\n",
    "    'aum_segment': \"AUM tier: {value}\",\n",
    "    'wc_assetmix_stocks': \"Stock allocation: ${value:,.0f}\",\n",
    "    'wc_assetmix_bonds': \"Bond allocation: ${value:,.0f}\",\n",
    "    'wc_assetmix_mutual_funds': \"Mutual fund allocation: ${value:,.0f}\",\n",
    "    'wc_assetmix_deposits': \"Deposit allocation: ${value:,.0f}\",\n",
    "    'aggressive_investor': \"Aggressive risk profile\",\n",
    "    'conservative_investor': \"Conservative risk profile\",\n",
    "    'client_age': \"Age {value:.0f}\",\n",
    "    'retirement_planning_trigger': \"Retirement planning phase\",\n",
    "    'wealth_building_trigger': \"Wealth building phase\",\n",
    "    'family_protection_trigger': \"Family protection phase\",\n",
    "    'monthly_preminum_amount': \"Premium capacity: ${value:,.0f}/month\",\n",
    "    'face_amt': \"Current coverage: ${value:,.0f}\",\n",
    "    'cash_val_amt': \"Cash value: ${value:,.0f}\",\n",
    "    'snp_close_lead_6': \"S&P 6-month trend: {value:+.1f}%\",\n",
    "    'snp_close_lead_12': \"S&P 12-month trend: {value:+.1f}%\",\n",
    "    'snp_close_variation': \"Market volatility: {value:.2f}\",\n",
    "    'client_tenure_years': \"{value:.0f} years with us\",\n",
    "    'channel': \"Channel: {value}\",\n",
    "    'product_category': \"First product: {value}\",\n",
    "    'advisor_investment_affinity': \"Strong advisor-investment relationship\",\n",
    "    'branch_retirement_affinity': \"Branch retirement expertise\"\n",
    "}\n",
    "\n",
    "def generate_talking_point(feature_name, feature_value, shap_value):\n",
    "    impact = \"+++\" if abs(shap_value) > 0.1 else \"++\" if abs(shap_value) > 0.05 else \"+\"\n",
    "    \n",
    "    # Handle encoded/generic features from training data\n",
    "    if feature_name.startswith('encoded_feature_') or (feature_name.startswith('feature_') and feature_name[8:].isdigit()):\n",
    "        return f\"{impact} Model factor (impact: {shap_value:.3f}, value: {feature_value})\"\n",
    "    \n",
    "    # Handle known features\n",
    "    if feature_name not in TALKING_POINT_TEMPLATES:\n",
    "        return f\"{impact} {feature_name}: {feature_value}\"\n",
    "    \n",
    "    template = TALKING_POINT_TEMPLATES[feature_name]\n",
    "    \n",
    "    if '{value' in template:\n",
    "        try:\n",
    "            # Convert string numbers back to numeric if needed\n",
    "            if isinstance(feature_value, str) and feature_value not in ['N/A', 'UNKNOWN']:\n",
    "                try:\n",
    "                    feature_value = float(feature_value)\n",
    "                except:\n",
    "                    pass\n",
    "            talking_point = template.format(value=feature_value)\n",
    "        except:\n",
    "            talking_point = f\"{feature_name}: {feature_value}\"\n",
    "    else:\n",
    "        talking_point = template\n",
    "    \n",
    "    return f\"{impact} {talking_point}\"\n",
    "\n",
    "# Generate talking points\n",
    "print(\"Generating talking points...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(len(df_explanations)):\n",
    "    for rank in range(1, 6):\n",
    "        feature = df_explanations.iloc[i][f'top_feature_{rank}']\n",
    "        value = df_explanations.iloc[i][f'top_feature_{rank}_value']\n",
    "        shap_val = df_explanations.iloc[i][f'top_feature_{rank}_shap']\n",
    "        \n",
    "        tp = generate_talking_point(feature, value, shap_val)\n",
    "        df_explanations.loc[i, f'talking_point_{rank}'] = tp\n",
    "\n",
    "print(f\"Generated talking points for {len(df_explanations):,} records\")\n",
    "\n",
    "# Preview (only if we have data)\n",
    "if len(df_explanations) > 0:\n",
    "    print(\"\\n\uD83D\uDCCB Sample Explanation:\")\n",
    "    print(\"=\" * 60)\n",
    "    sample = df_explanations.iloc[0]\n",
    "    print(f\"Client: {sample['axa_party_id']}\")\n",
    "    print(f\"Product: {sample['product'].title()}\")\n",
    "    print(f\"Score: {sample['score']:.3f}\")\n",
    "    print(f\"\\nTop Reasons:\")\n",
    "    for i in range(1, 6):\n",
    "        print(f\"{i}. {sample[f'talking_point_{i}']}\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No explanations to preview (SHAP analysis failed for all products)\")\n",
    "    print(\"   This may indicate:\")\n",
    "    print(\"   - No clients scored above the threshold\")\n",
    "    print(\"   - Model extraction from pipeline failed\")\n",
    "    print(\"   - Consider lowering min_score_threshold or checking model structure\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754c735a-9e5e-4290-abf7-0e83463b12f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Importance by Category\n",
    "\n",
    "Understand which types of features drive predictions for each product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91de3b11-2a69-44ca-a753-ce4361669775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "branch_code": "83",
        "min_score_threshold": "0.60",
        "prediction_month": "202510",
        "target_schema": "eda_smartlist.us_wealth_management_smartlist",
        "top_n_per_product": "500",
        "training_month": "202510",
        "wm_source_schema": "dl_tenants_daas.us_wealth_management"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature Category Breakdown\n",
    "print(\"Feature Importance by Category\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define feature categories\n",
    "FEATURE_CATEGORIES = {\n",
    "    'Wealth Indicators': ['acct_val_amt', 'wc_total_assets', 'aum_segment'],\n",
    "    'Asset Mix': ['wc_assetmix_stocks', 'wc_assetmix_bonds', 'wc_assetmix_mutual_funds', 'wc_assetmix_deposits'],\n",
    "    'Risk Profile': ['aggressive_investor', 'conservative_investor'],\n",
    "    'Demographics': ['client_age', 'client_tenure_years'],\n",
    "    'Life Triggers': ['retirement_planning_trigger', 'wealth_building_trigger', 'family_protection_trigger'],\n",
    "    'Market Timing': ['snp_close_lead_6', 'snp_close_lead_12', 'snp_close_variation'],\n",
    "    'Insurance': ['monthly_preminum_amount', 'face_amt', 'cash_val_amt'],\n",
    "    'Channel & Relationship': ['channel', 'product_category', 'advisor_investment_affinity', 'branch_retirement_affinity']\n",
    "}\n",
    "\n",
    "if len(shap_results) > 0:\n",
    "    for product in shap_results:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{product.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        shap_values = shap_results[product]['shap_values']\n",
    "        feature_names = shap_results[product]['transformed_features']\n",
    "        \n",
    "        # Calculate average absolute SHAP importance by category\n",
    "        category_importance = {}\n",
    "        category_counts = {}\n",
    "        \n",
    "        for category, category_features in FEATURE_CATEGORIES.items():\n",
    "            importance_sum = 0\n",
    "            count = 0\n",
    "            \n",
    "            for i, feat in enumerate(feature_names):\n",
    "                # Check if this transformed feature matches any category feature\n",
    "                if any(cat_feat in feat for cat_feat in category_features):\n",
    "                    importance_sum += np.abs(shap_values[:, i]).mean()\n",
    "                    count += 1\n",
    "            \n",
    "            if count > 0:\n",
    "                category_importance[category] = importance_sum / count\n",
    "                category_counts[category] = count\n",
    "        \n",
    "        # Sort and display\n",
    "        sorted_categories = sorted(category_importance.items(), key=lambda x: -x[1])\n",
    "        \n",
    "        print(f\"\\nCategory Importance (Average |SHAP| per feature):\")\n",
    "        print(f\"{'Category':<30} {'Avg Impact':>12} {'Features':>10}\")\n",
    "        print(\"-\" * 54)\n",
    "        \n",
    "        for category, importance in sorted_categories:\n",
    "            count = category_counts[category]\n",
    "            \n",
    "            # Add visual indicator\n",
    "            if importance > 0.08:\n",
    "                indicator = \"\uD83D\uDD25 \"\n",
    "            elif importance > 0.05:\n",
    "                indicator = \"⭐ \"\n",
    "            elif importance > 0.03:\n",
    "                indicator = \"\"\n",
    "            else:\n",
    "                indicator = \"   \"\n",
    "            \n",
    "            print(f\"{indicator}{category:<28} {importance:>11.4f} {count:>10}\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No SHAP results available for category analysis\")\n",
    "\n",
    "print(\"\\nCategory analysis complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09fd4d25-840f-4791-90d5-55b592b0cbea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Enhanced Talking Points with Context\n",
    "\n",
    "Generate richer talking points with context and recommended actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ba15ac1-32cd-40a5-8010-31cf20dab738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "branch_code": "83",
        "min_score_threshold": "0.60",
        "prediction_month": "202510",
        "target_schema": "eda_smartlist.us_wealth_management_smartlist",
        "top_n_per_product": "500",
        "training_month": "202510",
        "wm_source_schema": "dl_tenants_daas.us_wealth_management"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enhanced talking point templates with context and actions\n",
    "print(\"\uD83D\uDCAC Generating Enhanced Talking Points...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "ENHANCED_TEMPLATES = {\n",
    "    'acct_val_amt': {\n",
    "        'base': 'Account value of ${value:,.0f}',\n",
    "        'context': lambda v: 'strong capacity' if v > 50000 else 'good capacity' if v > 25000 else 'moderate capacity',\n",
    "        'action': 'Discuss portfolio diversification'\n",
    "    },\n",
    "    'wc_total_assets': {\n",
    "        'base': 'Total assets of ${value:,.0f}',\n",
    "        'context': lambda v: 'high net worth client' if v > 100000 else 'substantial assets',\n",
    "        'action': 'Explore comprehensive wealth planning'\n",
    "    },\n",
    "    'aum_segment': {\n",
    "        'base': 'AUM tier: {value}',\n",
    "        'context': lambda v: 'premium client segment' if v == 'HIGH' else 'core client segment',\n",
    "        'action': lambda v: 'White-glove service approach' if v == 'HIGH' else 'Standard advisory approach'\n",
    "    },\n",
    "    'wc_assetmix_stocks': {\n",
    "        'base': 'Stock allocation: ${value:,.0f}',\n",
    "        'context': 'equity-focused portfolio',\n",
    "        'action': 'Position growth products'\n",
    "    },\n",
    "    'aggressive_investor': {\n",
    "        'base': 'Aggressive risk profile',\n",
    "        'context': 'high-growth orientation',\n",
    "        'action': 'Emphasize equity and growth opportunities'\n",
    "    },\n",
    "    'conservative_investor': {\n",
    "        'base': 'Conservative risk profile',\n",
    "        'context': 'capital preservation focus',\n",
    "        'action': 'Highlight stability and guaranteed products'\n",
    "    },\n",
    "    'client_age': {\n",
    "        'base': 'Age {value:.0f}',\n",
    "        'context': lambda v: 'retirement planning window' if v > 55 else 'wealth accumulation phase' if v > 40 else 'early career',\n",
    "        'action': lambda v: 'Focus on retirement readiness' if v > 55 else 'Emphasize long-term growth'\n",
    "    },\n",
    "    'retirement_planning_trigger': {\n",
    "        'base': 'Retirement planning phase',\n",
    "        'context': 'active retirement preparation',\n",
    "        'action': 'Lead with retirement income solutions'\n",
    "    },\n",
    "    'snp_close_lead_6': {\n",
    "        'base': 'S&P 6-month trend: {value:+.1f}%',\n",
    "        'context': lambda v: 'positive market momentum' if v > 0 else 'market correction opportunity',\n",
    "        'action': lambda v: 'Act on current strength' if v > 0 else 'Position for recovery'\n",
    "    },\n",
    "    'channel': {\n",
    "        'base': 'Channel: {value}',\n",
    "        'context': lambda v: 'advisor relationship' if 'Advisor' in str(v) else 'direct channel',\n",
    "        'action': lambda v: 'Leverage advisor trust' if 'Advisor' in str(v) else 'Personal outreach approach'\n",
    "    },\n",
    "    'client_tenure_years': {\n",
    "        'base': '{value:.0f} years with us',\n",
    "        'context': lambda v: 'long-standing relationship' if v > 10 else 'established client' if v > 5 else 'newer client',\n",
    "        'action': lambda v: 'Deepen existing relationship' if v > 5 else 'Build trust and engagement'\n",
    "    }\n",
    "}\n",
    "\n",
    "def generate_enhanced_talking_point(feature_name, feature_value, shap_value):\n",
    "    \"\"\"Generate enhanced talking point with context and action\"\"\"\n",
    "    # Impact indicator\n",
    "    impact = \"\uD83D\uDD25\" if abs(shap_value) > 0.1 else \"⭐\" if abs(shap_value) > 0.05 else \"\"\n",
    "    \n",
    "    # Check if we have enhanced template\n",
    "    base_feature = feature_name.split('_')[0] + '_' + feature_name.split('_')[1] if '_' in feature_name else feature_name\n",
    "    \n",
    "    # Find matching template\n",
    "    template_dict = None\n",
    "    for template_key in ENHANCED_TEMPLATES:\n",
    "        if template_key in feature_name or feature_name.startswith(template_key):\n",
    "            template_dict = ENHANCED_TEMPLATES[template_key]\n",
    "            break\n",
    "    \n",
    "    if not template_dict:\n",
    "        # Fallback to simple format\n",
    "        return f\"{impact} {feature_name}: {feature_value} (impact: {shap_value:+.3f})\"\n",
    "    \n",
    "    # Format base message\n",
    "    try:\n",
    "        template = template_dict['base']\n",
    "        if '{value' in template:\n",
    "            # Convert to numeric if needed\n",
    "            if isinstance(feature_value, str) and feature_value not in ['N/A', 'UNKNOWN']:\n",
    "                try:\n",
    "                    feature_value = float(feature_value)\n",
    "                except:\n",
    "                    pass\n",
    "            message = template.format(value=feature_value)\n",
    "        else:\n",
    "            message = template\n",
    "    except:\n",
    "        message = f\"{feature_name}: {feature_value}\"\n",
    "    \n",
    "    # Add context\n",
    "    context = template_dict.get('context')\n",
    "    if context:\n",
    "        if callable(context):\n",
    "            try:\n",
    "                context_str = context(feature_value)\n",
    "            except:\n",
    "                context_str = None\n",
    "        else:\n",
    "            context_str = context\n",
    "        \n",
    "        if context_str:\n",
    "            message += f\" ({context_str})\"\n",
    "    \n",
    "    # Add action\n",
    "    action = template_dict.get('action')\n",
    "    if action:\n",
    "        if callable(action):\n",
    "            try:\n",
    "                action_str = action(feature_value)\n",
    "            except:\n",
    "                action_str = None\n",
    "        else:\n",
    "            action_str = action\n",
    "        \n",
    "        if action_str:\n",
    "            message += f\" → {action_str}\"\n",
    "    \n",
    "    return f\"{impact} {message}\"\n",
    "\n",
    "# Generate enhanced talking points and add as new columns\n",
    "if len(df_explanations) > 0:\n",
    "    for i in range(len(df_explanations)):\n",
    "        for rank in range(1, 6):\n",
    "            feature = df_explanations.iloc[i][f'top_feature_{rank}']\n",
    "            value = df_explanations.iloc[i][f'top_feature_{rank}_value']\n",
    "            shap_val = df_explanations.iloc[i][f'top_feature_{rank}_shap']\n",
    "            \n",
    "            enhanced_tp = generate_enhanced_talking_point(feature, value, shap_val)\n",
    "            df_explanations.loc[i, f'enhanced_talking_point_{rank}'] = enhanced_tp\n",
    "    \n",
    "    print(f\"Generated enhanced talking points for {len(df_explanations):,} records\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\n\uD83D\uDCCB Sample Enhanced Explanation:\")\n",
    "    print(\"=\" * 80)\n",
    "    sample = df_explanations.iloc[0]\n",
    "    print(f\"Client: {sample['axa_party_id']}\")\n",
    "    print(f\"Product: {sample['product'].title()}\")\n",
    "    print(f\"Score: {sample['score']:.3f}\")\n",
    "    print(f\"\\nTop Reasons (Enhanced):\")\n",
    "    for i in range(1, 6):\n",
    "        print(f\"{i}. {sample[f'enhanced_talking_point_{i}']}\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No explanations to enhance\")\n",
    "\n",
    "print(\"\\nEnhanced talking points complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f103d9c9-4604-46b5-867d-86f6c87ffe95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Product Intelligence Summary\n",
    "\n",
    "High-level insights about what drives predictions for each product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6799f83e-66f3-45bf-9d3c-98df6af9a2af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "branch_code": "83",
        "min_score_threshold": "0.60",
        "prediction_month": "202510",
        "target_schema": "eda_smartlist.us_wealth_management_smartlist",
        "top_n_per_product": "500",
        "training_month": "202510",
        "wm_source_schema": "dl_tenants_daas.us_wealth_management"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Product Intelligence Dashboard\n",
    "print(\"PRODUCT INTELLIGENCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(shap_results) > 0:\n",
    "    for product in shap_results:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{product.replace('_', ' ').upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        shap_values = shap_results[product]['shap_values']\n",
    "        feature_names = shap_results[product]['transformed_features']\n",
    "        client_ids = shap_results[product]['client_ids']\n",
    "        scores = shap_results[product]['scores']\n",
    "        \n",
    "        # Client stats\n",
    "        print(f\"\\n\uD83D\uDC65 CLIENT POPULATION:\")\n",
    "        print(f\"  Total clients analyzed: {len(client_ids):,}\")\n",
    "        print(f\"  Score range: {scores.min():.3f} - {scores.max():.3f}\")\n",
    "        print(f\"  Average score: {scores.mean():.3f}\")\n",
    "        print(f\"  Median score: {np.median(scores):.3f}\")\n",
    "        \n",
    "        # Score quartiles\n",
    "        q25, q50, q75, q90 = np.percentile(scores, [25, 50, 75, 90])\n",
    "        print(f\"\\n  Score Distribution:\")\n",
    "        print(f\"    Top 10%:  >{q90:.3f} ({int(len(scores) * 0.1):,} clients)\")\n",
    "        print(f\"    Top 25%:  >{q75:.3f} ({int(len(scores) * 0.25):,} clients)\")\n",
    "        print(f\"    Median:    {q50:.3f}\")\n",
    "        \n",
    "        # Top drivers\n",
    "        print(f\"\\n\uD83D\uDD11 TOP 5 PREDICTIVE DRIVERS:\")\n",
    "        avg_importance = np.abs(shap_values).mean(axis=0)\n",
    "        top_indices = np.argsort(avg_importance)[-5:][::-1]\n",
    "        \n",
    "        for i, idx in enumerate(top_indices, 1):\n",
    "            feat = feature_names[idx]\n",
    "            importance = avg_importance[idx]\n",
    "            \n",
    "            # Calculate how often this feature is in top 3\n",
    "            top_3_count = 0\n",
    "            for client_shap in shap_values:\n",
    "                top_3_indices = np.argsort(np.abs(client_shap))[-3:]\n",
    "                if idx in top_3_indices:\n",
    "                    top_3_count += 1\n",
    "            \n",
    "            top_3_pct = (top_3_count / len(shap_values)) * 100\n",
    "            \n",
    "            print(f\"  {i}. {feat:<35s}\")\n",
    "            print(f\"     Avg |SHAP|: {importance:.4f} | Top-3 for {top_3_pct:.1f}% of clients\")\n",
    "        \n",
    "        # Feature consistency\n",
    "        print(f\"\\nPREDICTION CONSISTENCY:\")\n",
    "        # Calculate variance in SHAP values\n",
    "        shap_variance = np.var(shap_values, axis=0)\n",
    "        avg_variance = shap_variance.mean()\n",
    "        \n",
    "        if avg_variance < 0.005:\n",
    "            consistency = \"Very Consistent - Similar drivers across clients\"\n",
    "        elif avg_variance < 0.01:\n",
    "            consistency = \"Moderately Consistent - Some variation in drivers\"\n",
    "        else:\n",
    "            consistency = \"Diverse - Different drivers for different clients\"\n",
    "        \n",
    "        print(f\"  {consistency}\")\n",
    "        print(f\"  (Average SHAP variance: {avg_variance:.5f})\")\n",
    "        \n",
    "        # Actionable insights\n",
    "        print(f\"\\n\uD83D\uDCA1 KEY INSIGHTS:\")\n",
    "        top_feature = feature_names[top_indices[0]]\n",
    "        \n",
    "        if 'acct_val' in top_feature or 'wc_total' in top_feature or 'aum' in top_feature:\n",
    "            print(f\"  • Wealth indicators are the primary driver\")\n",
    "            print(f\"  • Target high-net-worth segments first\")\n",
    "        elif 'age' in top_feature or 'retirement' in top_feature:\n",
    "            print(f\"  • Life stage is critical\")\n",
    "            print(f\"  • Align messaging with retirement readiness\")\n",
    "        elif 'aggressive' in top_feature or 'conservative' in top_feature:\n",
    "            print(f\"  • Risk profile drives suitability\")\n",
    "            print(f\"  • Match product features to risk appetite\")\n",
    "        elif 'snp' in top_feature:\n",
    "            print(f\"  • Market timing matters\")\n",
    "            print(f\"  • Leverage current market conditions in messaging\")\n",
    "        \n",
    "        # Calculate client segments\n",
    "        high_scorers = len([s for s in scores if s > 0.7])\n",
    "        medium_scorers = len([s for s in scores if 0.6 <= s <= 0.7])\n",
    "        \n",
    "        print(f\"\\nTARGETING RECOMMENDATION:\")\n",
    "        if high_scorers > 500:\n",
    "            print(f\"  • Focus on top {min(500, high_scorers)} highest-scoring clients first\")\n",
    "            print(f\"  • Strong pipeline with {high_scorers:,} high-confidence leads\")\n",
    "        elif high_scorers > 100:\n",
    "            print(f\"  • Work all {high_scorers:,} high-scoring clients (>0.7)\")\n",
    "            print(f\"  • Then expand to medium tier ({medium_scorers:,} clients)\")\n",
    "        else:\n",
    "            print(f\"  • Broaden criteria to include medium-scoring clients\")\n",
    "            print(f\"  • Focus on enhancing conversion approach\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No SHAP results available for intelligence summary\")\n",
    "\n",
    "print(\"\\nProduct intelligence summary complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6e0ac4c-0218-4222-b5d0-1425a69dda24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature Interaction Insights\n",
    "\n",
    "Identify powerful combinations of features that work together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67d9d42f-3dc2-4ed9-8071-6db510cc5883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "branch_code": "83",
        "min_score_threshold": "0.60",
        "prediction_month": "202510",
        "target_schema": "eda_smartlist.us_wealth_management_smartlist",
        "top_n_per_product": "500",
        "training_month": "202510",
        "wm_source_schema": "dl_tenants_daas.us_wealth_management"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature Interaction Analysis\n",
    "print(\"FEATURE INTERACTION INSIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define known powerful interactions\n",
    "INTERACTION_PATTERNS = {\n",
    "    ('acct_val_amt', 'aggressive_investor'): {\n",
    "        'insight': 'High wealth + Aggressive = Prime for growth products',\n",
    "        'action': 'Position equity-heavy portfolios and alternative investments'\n",
    "    },\n",
    "    ('client_age', 'retirement_planning_trigger'): {\n",
    "        'insight': 'Age + Retirement phase = Urgent retirement planning need',\n",
    "        'action': 'Lead with retirement income and preservation strategies'\n",
    "    },\n",
    "    ('wc_assetmix_stocks', 'snp_close_lead_6'): {\n",
    "        'insight': 'Stock allocation + Market momentum = Market timing opportunity',\n",
    "        'action': 'Leverage current market performance in pitch'\n",
    "    },\n",
    "    ('aum_segment', 'advisor_investment_affinity'): {\n",
    "        'insight': 'High AUM + Advisor relationship = Premium service opportunity',\n",
    "        'action': 'Offer white-glove advisory and exclusive products'\n",
    "    },\n",
    "    ('conservative_investor', 'client_age'): {\n",
    "        'insight': 'Conservative profile + Older age = Capital preservation focus',\n",
    "        'action': 'Emphasize guaranteed products and stable income'\n",
    "    },\n",
    "    ('face_amt', 'family_protection_trigger'): {\n",
    "        'insight': 'Existing coverage + Family phase = Additional protection need',\n",
    "        'action': 'Review coverage gaps and family situation changes'\n",
    "    },\n",
    "    ('wc_total_assets', 'client_tenure_years'): {\n",
    "        'insight': 'High assets + Long tenure = Deep relationship opportunity',\n",
    "        'action': 'Comprehensive financial planning and estate strategies'\n",
    "    }\n",
    "}\n",
    "\n",
    "if len(shap_results) > 0:\n",
    "    for product in shap_results:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{product.replace('_', ' ').upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        feature_names = shap_results[product]['transformed_features']\n",
    "        shap_values = shap_results[product]['shap_values']\n",
    "        \n",
    "        # Check which interactions are present and relevant\n",
    "        found_interactions = []\n",
    "        \n",
    "        for (feat1, feat2), interaction_info in INTERACTION_PATTERNS.items():\n",
    "            # Check if both features are present (or their encoded versions)\n",
    "            feat1_present = any(feat1 in f for f in feature_names)\n",
    "            feat2_present = any(feat2 in f for f in feature_names)\n",
    "            \n",
    "            if feat1_present and feat2_present:\n",
    "                # Get indices\n",
    "                feat1_idx = [i for i, f in enumerate(feature_names) if feat1 in f]\n",
    "                feat2_idx = [i for i, f in enumerate(feature_names) if feat2 in f]\n",
    "                \n",
    "                # Calculate combined importance\n",
    "                feat1_importance = np.abs(shap_values[:, feat1_idx]).mean()\n",
    "                feat2_importance = np.abs(shap_values[:, feat2_idx]).mean()\n",
    "                combined_importance = float(feat1_importance + feat2_importance)\n",
    "                \n",
    "                found_interactions.append({\n",
    "                    'features': (feat1, feat2),\n",
    "                    'importance': combined_importance,\n",
    "                    'info': interaction_info\n",
    "                })\n",
    "        \n",
    "        if found_interactions:\n",
    "            # Sort by importance\n",
    "            found_interactions.sort(key=lambda x: -x['importance'])\n",
    "            \n",
    "            print(f\"\\nFound {len(found_interactions)} relevant feature interactions:\")\n",
    "            print()\n",
    "            \n",
    "            for i, interaction in enumerate(found_interactions[:5], 1):\n",
    "                feat1, feat2 = interaction['features']\n",
    "                importance = interaction['importance']\n",
    "                info = interaction['info']\n",
    "                \n",
    "                # Visual indicator\n",
    "                if importance > 0.15:\n",
    "                    indicator = \"\uD83D\uDD25 CRITICAL\"\n",
    "                elif importance > 0.10:\n",
    "                    indicator = \"⭐ HIGH\"\n",
    "                else:\n",
    "                    indicator = \"MODERATE\"\n",
    "                \n",
    "                print(f\"{i}. {indicator}\")\n",
    "                print(f\"   {feat1} + {feat2}\")\n",
    "                print(f\"   Combined Impact: {importance:.4f}\")\n",
    "                print(f\"   \uD83D\uDCA1 {info['insight']}\")\n",
    "                print(f\"   {info['action']}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(f\"\\n  No predefined interactions found in feature set\")\n",
    "            print(f\"  Consider analyzing custom interactions for this product\")\n",
    "        \n",
    "        print(f\"{'='*80}\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No SHAP results available for interaction analysis\")\n",
    "\n",
    "print(\"\\nFeature interaction analysis complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21077dc9-0d6d-4e33-92a4-ef9a5e7fe129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save Explanations Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29562f30-d69f-46ac-b19c-14c757591e45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "branch_code": "83",
        "min_score_threshold": "0.60",
        "prediction_month": "202510",
        "target_schema": "eda_smartlist.us_wealth_management_smartlist",
        "top_n_per_product": "500",
        "training_month": "202510",
        "wm_source_schema": "dl_tenants_daas.us_wealth_management"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if we have any explanations before proceeding\n",
    "if len(df_explanations) == 0:\n",
    "    print(\"WARNING: No explanations generated. Skipping save and remaining cells.\")\n",
    "    print(\"   This usually means:\")\n",
    "    print(\"   1. No clients above threshold (lower min_score_threshold), OR\")\n",
    "    print(\"   2. SHAP analysis failed for all products (check model extraction)\")\n",
    "    print(\"\\nStopping notebook execution. Please review SHAP errors above.\")\n",
    "    dbutils.notebook.exit(\"No explanations to save - SHAP analysis failed\")\n",
    "\n",
    "# Convert to Spark and save\n",
    "print(\"Saving explanations to Delta table...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_explanations_spark = spark.createDataFrame(df_explanations)\n",
    "\n",
    "explanations_table = f\"{target_schema}.multi_product_explanations\"\n",
    "\n",
    "df_explanations_spark.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .partitionBy(\"prediction_month\", \"product\") \\\n",
    "    .saveAsTable(explanations_table)\n",
    "\n",
    "print(f\"Saved {len(df_explanations):,} explanations to: {explanations_table}\")\n",
    "print(f\"Partitioned by: prediction_month, product\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c62df225-9939-454a-aeda-a4bc045ada87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Advisor Smart Lists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8336685f-5fb2-42f8-98aa-9a75cfcbbbfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Create Final Output Table\n",
    "\n",
    "Combine enhanced scores with SHAP explanations to create the final comprehensive output table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ec7afb-ddaf-4b1e-a51d-5fb73eb0b202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "branch_code": "83",
        "min_score_threshold": "0.60",
        "prediction_month": "202510",
        "target_schema": "eda_smartlist.us_wealth_management_smartlist",
        "top_n_per_product": "500",
        "training_month": "202510",
        "wm_source_schema": "dl_tenants_daas.us_wealth_management"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 4: Create Final Output Table\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 4: CREATING FINAL OUTPUT TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if explanations table exists and has data\n",
    "explanations_table = f\"{target_schema}.multi_product_explanations\"\n",
    "explanations_exists = spark.catalog.tableExists(explanations_table)\n",
    "\n",
    "if not explanations_exists:\n",
    "    print(f\"WARNING: {explanations_table} does not exist yet.\")\n",
    "    print(\"   Final output table will contain only enhanced scores (no SHAP explanations).\")\n",
    "    print(\"   Run SHAP analysis cells first to include explanations.\")\n",
    "    \n",
    "    # Create final table with just enhanced scores\n",
    "    final_output = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            enhanced.*,\n",
    "            NULL AS shap_available\n",
    "        FROM {target_schema}.multi_product_client_scores_enhanced AS enhanced\n",
    "        WHERE enhanced.prediction_month = '{prediction_month}'\n",
    "    \"\"\")\n",
    "else:\n",
    "    # Check if explanations have data\n",
    "    explanations_count = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as cnt \n",
    "        FROM {explanations_table}\n",
    "        WHERE prediction_month = '{prediction_month}'\n",
    "    \"\"\").collect()[0]['cnt']\n",
    "    \n",
    "    if explanations_count == 0:\n",
    "        print(f\"WARNING: {explanations_table} exists but has no data for prediction_month={prediction_month}.\")\n",
    "        print(\"   Final output table will contain only enhanced scores (no SHAP explanations).\")\n",
    "        \n",
    "        # Create final table with just enhanced scores\n",
    "        final_output = spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                enhanced.*,\n",
    "                CAST(0 AS INT) AS shap_available\n",
    "            FROM {target_schema}.multi_product_client_scores_enhanced AS enhanced\n",
    "            WHERE enhanced.prediction_month = '{prediction_month}'\n",
    "        \"\"\")\n",
    "    else:\n",
    "        print(f\"Found {explanations_count:,} explanations. Creating comprehensive final table...\")\n",
    "        \n",
    "        # Create final table combining enhanced scores with SHAP explanations\n",
    "        final_output = spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                enhanced.*,\n",
    "                explanations.product AS explanation_product,\n",
    "                explanations.score AS explanation_score,\n",
    "                explanations.top_features,\n",
    "                explanations.top_feature_values,\n",
    "                explanations.talking_points,\n",
    "                explanations.feature_importance_json,\n",
    "                CAST(1 AS INT) AS shap_available\n",
    "            FROM {target_schema}.multi_product_client_scores_enhanced AS enhanced\n",
    "            LEFT JOIN {explanations_table} AS explanations\n",
    "                ON enhanced.axa_party_id = explanations.axa_party_id\n",
    "                AND enhanced.prediction_month = explanations.prediction_month\n",
    "                AND explanations.product = enhanced.best_product\n",
    "            WHERE enhanced.prediction_month = '{prediction_month}'\n",
    "        \"\"\")\n",
    "\n",
    "final_count = final_output.count()\n",
    "print(f\"\\nFinal output table: {final_count:,} records\")\n",
    "\n",
    "# Save final output table\n",
    "final_output_table = f\"{target_schema}.multi_product_final_output\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {final_output_table}\")\n",
    "\n",
    "print(f\"Writing final output to {final_output_table}...\")\n",
    "final_output.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .partitionBy(\"prediction_month\", \"best_product\") \\\n",
    "    .saveAsTable(final_output_table)\n",
    "\n",
    "saved_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {final_output_table}\").collect()[0]['cnt']\n",
    "print(f\"✅ Saved {saved_count:,} records to: {final_output_table}\")\n",
    "print(f\"   Partitioned by: prediction_month, best_product\")\n",
    "print(f\"\\nFinal output table includes:\")\n",
    "print(f\"   - All scores from Notebook 04\")\n",
    "print(f\"   - Branch {branch_code} filter\")\n",
    "print(f\"   - Current product category\")\n",
    "print(f\"   - First product category\")\n",
    "print(f\"   - Client demographics\")\n",
    "print(f\"   - SHAP explanations (if available)\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample of final output:\")\n",
    "final_output.select(\n",
    "    'axa_party_id',\n",
    "    'best_product',\n",
    "    'best_score',\n",
    "    'current_product_category',\n",
    "    'first_product_category',\n",
    "    'client_age',\n",
    "    'acct_val_amt',\n",
    "    'channel',\n",
    "    'shap_available'\n",
    ").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9653cc41-30ec-47c5-bebd-bbebc220b208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "branch_code": "83",
        "min_score_threshold": "0.60",
        "prediction_month": "202510",
        "target_schema": "eda_smartlist.us_wealth_management_smartlist",
        "top_n_per_product": "500",
        "training_month": "202510",
        "wm_source_schema": "dl_tenants_daas.us_wealth_management"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create smart lists\n",
    "print(\"Creating advisor smart lists...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "smart_lists_query = f\"\"\"\n",
    "SELECT \n",
    "    s.axa_party_id,\n",
    "    s.policy_no,\n",
    "    e.product,\n",
    "    e.score,\n",
    "    s.best_product,\n",
    "    s.best_score,\n",
    "    CASE \n",
    "        WHEN e.product = 'investment' THEN s.investment_rank\n",
    "        WHEN e.product = 'retirement' THEN s.retirement_rank\n",
    "        WHEN e.product = 'life_insurance' THEN s.life_insurance_rank\n",
    "        WHEN e.product = 'network_products' THEN s.network_products_rank\n",
    "    END as product_rank,\n",
    "    CASE \n",
    "        WHEN e.product = 'investment' THEN s.investment_decile\n",
    "        WHEN e.product = 'retirement' THEN s.retirement_decile\n",
    "        WHEN e.product = 'life_insurance' THEN s.life_insurance_decile\n",
    "        WHEN e.product = 'network_products' THEN s.network_products_decile\n",
    "    END as product_decile,\n",
    "    s.multi_product_opportunity_high,\n",
    "    s.multi_product_opportunity_medium,\n",
    "    s.multi_product_opportunity_base,\n",
    "    e.talking_point_1,\n",
    "    e.talking_point_2,\n",
    "    e.talking_point_3,\n",
    "    e.talking_point_4,\n",
    "    e.talking_point_5,\n",
    "    e.top_feature_1,\n",
    "    e.top_feature_1_shap,\n",
    "    s.prediction_month,\n",
    "    s.training_month\n",
    "FROM {target_schema}.multi_product_client_scores s\n",
    "INNER JOIN {target_schema}.multi_product_explanations e\n",
    "    ON s.axa_party_id = e.axa_party_id\n",
    "    AND s.prediction_month = e.prediction_month\n",
    "WHERE s.prediction_month = '{prediction_month}'\n",
    "  AND e.score > {min_score_threshold}\n",
    "\"\"\"\n",
    "\n",
    "smart_lists = spark.sql(smart_lists_query)\n",
    "smart_lists_count = smart_lists.count()\n",
    "print(f\"Created smart lists with {smart_lists_count:,} records\")\n",
    "\n",
    "# Save to table\n",
    "smart_lists_table = f\"{target_schema}.advisor_smart_lists\"\n",
    "\n",
    "smart_lists.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .partitionBy(\"prediction_month\", \"product\") \\\n",
    "    .saveAsTable(smart_lists_table)\n",
    "\n",
    "print(f\"Saved to: {smart_lists_table}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\n\uD83D\uDCCB Sample Smart List Entry:\")\n",
    "print(\"=\" * 60)\n",
    "display(smart_lists.filter(col('product') == 'investment').orderBy(col('score').desc()).limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb585b76-93c7-4d9c-8e15-e1c526207485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Multi-Product Strategy Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c96009d-d44a-4291-a824-f6037dae5964",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "branch_code": "83",
        "min_score_threshold": "0.60",
        "prediction_month": "202510",
        "target_schema": "eda_smartlist.us_wealth_management_smartlist",
        "top_n_per_product": "500",
        "training_month": "202510",
        "wm_source_schema": "dl_tenants_daas.us_wealth_management"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate multi-product strategy\n",
    "print(\"Generating multi-product strategy recommendations...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "multi_product_query = f\"\"\"\n",
    "WITH base AS (\n",
    "    SELECT \n",
    "        axa_party_id,\n",
    "        policy_no,\n",
    "        prediction_month,\n",
    "        best_product,\n",
    "        best_score,\n",
    "        second_best_product,\n",
    "        second_best_score,\n",
    "        multi_product_opportunity_medium,\n",
    "        medium_score_count\n",
    "    FROM {target_schema}.multi_product_client_scores\n",
    "    WHERE prediction_month = '{prediction_month}'\n",
    "      AND multi_product_opportunity_medium = 1\n",
    ")\n",
    "SELECT \n",
    "    b.*,\n",
    "    CASE \n",
    "        WHEN best_score - second_best_score > 0.15 THEN 'Focus on primary product first'\n",
    "        WHEN best_score - second_best_score < 0.05 THEN 'Dual product approach'\n",
    "        ELSE 'Lead with primary, mention secondary'\n",
    "    END as pitch_strategy,\n",
    "    CONCAT(best_product, ' + ', second_best_product) as product_bundle,\n",
    "    (best_score + second_best_score) / 2 as combined_score,\n",
    "    e1.talking_point_1 as primary_reason_1,\n",
    "    e1.talking_point_2 as primary_reason_2,\n",
    "    e2.talking_point_1 as secondary_reason_1,\n",
    "    e2.talking_point_2 as secondary_reason_2\n",
    "FROM base b\n",
    "LEFT JOIN {target_schema}.multi_product_explanations e1\n",
    "    ON b.axa_party_id = e1.axa_party_id\n",
    "    AND b.prediction_month = e1.prediction_month\n",
    "    AND e1.product = b.best_product\n",
    "LEFT JOIN {target_schema}.multi_product_explanations e2\n",
    "    ON b.axa_party_id = e2.axa_party_id\n",
    "    AND b.prediction_month = e2.prediction_month\n",
    "    AND e2.product = b.second_best_product\n",
    "ORDER BY combined_score DESC\n",
    "\"\"\"\n",
    "\n",
    "multi_product_strategy = spark.sql(multi_product_query)\n",
    "multi_count = multi_product_strategy.count()\n",
    "\n",
    "print(f\"Generated {multi_count:,} multi-product strategies\")\n",
    "\n",
    "if multi_count > 0:\n",
    "    strategy_table = f\"{target_schema}.multi_product_strategy\"\n",
    "    \n",
    "    multi_product_strategy.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "        .partitionBy(\"prediction_month\") \\\n",
    "        .saveAsTable(strategy_table)\n",
    "    \n",
    "    print(f\"Saved to: {strategy_table}\")\n",
    "    \n",
    "    print(\"\\nStrategy Breakdown:\")\n",
    "    strategy_counts = multi_product_strategy.groupBy('pitch_strategy').count().toPandas()\n",
    "    for _, row in strategy_counts.iterrows():\n",
    "        print(f\"  {row['pitch_strategy']}: {row['count']:,} clients\")\n",
    "    \n",
    "    print(\"\\n\uD83D\uDCCB Top 5 Multi-Product Opportunities:\")\n",
    "    print(\"=\" * 60)\n",
    "    display(multi_product_strategy.limit(5))\n",
    "else:\n",
    "    print(\"  WARNING: No multi-product opportunities found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec2c453b-b550-4924-be4e-d8697a03cbbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34775cdb-c38e-4e9c-acdf-c02a13b95b36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {
        "branch_code": "83",
        "min_score_threshold": "0.60",
        "prediction_month": "202510",
        "target_schema": "eda_smartlist.us_wealth_management_smartlist",
        "top_n_per_product": "500",
        "training_month": "202510",
        "wm_source_schema": "dl_tenants_daas.us_wealth_management"
       },
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPLAINABILITY & ADVISOR OUTPUTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nEXPLANATIONS GENERATED:\")\n",
    "print(\"-\" * 80)\n",
    "explanation_counts = df_explanations.groupby('product').size()\n",
    "for product, count in explanation_counts.items():\n",
    "    print(f\"  {product.title()}: {count:,} clients explained\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCB SMART LISTS CREATED:\")\n",
    "print(\"-\" * 80)\n",
    "smart_list_counts = smart_lists.groupBy('product').count().toPandas()\n",
    "for _, row in smart_list_counts.iterrows():\n",
    "    print(f\"  {row['product'].title()}: {row['count']:,} prospects with talking points\")\n",
    "\n",
    "if multi_count > 0:\n",
    "    print(f\"\\n\uD83D\uDD04 MULTI-PRODUCT STRATEGIES:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Total multi-product opportunities: {multi_count:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL OUTPUTS GENERATED\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nOutput Tables Created:\")\n",
    "print(f\"  1. {target_schema}.multi_product_explanations\")\n",
    "print(f\"  2. {target_schema}.advisor_smart_lists\")\n",
    "if multi_count > 0:\n",
    "    print(f\"  3. {target_schema}.multi_product_strategy\")\n",
    "\n",
    "print(\"\\nReady for Advisor Use!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1e39e8-e221-4594-a532-67a546d02e7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ✅ Notebook 05 Complete!\n",
    "\n",
    "### What We Accomplished:\n",
    "- ✅ Generated SHAP-based explanations for high-scoring clients\n",
    "- ✅ Converted feature importance into natural language talking points\n",
    "- ✅ Created advisor-ready smart lists with scores and reasoning\n",
    "- ✅ Identified multi-product bundle strategies\n",
    "- ✅ Saved all outputs to Delta tables\n",
    "\n",
    "### Output Tables:\n",
    "1. **`multi_product_explanations`**: SHAP values and feature importance\n",
    "2. **`advisor_smart_lists`**: Complete prospect lists with talking points\n",
    "3. **`multi_product_strategy`**: Multi-product bundle recommendations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d8e8dc5-bf68-4e0e-a06e-c79f3527cbeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14d953bb-f1e2-49ed-a259-fa2f19740184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Explainability_Advisor_Outputs",
   "widgets": {
    "branch_code": {
     "currentValue": "83",
     "nuid": "76b5f475-5ba7-4c4e-9fe1-d7590524aa44",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "83",
      "label": null,
      "name": "branch_code",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "83",
      "label": null,
      "name": "branch_code",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "min_score_threshold": {
     "currentValue": "0.60",
     "nuid": "85a14ecc-40fd-4109-b007-141e5daf643b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0.60",
      "label": null,
      "name": "min_score_threshold",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "0.60",
      "label": null,
      "name": "min_score_threshold",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "prediction_month": {
     "currentValue": "202510",
     "nuid": "e01417dc-fe6e-40bd-9ef4-73e547b41531",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "202510",
      "label": null,
      "name": "prediction_month",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "202510",
      "label": null,
      "name": "prediction_month",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_schema": {
     "currentValue": "eda_smartlist.us_wealth_management_smartlist",
     "nuid": "2d01a018-7f57-42cd-a3ac-705a0962631a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "eda_smartlist.us_wealth_management_smartlist",
      "label": null,
      "name": "target_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "eda_smartlist.us_wealth_management_smartlist",
      "label": null,
      "name": "target_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "top_n_per_product": {
     "currentValue": "500",
     "nuid": "5de78e78-6c8c-478f-8457-7582f48bb202",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "500",
      "label": null,
      "name": "top_n_per_product",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "500",
      "label": null,
      "name": "top_n_per_product",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "training_month": {
     "currentValue": "202510",
     "nuid": "3eddac40-6fb2-4cd3-9cea-a2f4b461bc8a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "202510",
      "label": null,
      "name": "training_month",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "202510",
      "label": null,
      "name": "training_month",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "wm_source_schema": {
     "currentValue": "dl_tenants_daas.us_wealth_management",
     "nuid": "75aaa405-e500-4975-85dc-ba65e507dc2f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dl_tenants_daas.us_wealth_management",
      "label": null,
      "name": "wm_source_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dl_tenants_daas.us_wealth_management",
      "label": null,
      "name": "wm_source_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}