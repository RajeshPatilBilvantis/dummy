{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prediction Reasoning Analysis\n",
        "\n",
        "This notebook provides detailed reasoning for ML predictions stored in `eda_smartlist.us_wealth_management_smartlist.ML_predictions_single_policy`.\n",
        "\n",
        "**Features:**\n",
        "- Why each product was recommended (top contributing features)\n",
        "- Key patterns influencing high confidence scores\n",
        "- Why other products were not predicted (comparative analysis)\n",
        "- Feature-level explanations using SHAP values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# IMPORTS AND CONFIGURATION\n",
        "# ============================================================================\n",
        "import os\n",
        "import pickle\n",
        "from typing import Dict, Any, List, Tuple\n",
        "\n",
        "import mlflow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "from mlflow.tracking import MlflowClient\n",
        "from pyspark.sql import SparkSession, Window\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Configuration\n",
        "PREDICTIONS_TABLE = \"eda_smartlist.us_wealth_management_smartlist.ML_predictions_single_policy\"\n",
        "SOURCE_TABLE = \"dl_tenants_daas.us_wealth_management.wealth_management_client_metrics\"\n",
        "MODEL_NAME = \"eda_smartlist.models.lgbm_model_hyperparameter_last2products\"\n",
        "MODEL_VERSION = \"1\"\n",
        "ARTIFACTS_LOCAL_PATH = \"/Workspace/EDA_workspace/RA Smart List/Smart List (NEW)/multiproduct_smartlist/Final_model_files/artifacts.pkl\"\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PREDICTION REASONING ANALYSIS\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PREPROCESSING HELPER FUNCTIONS (from training notebook)\n",
        "# ============================================================================\n",
        "\n",
        "def create_product_category_column(df):\n",
        "    \"\"\"Create product_category column from prod_lob, sub_product_level_1, and sub_product_level_2.\"\"\"\n",
        "    return df.withColumn(\n",
        "        \"product_category\",\n",
        "        F.when(F.col(\"prod_lob\") == \"LIFE\", \"LIFE_INSURANCE\")\n",
        "        .when(\n",
        "            F.col(\"sub_product_level_1\").isin(\n",
        "                \"VLI\", \"WL\", \"UL/IUL\", \"TERM\", \"PROTECTIVE PRODUCT\"\n",
        "            ),\n",
        "            \"LIFE_INSURANCE\",\n",
        "        )\n",
        "        .when(F.col(\"sub_product_level_2\").like(\"%LIFE%\"), \"LIFE_INSURANCE\")\n",
        "        .when(\n",
        "            F.col(\"sub_product_level_2\").isin(\n",
        "                \"VARIABLE UNIVERSAL LIFE\",\n",
        "                \"WHOLE LIFE\",\n",
        "                \"UNIVERSAL LIFE\",\n",
        "                \"INDEX UNIVERSAL LIFE\",\n",
        "                \"TERM PRODUCT\",\n",
        "                \"VARIABLE LIFE\",\n",
        "                \"SURVIVORSHIP WHOLE LIFE\",\n",
        "                \"MONY PROTECTIVE PRODUCT\",\n",
        "            ),\n",
        "            \"LIFE_INSURANCE\",\n",
        "        )\n",
        "        .when(\n",
        "            F.col(\"prod_lob\").isin(\"GROUP RETIREMENT\", \"INDIVIDUAL RETIREMENT\"),\n",
        "            \"RETIREMENT\",\n",
        "        )\n",
        "        .when(\n",
        "            F.col(\"sub_product_level_1\").isin(\n",
        "                \"EQUIVEST\",\n",
        "                \"RETIREMENT 401K\",\n",
        "                \"ACCUMULATOR\",\n",
        "                \"RETIREMENT CORNERSTONE\",\n",
        "                \"SCS\",\n",
        "                \"INVESTMENT EDGE\",\n",
        "            ),\n",
        "            \"RETIREMENT\",\n",
        "        )\n",
        "        .when(\n",
        "            (F.col(\"sub_product_level_2\").like(\"%403B%\"))\n",
        "            | (F.col(\"sub_product_level_2\").like(\"%401%\"))\n",
        "            | (F.col(\"sub_product_level_2\").like(\"%IRA%\"))\n",
        "            | (F.col(\"sub_product_level_2\").like(\"%SEP%\")),\n",
        "            \"RETIREMENT\",\n",
        "        )\n",
        "        .when(F.col(\"prod_lob\") == \"BROKER DEALER\", \"INVESTMENT\")\n",
        "        .when(\n",
        "            F.col(\"sub_product_level_1\").isin(\n",
        "                \"INVESTMENT PRODUCT - DIRECT\",\n",
        "                \"INVESTMENT PRODUCT - BROKERAGE\",\n",
        "                \"INVESTMENT PRODUCT - ADVISORY\",\n",
        "                \"DIRECT\",\n",
        "                \"BROKERAGE\",\n",
        "                \"ADVISORY\",\n",
        "                \"CASH SOLICITOR\",\n",
        "            ),\n",
        "            \"INVESTMENT\",\n",
        "        )\n",
        "        .when(\n",
        "            (F.col(\"sub_product_level_2\").like(\"%Investment%\"))\n",
        "            | (F.col(\"sub_product_level_2\").like(\"%Brokerage%\"))\n",
        "            | (F.col(\"sub_product_level_2\").like(\"%Advisory%\")),\n",
        "            \"INVESTMENT\",\n",
        "        )\n",
        "        .when(F.col(\"prod_lob\") == \"NETWORK\", \"NETWORK_PRODUCTS\")\n",
        "        .when(\n",
        "            (F.col(\"sub_product_level_1\") == \"NETWORK PRODUCTS\")\n",
        "            | (F.col(\"sub_product_level_2\") == \"NETWORK PRODUCTS\"),\n",
        "            \"NETWORK_PRODUCTS\",\n",
        "        )\n",
        "        .when(\n",
        "            (F.col(\"prod_lob\") == \"OTHERS\") & (F.col(\"sub_product_level_1\") == \"HAS\"),\n",
        "            \"DISABILITY\",\n",
        "        )\n",
        "        .when(F.col(\"sub_product_level_2\") == \"HAS - DISABILITY\", \"DISABILITY\")\n",
        "        .when(F.col(\"prod_lob\") == \"OTHERS\", \"HEALTH\")\n",
        "        .when(F.col(\"sub_product_level_2\") == \"GROUP HEALTH PRODUCTS\", \"HEALTH\")\n",
        "        .otherwise(\"OTHER\"),\n",
        "    )\n",
        "\n",
        "\n",
        "def add_asset_allocation_ratios(df):\n",
        "    \"\"\"Add asset allocation ratio features based on wealth metrics.\"\"\"\n",
        "    df = df.withColumn(\n",
        "        \"stock_allocation_ratio\",\n",
        "        F.col(\"wc_assetmix_stocks\")\n",
        "        / F.when(F.col(\"wc_total_assets\") != 0, F.col(\"wc_total_assets\")).otherwise(\n",
        "            F.lit(None)\n",
        "        ),\n",
        "    )\n",
        "    df = df.withColumn(\n",
        "        \"bond_allocation_ratio\",\n",
        "        F.col(\"wc_assetmix_bonds\")\n",
        "        / F.when(F.col(\"wc_total_assets\") != 0, F.col(\"wc_total_assets\")).otherwise(\n",
        "            F.lit(None)\n",
        "        ),\n",
        "    )\n",
        "    df = df.withColumn(\n",
        "        \"annuity_allocation_ratio\",\n",
        "        F.col(\"wc_assetmix_annuity\")\n",
        "        / F.when(F.col(\"wc_total_assets\") != 0, F.col(\"wc_total_assets\")).otherwise(\n",
        "            F.lit(None)\n",
        "        ),\n",
        "    )\n",
        "    df = df.withColumn(\n",
        "        \"mutual_fund_allocation_ratio\",\n",
        "        F.col(\"wc_assetmix_mutual_funds\")\n",
        "        / F.when(F.col(\"wc_total_assets\") != 0, F.col(\"wc_total_assets\")).otherwise(\n",
        "            F.lit(None)\n",
        "        ),\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "def impute_missing_values(df, categorical_cols, categorical_mode=\"UNKNOWN\"):\n",
        "    \"\"\"Impute missing values in DataFrame.\"\"\"\n",
        "    numeric_cols = [\n",
        "        \"first_acct_val_amt\",\n",
        "        \"first_face_amt\",\n",
        "        \"first_cash_val_amt\",\n",
        "        \"wc_total_assets\",\n",
        "        \"wc_assetmix_stocks\",\n",
        "        \"wc_assetmix_bonds\",\n",
        "        \"wc_assetmix_mutual_funds\",\n",
        "        \"wc_assetmix_annuity\",\n",
        "        \"wc_assetmix_deposits\",\n",
        "        \"wc_assetmix_other_assets\",\n",
        "        \"psn_age\",\n",
        "        \"stock_allocation_ratio\",\n",
        "        \"bond_allocation_ratio\",\n",
        "        \"annuity_allocation_ratio\",\n",
        "        \"mutual_fund_allocation_ratio\",\n",
        "        \"age_at_first_policy\",\n",
        "        \"years_to_second_policy\",\n",
        "    ]\n",
        "\n",
        "    # Median imputation for numeric columns\n",
        "    fill_numeric = {}\n",
        "    for c in numeric_cols:\n",
        "        if c in df.columns:\n",
        "            try:\n",
        "                median_val = df.approxQuantile(c, [0.5], 0.001)[0]\n",
        "            except Exception:\n",
        "                median_val = 0.0\n",
        "            if median_val is None:\n",
        "                median_val = 0.0\n",
        "            fill_numeric[c] = float(median_val)\n",
        "\n",
        "    result_df = df.fillna(fill_numeric)\n",
        "\n",
        "    # Mode imputation for categoricals\n",
        "    for c in categorical_cols:\n",
        "        if c in result_df.columns:\n",
        "            try:\n",
        "                mode_row = (\n",
        "                    result_df.groupBy(c)\n",
        "                    .count()\n",
        "                    .orderBy(F.desc(\"count\"))\n",
        "                    .first()\n",
        "                )\n",
        "                mode_val = (\n",
        "                    mode_row[0]\n",
        "                    if mode_row and mode_row[0] is not None\n",
        "                    else categorical_mode\n",
        "                )\n",
        "            except Exception:\n",
        "                mode_val = categorical_mode\n",
        "\n",
        "            result_df = result_df.withColumn(\n",
        "                c,\n",
        "                F.when(F.col(c).isNull(), F.lit(mode_val)).otherwise(F.col(c)),\n",
        "            )\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "def encode_categorical_features(df, categorical_cols, spark_context, categorical_mappings=None):\n",
        "    \"\"\"Encode categorical features to integer indices.\"\"\"\n",
        "    if categorical_mappings is None:\n",
        "        categorical_mappings = {}\n",
        "\n",
        "    result_df = df\n",
        "\n",
        "    for c in categorical_cols:\n",
        "        if c in categorical_mappings:\n",
        "            mapping = categorical_mappings[c]\n",
        "            b = spark_context.broadcast(mapping)\n",
        "            result_df = result_df.withColumn(\n",
        "                c + \"_idx\",\n",
        "                F.udf(lambda s: int(b.value.get(str(s), 0)), IntegerType())(\n",
        "                    F.coalesce(F.col(c).cast(\"string\"), F.lit(\"UNKNOWN\"))\n",
        "                ),\n",
        "            )\n",
        "        else:\n",
        "            vals = [r[0] for r in result_df.select(c).distinct().collect()]\n",
        "            mapping = {v: i for i, v in enumerate(sorted([str(x) for x in vals]))}\n",
        "            categorical_mappings[c] = mapping\n",
        "\n",
        "            b = spark_context.broadcast(mapping)\n",
        "            result_df = result_df.withColumn(\n",
        "                c + \"_idx\",\n",
        "                F.udf(lambda s: int(b.value.get(str(s), 0)), IntegerType())(\n",
        "                    F.coalesce(F.col(c).cast(\"string\"), F.lit(\"UNKNOWN\"))\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    return result_df, categorical_mappings\n",
        "\n",
        "\n",
        "def get_feature_columns():\n",
        "    \"\"\"Get the list of feature columns in the correct order.\"\"\"\n",
        "    return [\n",
        "        \"first_acct_val_amt\",\n",
        "        \"first_face_amt\",\n",
        "        \"first_cash_val_amt\",\n",
        "        \"wc_total_assets\",\n",
        "        \"wc_assetmix_stocks\",\n",
        "        \"wc_assetmix_bonds\",\n",
        "        \"wc_assetmix_mutual_funds\",\n",
        "        \"wc_assetmix_annuity\",\n",
        "        \"wc_assetmix_deposits\",\n",
        "        \"wc_assetmix_other_assets\",\n",
        "        \"psn_age\",\n",
        "        \"stock_allocation_ratio\",\n",
        "        \"bond_allocation_ratio\",\n",
        "        \"annuity_allocation_ratio\",\n",
        "        \"mutual_fund_allocation_ratio\",\n",
        "        \"age_at_first_policy\",\n",
        "        \"years_to_second_policy\",\n",
        "        \"first_product_category_idx\",\n",
        "        \"client_seg_idx\",\n",
        "        \"client_seg_1_idx\",\n",
        "        \"aum_band_idx\",\n",
        "        \"channel_idx\",\n",
        "        \"agent_segment_idx\",\n",
        "        \"branchoffice_code_idx\",\n",
        "        \"season_of_first_policy_idx\",\n",
        "    ]\n",
        "\n",
        "\n",
        "print(\"âœ“ Preprocessing functions loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MODEL AND ARTIFACTS LOADING\n",
        "# ============================================================================\n",
        "\n",
        "def load_model_and_artifacts() -> Dict[str, Any]:\n",
        "    \"\"\"Load the LightGBM model and its preprocessing artifacts.\"\"\"\n",
        "    client = MlflowClient()\n",
        "\n",
        "    # Load registered LightGBM model\n",
        "    model_uri = f\"models:/{MODEL_NAME}/{MODEL_VERSION}\"\n",
        "    print(f\"Loading model from {model_uri}...\")\n",
        "    lgbm_model = mlflow.lightgbm.load_model(model_uri)\n",
        "    print(\"âœ“ Model loaded successfully\")\n",
        "\n",
        "    # Try to load artifacts from local path first\n",
        "    artifacts = None\n",
        "    if ARTIFACTS_LOCAL_PATH and os.path.exists(ARTIFACTS_LOCAL_PATH):\n",
        "        print(f\"Loading artifacts from local path: {ARTIFACTS_LOCAL_PATH}\")\n",
        "        with open(ARTIFACTS_LOCAL_PATH, \"rb\") as f:\n",
        "            artifacts = pickle.load(f)\n",
        "        print(\"âœ“ Artifacts loaded from local path\")\n",
        "    else:\n",
        "        # Try MLflow run artifacts\n",
        "        try:\n",
        "            mv = client.get_model_version(name=MODEL_NAME, version=MODEL_VERSION)\n",
        "            run_id = mv.run_id\n",
        "            print(f\"Attempting to download artifacts from run {run_id}...\")\n",
        "            artifacts_dir = mlflow.artifacts.download_artifacts(\n",
        "                artifact_uri=f\"runs:/{run_id}/artifacts\"\n",
        "            )\n",
        "            artifacts_path = os.path.join(artifacts_dir, \"artifacts.pkl\")\n",
        "            if os.path.exists(artifacts_path):\n",
        "                with open(artifacts_path, \"rb\") as f:\n",
        "                    artifacts = pickle.load(f)\n",
        "                print(\"âœ“ Artifacts loaded from MLflow\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš  Could not load artifacts from MLflow: {e}\")\n",
        "\n",
        "    if artifacts is None:\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not find artifacts.pkl. Please ensure it exists at {ARTIFACTS_LOCAL_PATH} \"\n",
        "            \"or is available in MLflow.\"\n",
        "        )\n",
        "\n",
        "    prod2id = artifacts.get(\"prod2id\")\n",
        "    id2prod = artifacts.get(\"id2prod\")\n",
        "    label_map = artifacts.get(\"label_map\")\n",
        "    num_classes = artifacts.get(\"num_classes\")\n",
        "    categorical_mappings = artifacts.get(\"categorical_mappings\")\n",
        "    feature_cols = artifacts.get(\"feature_cols\") or get_feature_columns()\n",
        "\n",
        "    if prod2id is None or id2prod is None:\n",
        "        raise ValueError(\"prod2id/id2prod mappings missing in artifacts.pkl\")\n",
        "    if label_map is None:\n",
        "        raise ValueError(\"label_map missing in artifacts.pkl\")\n",
        "\n",
        "    print(\"âœ“ All artifacts loaded successfully\")\n",
        "    return {\n",
        "        \"model\": lgbm_model,\n",
        "        \"prod2id\": prod2id,\n",
        "        \"id2prod\": id2prod,\n",
        "        \"label_map\": label_map,\n",
        "        \"num_classes\": num_classes,\n",
        "        \"categorical_mappings\": categorical_mappings,\n",
        "        \"feature_cols\": feature_cols,\n",
        "    }\n",
        "\n",
        "\n",
        "# Load model and artifacts\n",
        "ctx = load_model_and_artifacts()\n",
        "model = ctx[\"model\"]\n",
        "id2prod = ctx[\"id2prod\"]\n",
        "label_map = ctx[\"label_map\"]\n",
        "categorical_mappings = ctx[\"categorical_mappings\"]\n",
        "feature_cols = ctx[\"feature_cols\"]\n",
        "\n",
        "# Create inverse label map for predictions\n",
        "inv_label_map = {v: k for k, v in label_map.items()}\n",
        "final_id2prod = {\n",
        "    model_id: id2prod[orig_id]\n",
        "    for model_id, orig_id in inv_label_map.items()\n",
        "    if orig_id in id2prod\n",
        "}\n",
        "\n",
        "print(f\"âœ“ Model context loaded: {len(feature_cols)} features, {len(final_id2prod)} product classes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# LOAD PREDICTIONS FROM TABLE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"LOADING PREDICTIONS FROM TABLE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load predictions table\n",
        "df_predictions = spark.table(PREDICTIONS_TABLE)\n",
        "\n",
        "# Optionally filter by business_month or other criteria\n",
        "# df_predictions = df_predictions.filter(F.col(\"business_month\") == \"202512\")\n",
        "\n",
        "print(f\"âœ“ Loaded {df_predictions.count():,} predictions from table\")\n",
        "print(f\"\\n  Columns in predictions table:\")\n",
        "for col in df_predictions.columns:\n",
        "    print(f\"    - {col}\")\n",
        "\n",
        "# Convert to pandas for easier manipulation (sample if too large)\n",
        "MAX_SAMPLES = 10000  # Adjust based on your needs\n",
        "pred_count = df_predictions.count()\n",
        "if pred_count > MAX_SAMPLES:\n",
        "    print(f\"\\nâš  Large dataset ({pred_count:,} records). Sampling {MAX_SAMPLES:,} records for analysis...\")\n",
        "    df_predictions_sample = df_predictions.sample(fraction=MAX_SAMPLES / pred_count, seed=42)\n",
        "    pred_pd = df_predictions_sample.toPandas()\n",
        "else:\n",
        "    pred_pd = df_predictions.toPandas()\n",
        "\n",
        "print(f\"âœ“ Working with {len(pred_pd):,} predictions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# RECONSTRUCT FEATURE DATA FOR SHAP ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"RECONSTRUCTING FEATURE DATA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get cont_ids from predictions\n",
        "cont_ids = pred_pd[\"cont_id\"].unique().tolist()\n",
        "print(f\"âœ“ Found {len(cont_ids):,} unique cont_ids\")\n",
        "\n",
        "# Load source data for these cont_ids\n",
        "df_raw = spark.table(SOURCE_TABLE)\n",
        "df_raw = df_raw.filter(F.col(\"cont_id\").isin(cont_ids))\n",
        "df_raw = df_raw.filter(F.col(\"policy_status\") == \"Active\")\n",
        "\n",
        "# Create product_category\n",
        "df_raw = create_product_category_column(df_raw)\n",
        "\n",
        "# Filter to single-policy clients (matching production logic)\n",
        "df_events = df_raw.select(\n",
        "    \"cont_id\",\n",
        "    \"axa_party_id\",\n",
        "    \"product_category\",\n",
        "    \"register_date\",\n",
        "    \"isrd_brth_date\",\n",
        "    \"acct_val_amt\",\n",
        "    \"face_amt\",\n",
        "    \"cash_val_amt\",\n",
        "    \"wc_total_assets\",\n",
        "    \"wc_assetmix_stocks\",\n",
        "    \"wc_assetmix_bonds\",\n",
        "    \"wc_assetmix_mutual_funds\",\n",
        "    \"wc_assetmix_annuity\",\n",
        "    \"wc_assetmix_deposits\",\n",
        "    \"wc_assetmix_other_assets\",\n",
        "    \"psn_age\",\n",
        "    \"client_seg\",\n",
        "    \"client_seg_1\",\n",
        "    \"aum_band\",\n",
        "    \"channel\",\n",
        "    \"agent_segment\",\n",
        "    \"branchoffice_code\",\n",
        "    \"policy_no\",\n",
        ").filter(\n",
        "    (F.col(\"axa_party_id\").isNotNull())\n",
        "    & (F.col(\"cont_id\").isNotNull())\n",
        "    & (F.col(\"register_date\").isNotNull())\n",
        "    & (F.col(\"product_category\").isNotNull())\n",
        ")\n",
        "\n",
        "# Count policies per axa_party_id\n",
        "party_counts = (\n",
        "    df_events.groupBy(\"axa_party_id\")\n",
        "    .agg(F.countDistinct(\"policy_no\").alias(\"policy_count\"))\n",
        "    .filter(F.col(\"policy_count\") == 1)  # Exactly 1 policy\n",
        ")\n",
        "\n",
        "# Filter to single-policy clients\n",
        "df_single_policy = df_events.join(\n",
        "    party_counts.select(\"axa_party_id\"), on=\"axa_party_id\", how=\"inner\"\n",
        ").dropDuplicates([\"cont_id\"])  # Remove duplicates\n",
        "\n",
        "print(f\"âœ“ Filtered to {df_single_policy.count():,} single-policy client records\")\n",
        "\n",
        "# Convert dates\n",
        "df_single_policy = df_single_policy.withColumn(\"register_ts\", F.to_timestamp(\"register_date\"))\n",
        "df_single_policy = df_single_policy.withColumn(\"birth_ts\", F.to_timestamp(\"isrd_brth_date\"))\n",
        "\n",
        "# Create \"first policy\" features\n",
        "df_first = df_single_policy.select(\n",
        "    \"cont_id\",\n",
        "    \"axa_party_id\",\n",
        "    F.col(\"product_category\").alias(\"first_product_category\"),\n",
        "    F.col(\"register_ts\").alias(\"first_register_ts\"),\n",
        "    \"birth_ts\",\n",
        "    F.col(\"acct_val_amt\").alias(\"first_acct_val_amt\"),\n",
        "    F.col(\"face_amt\").alias(\"first_face_amt\"),\n",
        "    F.col(\"cash_val_amt\").alias(\"first_cash_val_amt\"),\n",
        "    \"wc_total_assets\",\n",
        "    \"wc_assetmix_stocks\",\n",
        "    \"wc_assetmix_bonds\",\n",
        "    \"wc_assetmix_mutual_funds\",\n",
        "    \"wc_assetmix_annuity\",\n",
        "    \"wc_assetmix_deposits\",\n",
        "    \"wc_assetmix_other_assets\",\n",
        "    \"psn_age\",\n",
        "    \"client_seg\",\n",
        "    \"client_seg_1\",\n",
        "    \"aum_band\",\n",
        "    \"channel\",\n",
        "    \"agent_segment\",\n",
        "    \"branchoffice_code\",\n",
        ")\n",
        "\n",
        "# Add temporal features\n",
        "df_first = df_first.withColumn(\"second_register_ts\", F.current_timestamp())\n",
        "df_first = add_asset_allocation_ratios(df_first)\n",
        "\n",
        "df_first = df_first.withColumn(\n",
        "    \"season_of_first_policy\",\n",
        "    F.when(F.month(\"first_register_ts\").between(1, 3), \"Q1\")\n",
        "    .when(F.month(\"first_register_ts\").between(4, 6), \"Q2\")\n",
        "    .when(F.month(\"first_register_ts\").between(7, 9), \"Q3\")\n",
        "    .when(F.month(\"first_register_ts\").between(10, 12), \"Q4\")\n",
        "    .otherwise(\"Unknown\"),\n",
        ")\n",
        "\n",
        "df_first = df_first.withColumn(\n",
        "    \"age_at_first_policy\",\n",
        "    F.datediff(F.col(\"first_register_ts\"), F.col(\"birth_ts\")) / 365.25,\n",
        ")\n",
        "\n",
        "df_first = df_first.withColumn(\n",
        "    \"years_to_second_policy\",\n",
        "    F.datediff(F.col(\"second_register_ts\"), F.col(\"first_register_ts\")) / 365.25,\n",
        ")\n",
        "\n",
        "# Impute missing values\n",
        "categorical_cols = [\n",
        "    \"first_product_category\",\n",
        "    \"client_seg\",\n",
        "    \"client_seg_1\",\n",
        "    \"aum_band\",\n",
        "    \"channel\",\n",
        "    \"agent_segment\",\n",
        "    \"branchoffice_code\",\n",
        "    \"season_of_first_policy\",\n",
        "]\n",
        "\n",
        "df_first = impute_missing_values(df_first, categorical_cols)\n",
        "\n",
        "# Encode categorical features\n",
        "df_encoded, _ = encode_categorical_features(\n",
        "    df_first, categorical_cols, spark.sparkContext, categorical_mappings=categorical_mappings\n",
        ")\n",
        "\n",
        "# Convert to pandas\n",
        "select_cols = [\"cont_id\", \"axa_party_id\"] + feature_cols\n",
        "features_pd = df_encoded.select(select_cols).toPandas()\n",
        "features_pd.fillna(0, inplace=True)\n",
        "\n",
        "print(f\"âœ“ Reconstructed features for {len(features_pd):,} clients\")\n",
        "print(f\"âœ“ Feature columns: {len(feature_cols)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MERGE PREDICTIONS WITH FEATURES\n",
        "# ============================================================================\n",
        "\n",
        "# Merge predictions with features\n",
        "pred_with_features = pred_pd.merge(\n",
        "    features_pd[[\"cont_id\"] + feature_cols], on=\"cont_id\", how=\"inner\"\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Merged predictions with features: {len(pred_with_features):,} records\")\n",
        "\n",
        "# Ensure we have feature data for all predictions\n",
        "if len(pred_with_features) < len(pred_pd):\n",
        "    missing = len(pred_pd) - len(pred_with_features)\n",
        "    print(f\"âš  Warning: {missing} predictions missing feature data (will be skipped)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SHAP ANALYSIS FOR REASONING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPUTING SHAP VALUES FOR REASONING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Prepare feature matrix\n",
        "X = pred_with_features[feature_cols].values\n",
        "\n",
        "print(f\"âœ“ Feature matrix shape: {X.shape}\")\n",
        "\n",
        "# Compute SHAP values\n",
        "print(\"Computing SHAP values (this may take a while)...\")\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X)\n",
        "\n",
        "# Get predictions to determine which class SHAP values to use\n",
        "pred_probs = model.predict(X)\n",
        "pred_class_ids = np.argmax(pred_probs, axis=1)\n",
        "\n",
        "# Handle SHAP output format\n",
        "if isinstance(shap_values, list):\n",
        "    # List of arrays: one array per class\n",
        "    shap_array = np.array([shap_values[pred_class_ids[i]][i] for i in range(len(pred_with_features))])\n",
        "elif isinstance(shap_values, np.ndarray) and len(shap_values.shape) == 3:\n",
        "    # 3D array: (n_samples, n_features, n_classes) or (n_classes, n_samples, n_features)\n",
        "    n_samples, n_features, n_classes = shap_values.shape\n",
        "    if n_samples == len(pred_with_features) and n_features == len(feature_cols):\n",
        "        shap_array = np.array([shap_values[i, :, pred_class_ids[i]] for i in range(n_samples)])\n",
        "    else:\n",
        "        shap_values = shap_values.transpose(1, 0, 2)  # Try transpose\n",
        "        shap_array = np.array([shap_values[i, :, pred_class_ids[i]] for i in range(len(pred_with_features))])\n",
        "else:\n",
        "    shap_array = shap_values\n",
        "\n",
        "print(f\"âœ“ Computed SHAP values: {shap_array.shape}\")\n",
        "\n",
        "# Also compute SHAP values for all classes (for comparative analysis)\n",
        "if isinstance(shap_values, list):\n",
        "    shap_all_classes = shap_values  # Already in list format\n",
        "elif isinstance(shap_values, np.ndarray) and len(shap_values.shape) == 3:\n",
        "    # Convert to list format for easier access\n",
        "    if shap_values.shape[0] == len(pred_with_features):\n",
        "        shap_all_classes = [shap_values[:, :, i] for i in range(shap_values.shape[2])]\n",
        "    else:\n",
        "        shap_all_classes = [shap_values[i, :, :].T for i in range(shap_values.shape[0])]\n",
        "else:\n",
        "    shap_all_classes = [shap_array]  # Single class\n",
        "\n",
        "print(f\"âœ“ SHAP analysis complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# GENERATE DETAILED REASONING FOR EACH PREDICTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GENERATING DETAILED REASONING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "\n",
        "def format_feature_value(feat_name: str, feat_value: Any, categorical_mappings: Dict) -> str:\n",
        "    \"\"\"Format feature value for human-readable display.\"\"\"\n",
        "    if feat_name.endswith(\"_idx\"):\n",
        "        base_name = feat_name.replace(\"_idx\", \"\")\n",
        "        if base_name in categorical_mappings:\n",
        "            mapping = categorical_mappings[base_name]\n",
        "            inv_mapping = {v: k for k, v in mapping.items()}\n",
        "            try:\n",
        "                idx_val = int(float(feat_value))\n",
        "                return inv_mapping.get(idx_val, str(feat_value))\n",
        "            except:\n",
        "                return str(feat_value)\n",
        "    \n",
        "    # Format numeric values\n",
        "    if isinstance(feat_value, (int, float)):\n",
        "        if \"ratio\" in feat_name or \"allocation\" in feat_name:\n",
        "            return f\"{feat_value:.1%}\"\n",
        "        elif \"age\" in feat_name or \"years\" in feat_name:\n",
        "            return f\"{feat_value:.1f}\"\n",
        "        elif \"amt\" in feat_name or \"assets\" in feat_name or \"mix\" in feat_name:\n",
        "            return f\"${feat_value:,.0f}\"\n",
        "        else:\n",
        "            return f\"{feat_value:,.2f}\"\n",
        "    \n",
        "    return str(feat_value)\n",
        "\n",
        "\n",
        "def generate_reasoning(\n",
        "    cont_id: str,\n",
        "    pred_product: str,\n",
        "    pred_prob: float,\n",
        "    all_probs: np.ndarray,\n",
        "    feature_values: pd.Series,\n",
        "    shap_contributions: np.ndarray,\n",
        "    shap_all_classes: List[np.ndarray],\n",
        "    feature_cols: List[str],\n",
        "    final_id2prod: Dict[int, str],\n",
        "    categorical_mappings: Dict,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Generate detailed reasoning for a single prediction.\"\"\"\n",
        "    \n",
        "    # Get top contributing features (positive and negative)\n",
        "    feature_contributions = list(zip(feature_cols, shap_contributions))\n",
        "    feature_contributions.sort(key=lambda x: abs(float(x[1])), reverse=True)\n",
        "    \n",
        "    top_positive = [fc for fc in feature_contributions if fc[1] > 0][:5]\n",
        "    top_negative = [fc for fc in feature_contributions if fc[1] < 0][:5]\n",
        "    \n",
        "    # Get top 3 alternative products (by probability)\n",
        "    prob_with_class = [(all_probs[i], i) for i in range(len(all_probs))]\n",
        "    prob_with_class.sort(reverse=True)\n",
        "    \n",
        "    top_alternatives = []\n",
        "    for prob, class_id in prob_with_class[1:4]:  # Skip the predicted one\n",
        "        product_name = final_id2prod.get(class_id, f\"CLASS_{class_id}\")\n",
        "        top_alternatives.append({\n",
        "            \"product\": product_name,\n",
        "            \"probability\": float(prob),\n",
        "            \"class_id\": int(class_id),\n",
        "        })\n",
        "    \n",
        "    # Get SHAP contributions for top alternative\n",
        "    alternative_reasons = []\n",
        "    if top_alternatives and len(shap_all_classes) > 1:\n",
        "        alt_class_id = top_alternatives[0][\"class_id\"]\n",
        "        if alt_class_id < len(shap_all_classes):\n",
        "            # Get SHAP values for alternative class\n",
        "            if isinstance(shap_all_classes[alt_class_id], np.ndarray):\n",
        "                if len(shap_all_classes[alt_class_id].shape) == 2:\n",
        "                    alt_shap = shap_all_classes[alt_class_id][0]  # First sample\n",
        "                else:\n",
        "                    alt_shap = shap_all_classes[alt_class_id]\n",
        "            else:\n",
        "                alt_shap = shap_all_classes[alt_class_id]\n",
        "            \n",
        "            # Features that favor alternative over predicted\n",
        "            diff_contributions = [\n",
        "                (feature_cols[i], alt_shap[i] - shap_contributions[i]) \n",
        "                for i in range(len(feature_cols))\n",
        "            ]\n",
        "            diff_contributions.sort(key=lambda x: abs(float(x[1])), reverse=True)\n",
        "            \n",
        "            alternative_reasons = [\n",
        "                {\n",
        "                    \"feature\": fc[0],\n",
        "                    \"value\": format_feature_value(fc[0], feature_values[fc[0]], categorical_mappings),\n",
        "                    \"contribution_diff\": float(fc[1]),\n",
        "                }\n",
        "                for fc in diff_contributions[:3]\n",
        "            ]\n",
        "    \n",
        "    # Build reasoning text\n",
        "    reasoning_parts = []\n",
        "    \n",
        "    # Why this product was recommended\n",
        "    reasoning_parts.append(\"## Why This Product Was Recommended\")\n",
        "    reasoning_parts.append(f\"The model predicts **{pred_product}** with {pred_prob:.1%} confidence.\")\n",
        "    reasoning_parts.append(\"\\n**Top Contributing Features (Supporting Prediction):**\")\n",
        "    \n",
        "    for i, (feat_name, shap_val) in enumerate(top_positive[:5], 1):\n",
        "        feat_value = format_feature_value(feat_name, feature_values[feat_name], categorical_mappings)\n",
        "        impact = \"ðŸ”¥ High\" if abs(shap_val) > 0.1 else \"â­ Moderate\" if abs(shap_val) > 0.05 else \"ðŸ“Š Low\"\n",
        "        reasoning_parts.append(f\"{i}. **{feat_name}** = {feat_value} ({impact} impact: +{shap_val:.4f})\")\n",
        "    \n",
        "    if top_negative:\n",
        "        reasoning_parts.append(\"\\n**Features Reducing Confidence:**\")\n",
        "        for i, (feat_name, shap_val) in enumerate(top_negative[:3], 1):\n",
        "            feat_value = format_feature_value(feat_name, feature_values[feat_name], categorical_mappings)\n",
        "            reasoning_parts.append(f\"{i}. **{feat_name}** = {feat_value} (reducing confidence: {shap_val:.4f})\")\n",
        "    \n",
        "    # Confidence score explanation\n",
        "    reasoning_parts.append(\"\\n## Confidence Score Analysis\")\n",
        "    if pred_prob >= 0.7:\n",
        "        reasoning_parts.append(f\"**High Confidence ({pred_prob:.1%})**: Strong feature alignment with {pred_product} patterns.\")\n",
        "    elif pred_prob >= 0.5:\n",
        "        reasoning_parts.append(f\"**Moderate Confidence ({pred_prob:.1%})**: Reasonable alignment, but some uncertainty remains.\")\n",
        "    else:\n",
        "        reasoning_parts.append(f\"**Low Confidence ({pred_prob:.1%})**: Weak feature alignment; consider reviewing alternative products.\")\n",
        "    \n",
        "    # Why other products weren't predicted\n",
        "    reasoning_parts.append(\"\\n## Why Other Products Were Not Predicted\")\n",
        "    reasoning_parts.append(\"**Top Alternative Products:**\")\n",
        "    \n",
        "    for alt in top_alternatives:\n",
        "        prob_diff = pred_prob - alt[\"probability\"]\n",
        "        reasoning_parts.append(f\"- **{alt['product']}**: {alt['probability']:.1%} probability (gap: {prob_diff:.1%})\")\n",
        "    \n",
        "    if alternative_reasons:\n",
        "        reasoning_parts.append(\"\\n**Key Differences from Top Alternative:**\")\n",
        "        for reason in alternative_reasons[:3]:\n",
        "            reasoning_parts.append(f\"- **{reason['feature']}** = {reason['value']} (contribution difference: {reason['contribution_diff']:+.4f})\")\n",
        "    \n",
        "    return {\n",
        "        \"cont_id\": cont_id,\n",
        "        \"predicted_product\": pred_product,\n",
        "        \"prediction_confidence\": pred_prob,\n",
        "        \"reasoning_text\": \"\\n\".join(reasoning_parts),\n",
        "        \"top_positive_features\": [\n",
        "            {\n",
        "                \"feature\": feat_name,\n",
        "                \"value\": format_feature_value(feat_name, feature_values[feat_name], categorical_mappings),\n",
        "                \"shap_contribution\": float(shap_val),\n",
        "            }\n",
        "            for feat_name, shap_val in top_positive[:5]\n",
        "        ],\n",
        "        \"top_negative_features\": [\n",
        "            {\n",
        "                \"feature\": feat_name,\n",
        "                \"value\": format_feature_value(feat_name, feature_values[feat_name], categorical_mappings),\n",
        "                \"shap_contribution\": float(shap_val),\n",
        "            }\n",
        "            for feat_name, shap_val in top_negative[:3]\n",
        "        ],\n",
        "        \"top_alternatives\": top_alternatives,\n",
        "        \"alternative_reasons\": alternative_reasons,\n",
        "    }\n",
        "\n",
        "\n",
        "# Generate reasoning for all predictions\n",
        "reasoning_list = []\n",
        "print(f\"Generating reasoning for {len(pred_with_features):,} predictions...\")\n",
        "\n",
        "for idx in range(len(pred_with_features)):\n",
        "    if (idx + 1) % 1000 == 0:\n",
        "        print(f\"  Processed {idx + 1:,} / {len(pred_with_features):,}...\")\n",
        "    \n",
        "    row = pred_with_features.iloc[idx]\n",
        "    \n",
        "    # Get SHAP values for this prediction\n",
        "    if isinstance(shap_all_classes, list) and len(shap_all_classes) > 0:\n",
        "        pred_class_id = int(row[\"pred_class_id\"]) if \"pred_class_id\" in row else pred_class_ids[idx]\n",
        "        if pred_class_id < len(shap_all_classes):\n",
        "            if isinstance(shap_all_classes[pred_class_id], np.ndarray):\n",
        "                if len(shap_all_classes[pred_class_id].shape) == 2:\n",
        "                    shap_for_pred = shap_all_classes[pred_class_id][idx]\n",
        "                else:\n",
        "                    shap_for_pred = shap_all_classes[pred_class_id]\n",
        "            else:\n",
        "                shap_for_pred = shap_array[idx]\n",
        "        else:\n",
        "            shap_for_pred = shap_array[idx]\n",
        "    else:\n",
        "        shap_for_pred = shap_array[idx]\n",
        "    \n",
        "    # Get all probabilities\n",
        "    all_probs = pred_probs[idx]\n",
        "    \n",
        "    reasoning = generate_reasoning(\n",
        "        cont_id=str(row[\"cont_id\"]),\n",
        "        pred_product=str(row.get(\"predicted_product\", row.get(\"pred_product\", \"UNKNOWN\"))),\n",
        "        pred_prob=float(row[\"pred_prob\"]),\n",
        "        all_probs=all_probs,\n",
        "        feature_values=row[feature_cols],\n",
        "        shap_contributions=shap_for_pred,\n",
        "        shap_all_classes=shap_all_classes,\n",
        "        feature_cols=feature_cols,\n",
        "        final_id2prod=final_id2prod,\n",
        "        categorical_mappings=categorical_mappings,\n",
        "    )\n",
        "    \n",
        "    reasoning_list.append(reasoning)\n",
        "\n",
        "print(f\"âœ“ Generated reasoning for {len(reasoning_list):,} predictions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CREATE REASONING DATAFRAME AND SAVE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CREATING REASONING OUTPUT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create DataFrame with reasoning\n",
        "reasoning_df = pd.DataFrame(reasoning_list)\n",
        "\n",
        "# Merge back with original predictions\n",
        "final_reasoning_df = pred_pd.merge(\n",
        "    reasoning_df[[\"cont_id\", \"reasoning_text\", \"top_positive_features\", \"top_negative_features\", \"top_alternatives\", \"alternative_reasons\"]],\n",
        "    on=\"cont_id\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Created reasoning DataFrame: {len(final_reasoning_df):,} records\")\n",
        "print(f\"\\n  Columns:\")\n",
        "for col in final_reasoning_df.columns:\n",
        "    print(f\"    - {col}\")\n",
        "\n",
        "# Display sample reasoning\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SAMPLE REASONING OUTPUT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "sample_idx = 0\n",
        "if len(final_reasoning_df) > 0:\n",
        "    sample = final_reasoning_df.iloc[sample_idx]\n",
        "    print(f\"\\nClient: {sample['cont_id']}\")\n",
        "    print(f\"Predicted Product: {sample.get('predicted_product', sample.get('pred_product', 'N/A'))}\")\n",
        "    print(f\"Confidence: {sample['pred_prob']:.1%}\")\n",
        "    print(f\"\\nReasoning:\\n{sample['reasoning_text']}\")\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "final_reasoning_spark = spark.createDataFrame(final_reasoning_df)\n",
        "\n",
        "# # Optionally save to table\n",
        "# REASONING_TABLE = \"eda_smartlist.us_wealth_management_smartlist.ML_predictions_reasoning\"\n",
        "\n",
        "# try:\n",
        "#     final_reasoning_spark.write.mode(\"overwrite\").saveAsTable(REASONING_TABLE)\n",
        "#     print(f\"\\nâœ“ Reasoning saved to table: {REASONING_TABLE}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"\\nâš  Could not save to table: {e}\")\n",
        "#     print(\"Reasoning DataFrame is available as 'final_reasoning_df' (pandas) and 'final_reasoning_spark' (Spark)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"REASONING ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
