import mlflow
from pyspark.sql import functions as F, Window
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# ------------------------------------------------------------------------------
# CONFIG
# ------------------------------------------------------------------------------
CATALOG_NAME = "eda_smartlist"
SCHEMA_NAME = "models"
MODEL_NAME = "lgbm_hyperparameter_preprocess_and_predict_model"
MODEL_VERSION = "1"

MODEL_URI = f"models:/{CATALOG_NAME}.{SCHEMA_NAME}.{MODEL_NAME}/{MODEL_VERSION}"

TABLE_NAME = "dl_tenants_daas.us_wealth_management.wealth_management_client_metrics"
BRANCHOFFICE_CODE = "83"      # or None
BUSINESS_MONTH = None         # e.g. 202501 or None
VALIDATION_SAMPLE_SIZE = 5000
RANDOM_SEED = 42

OUTPUT_TABLE = (
    "eda_smartlist.us_wealth_management_smartlist."
    "ML_validation_first_to_second_using_pyfunc"
)

print("=" * 80)
print("Validation using lgbm_hyperparameter_preprocess_and_predict_model")
print("=" * 80)

# ------------------------------------------------------------------------------
# 1) Load and filter raw data
# ------------------------------------------------------------------------------
df_raw = spark.table(TABLE_NAME)

if BRANCHOFFICE_CODE:
    df_raw = df_raw.filter(F.col("branchoffice_code") == BRANCHOFFICE_CODE)
    print(f"✓ Filtered by branchoffice_code = {BRANCHOFFICE_CODE}")

if BUSINESS_MONTH:
    df_raw = df_raw.filter(F.col("business_month") == BUSINESS_MONTH)
    print(f"✓ Filtered by business_month = {BUSINESS_MONTH}")

df_raw = df_raw.filter(F.col("policy_status") == "Active")
print(f"✓ Filtered to Active policies only, rows: {df_raw.count():,}")

# ------------------------------------------------------------------------------
# 2) Create product_category (same mapping as training/final_pipeline)
# ------------------------------------------------------------------------------
df_raw = df_raw.withColumn(
    "product_category",
    F.when(F.col("prod_lob") == "LIFE", "LIFE_INSURANCE")
    .when(F.col("sub_product_level_1").isin("VLI", "WL", "UL/IUL", "TERM", "PROTECTIVE PRODUCT"), "LIFE_INSURANCE")
    .when(F.col("sub_product_level_2").like("%LIFE%"), "LIFE_INSURANCE")
    .when(F.col("sub_product_level_2").isin(
        "VARIABLE UNIVERSAL LIFE", "WHOLE LIFE", "UNIVERSAL LIFE",
        "INDEX UNIVERSAL LIFE", "TERM PRODUCT", "VARIABLE LIFE",
        "SURVIVORSHIP WHOLE LIFE", "MONY PROTECTIVE PRODUCT"
    ), "LIFE_INSURANCE")
    .when(F.col("prod_lob").isin("GROUP RETIREMENT", "INDIVIDUAL RETIREMENT"), "RETIREMENT")
    .when(F.col("sub_product_level_1").isin(
        "EQUIVEST", "RETIREMENT 401K", "ACCUMULATOR",
        "RETIREMENT CORNERSTONE", "SCS", "INVESTMENT EDGE"
    ), "RETIREMENT")
    .when(
        (F.col("sub_product_level_2").like("%403B%")) |
        (F.col("sub_product_level_2").like("%401%")) |
        (F.col("sub_product_level_2").like("%IRA%")) |
        (F.col("sub_product_level_2").like("%SEP%")),
        "RETIREMENT"
    )
    .when(F.col("prod_lob") == "BROKER DEALER", "INVESTMENT")
    .when(F.col("sub_product_level_1").isin(
        "INVESTMENT PRODUCT - DIRECT", "INVESTMENT PRODUCT - BROKERAGE",
        "INVESTMENT PRODUCT - ADVISORY", "DIRECT", "BROKERAGE",
        "ADVISORY", "CASH SOLICITOR"
    ), "INVESTMENT")
    .when(
        (F.col("sub_product_level_2").like("%Investment%")) |
        (F.col("sub_product_level_2").like("%Brokerage%")) |
        (F.col("sub_product_level_2").like("%Advisory%")),
        "INVESTMENT"
    )
    .when(F.col("prod_lob") == "NETWORK", "NETWORK_PRODUCTS")
    .when(
        (F.col("sub_product_level_1") == "NETWORK PRODUCTS") |
        (F.col("sub_product_level_2") == "NETWORK PRODUCTS"),
        "NETWORK_PRODUCTS"
    )
    .when(
        (F.col("prod_lob") == "OTHERS") & (F.col("sub_product_level_1") == "HAS"),
        "DISABILITY"
    )
    .when(F.col("sub_product_level_2") == "HAS - DISABILITY", "DISABILITY")
    .when(F.col("prod_lob") == "OTHERS", "HEALTH")
    .when(F.col("sub_product_level_2") == "GROUP HEALTH PRODUCTS", "HEALTH")
    .otherwise("OTHER")
)

# ------------------------------------------------------------------------------
# 3) Identify first and second policies per cont_id (clients with >= 2)
# ------------------------------------------------------------------------------
df_events = df_raw.withColumn("register_ts", F.to_timestamp("register_date"))

w = Window.partitionBy("cont_id").orderBy("register_ts")
df_events = df_events.withColumn("event_idx", F.row_number().over(w))

wc = Window.partitionBy("cont_id")
df_events = df_events.withColumn("total_policies", F.count("*").over(wc))

df_multi = df_events.filter(F.col("total_policies") >= 2)
print(f"✓ Multi‑policy rows (cont_id with ≥2 policies): {df_multi.count():,}")

# First policy row per client (model input)
df_first = df_multi.filter(F.col("event_idx") == 1).drop("event_idx", "total_policies")

# Second policy product per client (ground truth label)
df_second = df_multi.filter(F.col("event_idx") == 2).select(
    "cont_id",
    F.col("product_category").alias("original_second_product_category")
)

# ------------------------------------------------------------------------------
# 4) Sample 5,000 clients and keep only their FIRST policy rows
# ------------------------------------------------------------------------------
distinct_clients = df_first.select("cont_id").dropDuplicates()
n_total_clients = distinct_clients.count()
n_sample = min(VALIDATION_SAMPLE_SIZE, n_total_clients)

print(f"Total eligible clients: {n_total_clients:,}")
print(f"Sampling {n_sample:,} clients for validation...")

sampled_cont_ids = distinct_clients.orderBy(F.rand(seed=RANDOM_SEED)).limit(n_sample)
df_first_sample = df_first.join(sampled_cont_ids, on="cont_id", how="inner")

print(f"Rows in first‑policy sample: {df_first_sample.count():,}")

# ------------------------------------------------------------------------------
# 5) Load pyfunc model and run predictions on FIRST‑policy only
# ------------------------------------------------------------------------------
print("\nLoading pyfunc model and running predictions...")
model = mlflow.pyfunc.load_model(MODEL_URI)
print(f"✓ Model loaded from {MODEL_URI}")

# Access the underlying PythonModel (same pattern as final_pipeline.py)
python_model = model._model_impl.python_model

# IMPORTANT: we pass ONLY the first-policy rows, so inside the model's
# preprocess_for_prediction(single_policy_only=True) each cont_id appears
# as having exactly 1 policy.
predictions_pd = python_model.predict(None, df_first_sample)

print("✓ Predictions completed")
print(f"  Shape: {predictions_pd.shape}")

# ------------------------------------------------------------------------------
# 6) Attach original second product category and save
# ------------------------------------------------------------------------------
pred_spark = spark.createDataFrame(predictions_pd)

# Join predicted next product with the true second product
result_spark = pred_spark.join(df_second, on="cont_id", how="left")

# At this point you have:
# - pred_spark['pred_product']  (model output)
# - df_second['original_second_product_category'] (ground truth)
# in the same Spark DataFrame.
result_spark.write.mode("overwrite").saveAsTable(OUTPUT_TABLE)
print(f"✓ Validation results saved to: {OUTPUT_TABLE}")
