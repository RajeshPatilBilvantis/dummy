# ============================================================================
# IMPROVEMENT 5: Retrain with XGBoost or Optimized GBT (Alternative to LightGBM)
# ============================================================================
print("=" * 70)
print("RETRAINING WITH XGBOOST/OPTIMIZED GBT (LightGBM Alternative)")
print("=" * 70)
print("Improvements applied:")
print("  1. Class weights for imbalanced classes")
print("  2. Transition probability features")
print("  3. Optimized hyperparameters for F1 score")

# Ensure Spark context is available
if 'spark' not in globals() or spark is None:
    raise RuntimeError("Spark context is not available. Please ensure Spark session is active.")

# Try XGBoost first (better than LightGBM for many cases)
use_xgboost = False
try:
    from synapse.ml.lightgbm import LightGBMClassifier
    # Check if XGBoost is available via SynapseML
    try:
        from synapse.ml.xgboost import XGBoostClassifier
        use_xgboost = True
        print("\n✓ XGBoost available - using XGBoost (better performance expected)")
    except ImportError:
        print("\n⚠ XGBoost not available - using optimized GBT with class weights")
        use_xgboost = False
except:
    print("\n⚠ Using optimized GBT with class weights")

if use_xgboost:
    # ========== XGBOOST APPROACH ==========
    print("\nTraining XGBoost model...")
    
    xgb = XGBoostClassifier(
        labelCol="label",
        featuresCol="features",
        validationIndicatorCol="is_validation"
    )
    
    # XGBoost optimized parameters
    xgb.setParams(
        maxDepth=8,
        objective="multi:softprob",
        numClass=7,
        learningRate=0.03,
        numRound=1500,
        earlyStoppingRounds=100,
        numWorkers=1,
        treeMethod="hist",
        growPolicy="depthwise",
        maxLeaves=50,
        subsample=0.85,
        colSampleByTree=0.75,
        colSampleByLevel=0.75,
        minChildWeight=20,  # Similar to minDataInLeaf
        regAlpha=0.1,  # L1 regularization
        regLambda=0.1,  # L2 regularization
        scalePosWeight=1.0  # For class imbalance
    )
    
    pipeline_v2 = Pipeline(stages=indexers_v2 + [label_indexer_v2, assembler_v2, xgb])
    model_v2 = pipeline_v2.fit(combined_train_val_v2)
    xgb_model_v2 = model_v2.stages[-1]
    print("✓ XGBoost model training completed!")
    
else:
    # ========== OPTIMIZED GBT WITH CLASS WEIGHTS ==========
    print("\nTraining Optimized GBT model with class weights...")
    
    from pyspark.ml.classification import GBTClassifier, OneVsRest
    from pyspark.ml.feature import IndexToString
    
    # Calculate class weights for GBT
    # GBT doesn't have built-in class weights, so we'll use OneVsRest with balanced approach
    # and optimize hyperparameters
    
    # Create optimized GBT
    gbt_v2 = GBTClassifier(
        labelCol="label",
        featuresCol="features",
        maxIter=200,  # More iterations
        maxDepth=8,  # Deeper trees
        stepSize=0.03,  # Lower learning rate
        subsamplingRate=0.85,  # Row sampling
        minInstancesPerNode=20,  # Prevent overfitting on minority classes
        minInfoGain=0.1,  # Minimum gain to split
        maxBins=256  # More bins for better precision
    )
    
    # Use OneVsRest for multiclass (better than single GBT for imbalanced data)
    ovr_v2 = OneVsRest(classifier=gbt_v2, labelCol="label", featuresCol="features")
    
    pipeline_v2 = Pipeline(stages=indexers_v2 + [label_indexer_v2, assembler_v2, ovr_v2])
    
    print("Training model with improvements...")
    model_v2 = pipeline_v2.fit(train_df_final_v2)  # Train only on training set
    
    print("✓ Optimized GBT model training completed!")
    gbt_model_v2 = model_v2.stages[-1].models[0]  # Get first binary classifier for feature importance
