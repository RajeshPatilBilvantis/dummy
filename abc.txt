# ============================================================================
# PreprocessAndPredictModel Class for Unity Catalog
# ============================================================================
# This model loads data from 'dl_tenants_daas.us_wealth_management.wealth_management_client_metrics',
# applies preprocessing steps from cell 4 of the training notebook, loads a saved LightGBM model,
# and makes predictions.
#
# NOTE: This model is specifically designed for SINGLE-POLICY CLIENTS ONLY.
# It treats the single policy as the "first policy" and sets years_to_second_policy to 0.

import mlflow.pyfunc
import mlflow
import sys
import os
import numpy as np
import pandas as pd
from typing import Dict, Any, Optional
from pyspark.sql import SparkSession, DataFrame, Window
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Add path to preprocessing.py
sys.path.append('/Workspace/Users/rajesh.patil@equitable.com/Final_model_files')

# Import preprocessing functions from final_preprocessor
from final_preprocessor import (
    create_product_category_column,
    add_asset_allocation_ratios,
    add_temporal_features,
    impute_missing_values,
    encode_categorical_features,
    get_feature_columns,
    preprocess_for_prediction
)

class PreprocessAndPredictModel(mlflow.pyfunc.PythonModel):
    """
    MLflow PythonModel for preprocessing and prediction.
    
    This model is specifically designed for SINGLE-POLICY CLIENTS ONLY.
    
    This model:
    1. Loads data from Unity Catalog table
    2. Filters to clients with exactly ONE policy
    3. Applies preprocessing transformations from cell 4 (treats single policy as "first policy")
    4. Loads LightGBM model from Unity Catalog (eda_smartlist.models.lgbm_model_hyperparameter_310126)
    5. Makes predictions for the next best product
    
    Uses the feature set from cell 4:
    - First policy numeric features (treats single policy as first policy)
    - Asset allocation ratios
    - Temporal features (age_at_first_policy, years_to_second_policy=time since first policy, season_of_first_policy)
    - Encoded categorical features
    
    Note: For single-policy clients, years_to_second_policy is calculated as the time elapsed
    since the first policy registration (current_timestamp - first_register_ts), representing
    how long the client has had their single policy. This matches the training logic which uses
    timestamps for consistency.
    """
    
    def __init__(self):
        """Initialize the model."""
        self.model = None
        self.prod2id = None
        self.id2prod = None
        self.label_map = None
        self.num_classes = None
        self.categorical_mappings = None
        self.feature_cols = None
        
    def load_context(self, context: mlflow.pyfunc.PythonModelContext):
        """
        Load model artifacts from MLflow context.
        
        This method is called when the model is loaded. It should load:
        - The LightGBM model from eda_smartlist.models.lgbm_model_hyperparameter_310126
        - Product mappings (prod2id, id2prod)
        - Label mappings
        - Categorical feature mappings
        - Other artifacts needed for preprocessing
        """
        import lightgbm as lgb
        import pickle
        
        if context is None:
            raise ValueError("MLflow context is required to load model artifacts")
        
        # Get artifacts directory from context
        artifacts_dir = context.artifacts.get("artifacts_path") or context.artifacts.get("model_path")
        
        # Load LightGBM model from Unity Catalog
        model_uri = "models:/eda_smartlist.models.lgbm_model_hyperparameter_310126/1"
        try:
            self.model = mlflow.lightgbm.load_model(model_uri)
            print(f"✓ Loaded LightGBM model from {model_uri}")
        except Exception as e:
            print(f"Could not load LightGBM model from {model_uri}: {e}")
            raise
        
        # Load other artifacts
        artifacts_file = os.path.join(artifacts_dir, "artifacts.pkl")
        if os.path.exists(artifacts_file):
            with open(artifacts_file, 'rb') as f:
                artifacts = pickle.load(f)
                self.prod2id = artifacts.get('prod2id')
                self.id2prod = artifacts.get('id2prod')
                self.label_map = artifacts.get('label_map')
                self.num_classes = artifacts.get('num_classes', 7)
                self.categorical_mappings = artifacts.get('categorical_mappings')
                self.feature_cols = artifacts.get('feature_cols')
                print(f"✓ Loaded artifacts from {artifacts_file}")
        else:
            raise FileNotFoundError(f"Artifacts file not found at {artifacts_file}. Ensure artifacts.pkl is in artifacts.")
        
        # Validate required artifacts
        if self.prod2id is None or self.id2prod is None:
            raise ValueError("prod2id and id2prod mappings are required")
        if self.label_map is None:
            raise ValueError("label_map is required")
        if self.feature_cols is None:
            # Use default feature columns from cell 4
            self.feature_cols = get_feature_columns()
            print(f"✓ Using default feature columns: {len(self.feature_cols)} features")
    
    def _load_data_from_table(
        self, 
        spark: SparkSession,
        table_name: str = "dl_tenants_daas.us_wealth_management.wealth_management_client_metrics",
        branchoffice_code: Optional[str] = None,
        business_month: Optional[int] = None
    ) -> DataFrame:
        """
        Load data from Unity Catalog table.
        
        Args:
            spark: SparkSession
            table_name: Name of the table to load from
            branchoffice_code: Optional branch office code filter
            business_month: Optional business month filter
            
        Returns:
            Spark DataFrame with raw data
        """
        df_raw = spark.table(table_name)
        
        # Apply filters if provided
        if branchoffice_code:
            df_raw = df_raw.filter(F.col("branchoffice_code") == branchoffice_code)
        
        if business_month:
            df_raw = df_raw.filter(F.col("business_month") <= business_month)
        
        return df_raw
    
    def _preprocess_data(
        self,
        spark: SparkSession,
        df_raw: DataFrame,
        prod2id: Dict[str, int],
        num_classes: int,
        categorical_mappings: Optional[Dict] = None
    ) -> pd.DataFrame:
        """
        Preprocess raw data following the exact steps from cell 4 of training notebook.
        
        This method processes SINGLE-POLICY CLIENTS ONLY. It treats the single policy
        as the "first policy" and calculates years_to_second_policy as the time elapsed
        since the first policy registration (current_timestamp - first_register_ts),
        matching the training logic which uses timestamps.
        
        Args:
            spark: SparkSession
            df_raw: Raw Spark DataFrame
            prod2id: Product to ID mapping
            num_classes: Number of product classes
            categorical_mappings: Categorical feature mappings
            
        Returns:
            Preprocessed Pandas DataFrame ready for prediction
        """
        # Use the preprocess_for_prediction function from final_preprocessor
        # Hardcode single_policy_only=True since this model is for single-policy clients only
        pred_pd, feature_cols, updated_categorical_mappings = preprocess_for_prediction(
            spark=spark,
            df_raw=df_raw,
            prod2id=prod2id,
            num_classes=num_classes,
            categorical_cols=None,  # Will use defaults
            categorical_mappings=categorical_mappings,
            branchoffice_code=None,  # Already filtered in _load_data_from_table
            business_month=None,  # Already filtered in _load_data_from_table
            single_policy_only=True  # Always True for this model
        )
        
        # Update feature columns if needed
        if self.feature_cols is None:
            self.feature_cols = feature_cols
        
        # Update categorical mappings
        if updated_categorical_mappings:
            self.categorical_mappings = updated_categorical_mappings
        
        return pred_pd
    
    def predict(
        self, 
        context: mlflow.pyfunc.PythonModelContext, 
        model_input: Any
    ) -> pd.DataFrame:
        """
        Make predictions on input data for SINGLE-POLICY CLIENTS ONLY.
        
        Args:
            context: MLflow context
            model_input: Can be:
                - Spark DataFrame (will be used directly, must contain single-policy clients)
                - Pandas DataFrame (will be converted to Spark, must contain single-policy clients)
                - Dict with parameters (will load from table and filter to single-policy clients)
                
        Returns:
            Pandas DataFrame with predictions
        """
        spark = SparkSession.builder.getOrCreate()
        
        # Handle different input types
        if isinstance(model_input, dict):
            # If input is a dict, treat it as parameters for loading from table
            table_name = model_input.get("table_name", "dl_tenants_daas.us_wealth_management.wealth_management_client_metrics")
            branchoffice_code = model_input.get("branchoffice_code")
            business_month = model_input.get("business_month")
            df_raw = self._load_data_from_table(spark, table_name, branchoffice_code, business_month)
        elif isinstance(model_input, pd.DataFrame):
            # Check if this is a dict that was converted to DataFrame by MLflow
            expected_dict_keys = {"table_name", "branchoffice_code", "business_month"}
            if set(model_input.columns).issubset(expected_dict_keys) and len(model_input.columns) <= 3:
                # This is likely a dict that was converted to DataFrame
                if len(model_input) > 0:
                    row_dict = model_input.iloc[0].to_dict()
                    table_name = row_dict.get("table_name", "dl_tenants_daas.us_wealth_management.wealth_management_client_metrics")
                    branchoffice_code = row_dict.get("branchoffice_code")
                    business_month = row_dict.get("business_month")
                else:
                    table_name = "dl_tenants_daas.us_wealth_management.wealth_management_client_metrics"
                    branchoffice_code = None
                    business_month = None
                df_raw = self._load_data_from_table(spark, table_name, branchoffice_code, business_month)
            else:
                # Convert Pandas to Spark DataFrame (actual data)
                df_raw = spark.createDataFrame(model_input)
        elif isinstance(model_input, DataFrame):
            # Check if this is a dict that was converted to Spark DataFrame
            expected_dict_keys = {"table_name", "branchoffice_code", "business_month"}
            if set(model_input.columns).issubset(expected_dict_keys) and len(model_input.columns) <= 3:
                # This is likely a dict that was converted to DataFrame
                row = model_input.first().asDict() if model_input.count() > 0 else {}
                table_name = row.get("table_name", "dl_tenants_daas.us_wealth_management.wealth_management_client_metrics")
                branchoffice_code = row.get("branchoffice_code")
                business_month = row.get("business_month")
                df_raw = self._load_data_from_table(spark, table_name, branchoffice_code, business_month)
            else:
                df_raw = model_input
        else:
            # Try to convert to dict first
            try:
                if hasattr(model_input, 'to_dict'):
                    model_input_dict = model_input.to_dict()
                    if isinstance(model_input_dict, dict):
                        table_name = model_input_dict.get("table_name", "dl_tenants_daas.us_wealth_management.wealth_management_client_metrics")
                        branchoffice_code = model_input_dict.get("branchoffice_code")
                        business_month = model_input_dict.get("business_month")
                        df_raw = self._load_data_from_table(spark, table_name, branchoffice_code, business_month)
                    else:
                        df_raw = spark.createDataFrame(model_input)
                else:
                    df_raw = spark.createDataFrame(model_input)
            except:
                df_raw = spark.createDataFrame(model_input)
        
        # Ensure we have required artifacts
        if self.prod2id is None or self.id2prod is None:
            raise ValueError("Model artifacts not loaded. Ensure load_context was called properly.")
        
        # Preprocess data using cell 4 logic (single-policy clients only)
        pred_pd = self._preprocess_data(
            spark=spark,
            df_raw=df_raw,
            prod2id=self.prod2id,
            num_classes=self.num_classes,
            categorical_mappings=self.categorical_mappings
        )
        
        # Make predictions
        if self.model is None:
            raise ValueError("Model not loaded. Ensure load_context was called properly.")
        
        pred_probs = self.model.predict(pred_pd[self.feature_cols])
        pred_class_ids = np.argmax(pred_probs, axis=1)
        
        # Convert predictions to product names
        # Handle label_map if available
        if self.label_map:
            inv_label_map = {v: k for k, v in self.label_map.items()}
            final_id2prod = {
                model_id: self.id2prod[original_id]
                for model_id, original_id in inv_label_map.items()
            }
        else:
            # Use id2prod directly if label_map is not available
            final_id2prod = self.id2prod.copy()
        
        pred_pd["pred_class_id"] = pred_class_ids
        pred_pd["pred_product"] = pred_pd["pred_class_id"].apply(
            lambda x: final_id2prod.get(x, self.id2prod.get(x, "UNKNOWN"))
        )
        
        # Add probability columns
        num_classes_pred = pred_probs.shape[1]
        for i in range(num_classes_pred):
            pred_pd[f"prob_{i}"] = pred_probs[:, i]
        pred_pd["pred_prob"] = pred_pd.apply(
            lambda r: r[f"prob_{r['pred_class_id']}"], axis=1
        )
        
        # Return relevant columns
        return pred_pd[["cont_id", "pred_class_id", "pred_product", "pred_prob"] + 
                       [f"prob_{i}" for i in range(num_classes_pred)]]

