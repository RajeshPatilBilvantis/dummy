# ============================================
# SHAP ANALYSIS FOR MODEL INTERPRETABILITY
# ============================================

import shap
import matplotlib.pyplot as plt

# Note: SHAP can be memory intensive, so we'll use a sample of the data
# Use test set for SHAP analysis (or a sample of predictions)
shap_sample_size = min(100, len(test_pd))  # Use up to 100 samples for SHAP
shap_data = test_pd[feature_cols_final].iloc[:shap_sample_size].copy()

print(f"Computing SHAP values for {len(shap_data)} samples...")
print(f"Number of features: {len(feature_cols_final)}")

# Create SHAP explainer for LightGBM
# TreeExplainer is optimized for tree-based models like LightGBM
explainer = shap.TreeExplainer(model)

# Calculate SHAP values
shap_values = explainer.shap_values(shap_data)

print(f"SHAP values shape: {np.array(shap_values).shape}")
print(f"Number of classes: {len(shap_values)}")

# ------------- 1) Summary Plot (overall feature importance) -------------
print("\n=== SHAP Summary Plot ===")
# For multiclass, we can plot for each class or use mean absolute SHAP values
# Let's create a summary plot using mean absolute SHAP across all classes
shap_values_mean = np.mean([np.abs(sv) for sv in shap_values], axis=0)

# Create a summary plot
plt.figure(figsize=(12, 8))
shap.summary_plot(shap_values, shap_data, plot_type="bar", max_display=20, show=False)
plt.title("SHAP Feature Importance (Mean |SHAP value| across all classes)")
plt.tight_layout()
plt.show()

# ------------- 2) Summary Plot (detailed) -------------
print("\n=== SHAP Summary Plot (Detailed) ===")
# Show how each feature affects predictions
plt.figure(figsize=(12, 10))
shap.summary_plot(shap_values, shap_data, max_display=20, show=False)
plt.title("SHAP Summary Plot - Feature Impact on Predictions")
plt.tight_layout()
plt.show()

# ------------- 3) Feature Importance by Class -------------
print("\n=== SHAP Feature Importance by Class ===")
class_names = [final_id2prod.get(i, f"Class_{i}") for i in range(len(shap_values))]

fig, axes = plt.subplots(len(shap_values), 1, figsize=(12, 6*len(shap_values)))
if len(shap_values) == 1:
    axes = [axes]

for idx, (sv, class_name) in enumerate(zip(shap_values, class_names)):
    shap.summary_plot(sv, shap_data, plot_type="bar", max_display=15, show=False, ax=axes[idx])
    axes[idx].set_title(f"SHAP Feature Importance - {class_name}")
    
plt.tight_layout()
plt.show()

# ------------- 4) Waterfall Plot for a few examples -------------
print("\n=== SHAP Waterfall Plot (Sample Predictions) ===")
# Show detailed explanation for a few individual predictions
num_examples = min(3, len(shap_data))

for example_idx in range(num_examples):
    print(f"\n--- Example {example_idx + 1} ---")
    print(f"Actual class: {test_pd.iloc[example_idx]['label0']} ({final_id2prod.get(test_pd.iloc[example_idx]['label0'], 'Unknown')})")
    print(f"Predicted class: {test_pred[example_idx]} ({final_id2prod.get(test_pred[example_idx], 'Unknown')})")
    print(f"Prediction probability: {test_pred_prob[example_idx][test_pred[example_idx]]:.4f}")
    
    # Get SHAP values for the predicted class
    pred_class = test_pred[example_idx]
    shap_values_example = shap_values[pred_class][example_idx]
    
    # Create waterfall plot
    plt.figure(figsize=(10, 6))
    shap.waterfall_plot(
        shap.Explanation(
            values=shap_values_example,
            base_values=explainer.expected_value[pred_class],
            data=shap_data.iloc[example_idx].values,
            feature_names=feature_cols_final
        ),
        max_display=15,
        show=False
    )
    plt.title(f"SHAP Waterfall Plot - Example {example_idx + 1}\nPredicted: {final_id2prod.get(pred_class, 'Unknown')}")
    plt.tight_layout()
    plt.show()

# ------------- 5) Feature Importance DataFrame -------------
print("\n=== SHAP Feature Importance Summary ===")
# Calculate mean absolute SHAP values for each feature
feature_importance = {}
for i, feat in enumerate(feature_cols_final):
    # Mean absolute SHAP value across all classes and samples
    importance = np.mean([np.abs(sv[:, i]) for sv in shap_values])
    feature_importance[feat] = importance

# Sort by importance
feature_importance_sorted = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)

# Create DataFrame
shap_importance_df = pd.DataFrame(feature_importance_sorted, columns=['Feature', 'Mean_|SHAP_Value|'])
shap_importance_df['Rank'] = range(1, len(shap_importance_df) + 1)

print("\nTop 20 Most Important Features:")
display(shap_importance_df.head(20))

# ------------- 6) Partial Dependence Plots for Top Features -------------
print("\n=== SHAP Partial Dependence Plots (Top 5 Features) ===")
top_features = shap_importance_df.head(5)['Feature'].tolist()

for feat in top_features:
    if feat in shap_data.columns:
        plt.figure(figsize=(10, 6))
        shap.plots.partial_dependence(
            feat, model.predict, shap_data[[feat]], ice=False,
            model_expected_value=True, feature_expected_value=True, show=False
        )
        plt.title(f"Partial Dependence Plot - {feat}")
        plt.tight_layout()
        plt.show()

# ------------- 7) SHAP Values Statistics -------------
print("\n=== SHAP Values Statistics ===")
print(f"SHAP values computed for {len(shap_data)} samples")
print(f"Number of features: {len(feature_cols_final)}")
print(f"Number of classes: {len(shap_values)}")

# Calculate statistics per class
for idx, class_name in enumerate(class_names):
    sv_class = shap_values[idx]
    print(f"\n{class_name}:")
    print(f"  Mean |SHAP value|: {np.mean(np.abs(sv_class)):.6f}")
    print(f"  Max |SHAP value|: {np.max(np.abs(sv_class)):.6f}")
    print(f"  Expected value (base): {explainer.expected_value[idx]:.6f}")

print("\nSHAP Analysis Complete!")
