# ============================================================================
# GLOBAL SHAP COMPARISON: HIGH vs LOW CONFIDENCE PREDICTIONS
# ============================================================================

print("\n" + "=" * 80)
print("GLOBAL SHAP COMPARISON: HIGH vs LOW CONFIDENCE PREDICTIONS")
print("=" * 80)

# Load predictions from table
print("\nLoading predictions from table...")
df_predictions_full = spark.table(PREDICTIONS_TABLE)
print(f"✓ Loaded {df_predictions_full.count():,} total predictions")

# Divide into two buckets based on pred_prob
print("\nDividing clients into confidence buckets...")
df_high_conf = df_predictions_full.filter(F.col("pred_prob") >= 0.90)
df_low_conf = df_predictions_full.filter(F.col("pred_prob") <= 0.80)

high_conf_count = df_high_conf.count()
low_conf_count = df_low_conf.count()

print(f"✓ High confidence (pred_prob >= 0.90): {high_conf_count:,} clients")
print(f"✓ Low confidence (pred_prob <= 0.80): {low_conf_count:,} clients")

# Sample if too large (for computational efficiency)
MAX_SAMPLES_PER_BUCKET = 5000
if high_conf_count > MAX_SAMPLES_PER_BUCKET:
    print(f"⚠ Sampling {MAX_SAMPLES_PER_BUCKET:,} from high confidence bucket...")
    df_high_conf = df_high_conf.sample(fraction=MAX_SAMPLES_PER_BUCKET / high_conf_count, seed=42)
    high_conf_count = df_high_conf.count()

if low_conf_count > MAX_SAMPLES_PER_BUCKET:
    print(f"⚠ Sampling {MAX_SAMPLES_PER_BUCKET:,} from low confidence bucket...")
    df_low_conf = df_low_conf.sample(fraction=MAX_SAMPLES_PER_BUCKET / low_conf_count, seed=42)
    low_conf_count = df_low_conf.count()

# Convert to pandas
pred_high_pd = df_high_conf.toPandas()
pred_low_pd = df_low_conf.toPandas()

print(f"\n✓ Working with {len(pred_high_pd):,} high confidence and {len(pred_low_pd):,} low confidence predictions")

# ============================================================================
# RECONSTRUCT FEATURES FOR BOTH BUCKETS
# ============================================================================

def reconstruct_features_for_bucket(pred_pd_bucket, bucket_name):
    """Reconstruct features for a bucket of predictions."""
    print(f"\n{'='*80}")
    print(f"RECONSTRUCTING FEATURES FOR {bucket_name.upper()} CONFIDENCE BUCKET")
    print(f"{'='*80}")
    
    # Get cont_ids
    cont_ids = pred_pd_bucket["cont_id"].unique().tolist()
    print(f"✓ Found {len(cont_ids):,} unique cont_ids")
    
    # Load source data
    df_raw = spark.table(SOURCE_TABLE)
    df_raw = df_raw.filter(F.col("cont_id").isin(cont_ids))
    df_raw = df_raw.filter(F.col("policy_status") == "Active")
    
    # Create product_category
    df_raw = create_product_category_column(df_raw)
    
    # Filter to single-policy clients
    df_events = df_raw.select(
        "cont_id",
        "axa_party_id",
        "product_category",
        "register_date",
        "isrd_brth_date",
        "acct_val_amt",
        "face_amt",
        "cash_val_amt",
        "wc_total_assets",
        "wc_assetmix_stocks",
        "wc_assetmix_bonds",
        "wc_assetmix_mutual_funds",
        "wc_assetmix_annuity",
        "wc_assetmix_deposits",
        "wc_assetmix_other_assets",
        "psn_age",
        "client_seg",
        "client_seg_1",
        "aum_band",
        "channel",
        "agent_segment",
        "branchoffice_code",
        "policy_no",
    ).filter(
        (F.col("axa_party_id").isNotNull())
        & (F.col("cont_id").isNotNull())
        & (F.col("register_date").isNotNull())
        & (F.col("product_category").isNotNull())
    )
    
    # Count policies per axa_party_id
    party_counts = (
        df_events.groupBy("axa_party_id")
        .agg(F.countDistinct("policy_no").alias("policy_count"))
        .filter(F.col("policy_count") == 1)
    )
    
    # Filter to single-policy clients
    df_single_policy = df_events.join(
        party_counts.select("axa_party_id"), on="axa_party_id", how="inner"
    ).dropDuplicates(["cont_id"])
    
    print(f"✓ Filtered to {df_single_policy.count():,} single-policy client records")
    
    # Convert dates
    df_single_policy = df_single_policy.withColumn("register_ts", F.to_timestamp("register_date"))
    df_single_policy = df_single_policy.withColumn("birth_ts", F.to_timestamp("isrd_brth_date"))
    
    # Create "first policy" features
    df_first = df_single_policy.select(
        "cont_id",
        "axa_party_id",
        F.col("product_category").alias("first_product_category"),
        F.col("register_ts").alias("first_register_ts"),
        "birth_ts",
        F.col("acct_val_amt").alias("first_acct_val_amt"),
        F.col("face_amt").alias("first_face_amt"),
        F.col("cash_val_amt").alias("first_cash_val_amt"),
        "wc_total_assets",
        "wc_assetmix_stocks",
        "wc_assetmix_bonds",
        "wc_assetmix_mutual_funds",
        "wc_assetmix_annuity",
        "wc_assetmix_deposits",
        "wc_assetmix_other_assets",
        "psn_age",
        "client_seg",
        "client_seg_1",
        "aum_band",
        "channel",
        "agent_segment",
        "branchoffice_code",
    )
    
    # Add temporal features
    df_first = df_first.withColumn("second_register_ts", F.current_timestamp())
    df_first = add_asset_allocation_ratios(df_first)
    
    df_first = df_first.withColumn(
        "season_of_first_policy",
        F.when(F.month("first_register_ts").between(1, 3), "Q1")
        .when(F.month("first_register_ts").between(4, 6), "Q2")
        .when(F.month("first_register_ts").between(7, 9), "Q3")
        .when(F.month("first_register_ts").between(10, 12), "Q4")
        .otherwise("Unknown"),
    )
    
    df_first = df_first.withColumn(
        "age_at_first_policy",
        F.datediff(F.col("first_register_ts"), F.col("birth_ts")) / 365.25,
    )
    
    df_first = df_first.withColumn(
        "years_to_second_policy",
        F.datediff(F.col("second_register_ts"), F.col("first_register_ts")) / 365.25,
    )
    
    # Impute missing values
    categorical_cols = [
        "first_product_category",
        "client_seg",
        "client_seg_1",
        "aum_band",
        "channel",
        "agent_segment",
        "branchoffice_code",
        "season_of_first_policy",
    ]
    
    df_first = impute_missing_values(df_first, categorical_cols)
    
    # Encode categorical features
    df_encoded, _ = encode_categorical_features(
        df_first, categorical_cols, spark.sparkContext, categorical_mappings=categorical_mappings
    )
    
    # Convert to pandas
    select_cols = ["cont_id", "axa_party_id"] + feature_cols
    available_cols = [col for col in select_cols if col in df_encoded.columns]
    features_pd = df_encoded.select(available_cols).toPandas()
    features_pd.fillna(0, inplace=True)
    
    print(f"✓ Reconstructed features for {len(features_pd):,} clients")
    
    return features_pd

# Reconstruct features for both buckets
features_high_pd = reconstruct_features_for_bucket(pred_high_pd, "HIGH")
features_low_pd = reconstruct_features_for_bucket(pred_low_pd, "LOW")

# Merge predictions with features
pred_high_with_features = pred_high_pd.merge(
    features_high_pd[["cont_id"] + feature_cols], on="cont_id", how="inner"
)
pred_low_with_features = pred_low_pd.merge(
    features_low_pd[["cont_id"] + feature_cols], on="cont_id", how="inner"
)

print(f"\n✓ High confidence: {len(pred_high_with_features):,} records with features")
print(f"✓ Low confidence: {len(pred_low_with_features):,} records with features")

# ============================================================================
# COMPUTE SHAP VALUES FOR BOTH BUCKETS
# ============================================================================

print("\n" + "=" * 80)
print("COMPUTING SHAP VALUES FOR BOTH BUCKETS")
print("=" * 80)

def compute_shap_for_bucket(pred_with_features_bucket, bucket_name):
    """Compute SHAP values for a bucket."""
    print(f"\nComputing SHAP values for {bucket_name} confidence bucket...")
    
    # Verify all feature columns are present
    missing_cols = [col for col in feature_cols if col not in pred_with_features_bucket.columns]
    if missing_cols:
        print(f"⚠ Missing columns: {missing_cols}")
        available_cols = [col for col in feature_cols if col in pred_with_features_bucket.columns]
    else:
        available_cols = feature_cols
    
    # Prepare feature matrix
    X = pred_with_features_bucket[available_cols].values
    print(f"  Feature matrix shape: {X.shape}")
    
    # Compute SHAP values
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X)
    
    # Get predictions to determine which class SHAP values to use
    pred_probs = model.predict(X)
    pred_class_ids = np.argmax(pred_probs, axis=1)
    
    # Handle SHAP output format
    if isinstance(shap_values, list):
        shap_array = np.array([shap_values[pred_class_ids[i]][i] for i in range(len(pred_with_features_bucket))])
    elif isinstance(shap_values, np.ndarray) and len(shap_values.shape) == 3:
        n_samples, n_features, n_classes = shap_values.shape
        if n_samples == len(pred_with_features_bucket) and n_features == len(available_cols):
            shap_array = np.array([shap_values[i, :, pred_class_ids[i]] for i in range(n_samples)])
        else:
            shap_values = shap_values.transpose(1, 0, 2)
            shap_array = np.array([shap_values[i, :, pred_class_ids[i]] for i in range(len(pred_with_features_bucket))])
    else:
        shap_array = shap_values
    
    print(f"  ✓ Computed SHAP values: {shap_array.shape}")
    
    return shap_array, available_cols

# Compute SHAP for both buckets
shap_high, feature_cols_high = compute_shap_for_bucket(pred_high_with_features, "HIGH")
shap_low, feature_cols_low = compute_shap_for_bucket(pred_low_with_features, "LOW")

# Ensure both use the same feature columns
common_features = [f for f in feature_cols if f in feature_cols_high and f in feature_cols_low]
print(f"\n✓ Common features for comparison: {len(common_features)}/{len(feature_cols)}")

# Filter SHAP arrays to common features
feature_idx_map_high = {f: i for i, f in enumerate(feature_cols_high) if f in common_features}
feature_idx_map_low = {f: i for i, f in enumerate(feature_cols_low) if f in common_features}

shap_high_common = np.array([[shap_high[i, feature_idx_map_high[f]] for f in common_features] 
                              for i in range(len(shap_high))])
shap_low_common = np.array([[shap_low[i, feature_idx_map_low[f]] for f in common_features] 
                            for i in range(len(shap_low))])

# ============================================================================
# COMPARE FEATURE IMPORTANCE BETWEEN BUCKETS
# ============================================================================

print("\n" + "=" * 80)
print("COMPARING FEATURE IMPORTANCE BETWEEN BUCKETS")
print("=" * 80)

# Compute mean absolute SHAP per feature for each bucket
mean_abs_shap_high = np.mean(np.abs(shap_high_common), axis=0)
mean_abs_shap_low = np.mean(np.abs(shap_low_common), axis=0)

# Create comparison DataFrame
comparison_df = pd.DataFrame({
    "feature": common_features,
    "high_conf_mean_abs_shap": mean_abs_shap_high,
    "low_conf_mean_abs_shap": mean_abs_shap_low,
})

# Compute difference
comparison_df["difference"] = comparison_df["high_conf_mean_abs_shap"] - comparison_df["low_conf_mean_abs_shap"]
comparison_df["ratio"] = comparison_df["high_conf_mean_abs_shap"] / (comparison_df["low_conf_mean_abs_shap"] + 1e-10)

# Sort by absolute difference
comparison_df = comparison_df.sort_values("difference", key=abs, ascending=False)

print(f"\n✓ Feature importance comparison:")
print(f"  Total features compared: {len(comparison_df)}")
print(f"\n  Top 10 features with largest difference:")
display(comparison_df.head(10))

# ============================================================================
# VISUALIZATIONS
# ============================================================================

print("\n" + "=" * 80)
print("CREATING COMPARISON VISUALIZATIONS")
print("=" * 80)

# 1. Side-by-side bar plot of top features
plt.figure(figsize=(14, 8))
top_n = 15
top_features = comparison_df.head(top_n)["feature"].tolist()

x = np.arange(len(top_features))
width = 0.35

high_values = [comparison_df[comparison_df["feature"] == f]["high_conf_mean_abs_shap"].values[0] for f in top_features]
low_values = [comparison_df[comparison_df["feature"] == f]["low_conf_mean_abs_shap"].values[0] for f in top_features]

plt.bar(x - width/2, high_values, width, label="High Confidence (≥0.90)", color="green", alpha=0.7)
plt.bar(x + width/2, low_values, width, label="Low Confidence (≤0.80)", color="red", alpha=0.7)

plt.xlabel("Feature", fontsize=12)
plt.ylabel("Mean |SHAP value|", fontsize=12)
plt.title(f"Top {top_n} Features: SHAP Importance Comparison (High vs Low Confidence)", fontsize=14, fontweight="bold")
plt.xticks(x, top_features, rotation=45, ha="right")
plt.legend(fontsize=11)
plt.grid(axis="y", alpha=0.3)
plt.tight_layout()
plt.show()

# 2. Scatter plot: High vs Low confidence SHAP importance
plt.figure(figsize=(10, 8))
plt.scatter(comparison_df["low_conf_mean_abs_shap"], 
            comparison_df["high_conf_mean_abs_shap"],
            alpha=0.6, s=100, edgecolors="black", linewidth=0.5)

# Add diagonal line
max_val = max(comparison_df["high_conf_mean_abs_shap"].max(), 
              comparison_df["low_conf_mean_abs_shap"].max())
plt.plot([0, max_val], [0, max_val], "r--", alpha=0.5, label="Equal importance")

# Annotate top features
for idx, row in comparison_df.head(10).iterrows():
    plt.annotate(row["feature"], 
                (row["low_conf_mean_abs_shap"], row["high_conf_mean_abs_shap"]),
                fontsize=8, alpha=0.7)

plt.xlabel("Low Confidence Mean |SHAP| (≤0.80)", fontsize=12)
plt.ylabel("High Confidence Mean |SHAP| (≥0.90)", fontsize=12)
plt.title("Feature Importance: High vs Low Confidence Predictions", fontsize=14, fontweight="bold")
plt.legend(fontsize=11)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# 3. Difference plot (features that matter more for high vs low confidence)
plt.figure(figsize=(12, 8))
top_diff = comparison_df.head(20)
colors = ["green" if d > 0 else "red" for d in top_diff["difference"]]

plt.barh(range(len(top_diff)), top_diff["difference"], color=colors, alpha=0.7)
plt.yticks(range(len(top_diff)), top_diff["feature"])
plt.xlabel("Difference in Mean |SHAP| (High - Low)", fontsize=12)
plt.title("Top 20 Features: Difference in Importance Between High and Low Confidence", 
          fontsize=14, fontweight="bold")
plt.axvline(x=0, color="black", linestyle="--", linewidth=1)
plt.grid(axis="x", alpha=0.3)
plt.tight_layout()
plt.show()

# 4. Ratio plot (how many times more important in high vs low)
plt.figure(figsize=(12, 8))
top_ratio = comparison_df.nlargest(20, "ratio")
top_ratio = top_ratio[top_ratio["ratio"] < 10]  # Filter extreme ratios for readability

plt.barh(range(len(top_ratio)), top_ratio["ratio"], color="green", alpha=0.7)
plt.yticks(range(len(top_ratio)), top_ratio["feature"])
plt.xlabel("Ratio (High / Low)", fontsize=12)
plt.title("Top 20 Features: Relative Importance Ratio (High vs Low Confidence)", 
          fontsize=14, fontweight="bold")
plt.axvline(x=1, color="red", linestyle="--", linewidth=1, label="Equal importance")
plt.legend()
plt.grid(axis="x", alpha=0.3)
plt.tight_layout()
plt.show()

# 5. Summary statistics by predicted product
print("\n" + "=" * 80)
print("FEATURE IMPORTANCE BY PREDICTED PRODUCT")
print("=" * 80)

# Get predicted products for each bucket
pred_product_col = "predicted_product" if "predicted_product" in pred_high_with_features.columns else "pred_product"

if pred_product_col in pred_high_with_features.columns:
    # High confidence bucket
    high_prod_counts = pred_high_with_features[pred_product_col].value_counts()
    print(f"\nHigh Confidence Bucket - Product Distribution:")
    for product, count in high_prod_counts.items():
        print(f"  {product}: {count:,} ({count/len(pred_high_with_features)*100:.1f}%)")
    
    # Low confidence bucket
    low_prod_counts = pred_low_with_features[pred_product_col].value_counts()
    print(f"\nLow Confidence Bucket - Product Distribution:")
    for product, count in low_prod_counts.items():
        print(f"  {product}: {count:,} ({count/len(pred_low_with_features)*100:.1f}%)")
    
    # Feature importance by product for high confidence
    print(f"\n\nTop Features for High Confidence Predictions (by product):")
    for product in high_prod_counts.head(5).index:
        product_mask = pred_high_with_features[pred_product_col] == product
        if product_mask.sum() > 0:
            product_shap = shap_high_common[product_mask]
            product_mean_abs = np.mean(np.abs(product_shap), axis=0)
            product_importance = pd.DataFrame({
                "feature": common_features,
                "mean_abs_shap": product_mean_abs,
            }).sort_values("mean_abs_shap", ascending=False)
            
            print(f"\n  {product} (n={product_mask.sum()}):")
            for _, row in product_importance.head(5).iterrows():
                print(f"    - {row['feature']}: {row['mean_abs_shap']:.4f}")

# ============================================================================
# SUMMARY INSIGHTS
# ============================================================================

print("\n" + "=" * 80)
print("SUMMARY INSIGHTS")
print("=" * 80)

print(f"\n1. Overall Comparison:")
print(f"   - High confidence bucket: {len(pred_high_with_features):,} predictions")
print(f"   - Low confidence bucket: {len(pred_low_with_features):,} predictions")

print(f"\n2. Feature Importance Differences:")
top_3_diff = comparison_df.head(3)
for _, row in top_3_diff.iterrows():
    direction = "more important" if row["difference"] > 0 else "less important"
    print(f"   - {row['feature']}: {abs(row['difference']):.4f} {direction} in high confidence predictions")

print(f"\n3. Features with Largest Ratio (High/Low):")
top_3_ratio = comparison_df.nlargest(3, "ratio")
for _, row in top_3_ratio.iterrows():
    print(f"   - {row['feature']}: {row['ratio']:.2f}x more important in high confidence")

print(f"\n4. Key Findings:")
high_avg = comparison_df["high_conf_mean_abs_shap"].mean()
low_avg = comparison_df["low_conf_mean_abs_shap"].mean()
print(f"   - Average feature importance (high confidence): {high_avg:.4f}")
print(f"   - Average feature importance (low confidence): {low_avg:.4f}")
print(f"   - Overall difference: {high_avg - low_avg:.4f} ({((high_avg/low_avg - 1)*100):.1f}% higher)")

print("\n" + "=" * 80)
print("GLOBAL SHAP COMPARISON COMPLETE")
print("=" * 80)
