======================================================================
RETRAINING LIGHTGBM WITH IMPROVEMENTS
======================================================================
Improvements applied:
  1. Class weights for imbalanced classes
  2. Transition probability features
  3. Optimized hyperparameters for F1 score

TypeError: 'JavaPackage' object is not callable
File <command-5386568783283493>, line 16
     12 # Note: LightGBM's isUnbalance=True handles class imbalance automatically
     13 # But we can also use class_weight parameter if available
     14 # For now, we'll use isUnbalance=True and optimize other parameters
     15 from synapse.ml.lightgbm import LightGBMClassifier
---> 16 lgbm_v2 = LightGBMClassifier(
     17     labelCol="label",
     18     featuresCol="features",
     19     isUnbalance=True,  # Handles class imbalance
     20     validationIndicatorCol="is_validation"
     21 )
     23 # Optimized hyperparameters for better F1 score
     24 lgbm_v2.setParams(
     25     maxDepth=8,  # Slightly deeper for more complex patterns
     26     objective="multiclass",
   (...)
     38     minGainToSplit=0.1  # Minimum gain to split
     39 )
File /local_disk0/.ephemeral_nfs/envs/pythonEnv-c62ffb4e-832e-4831-a6eb-73cc48463898/lib/python3.12/site-packages/synapse/ml/lightgbm/LightGBMClassifier.py:414, in LightGBMClassifier.__init__(self, java_obj, baggingFraction, baggingFreq, baggingSeed, binSampleCount, boostFromAverage, boostingType, catSmooth, categoricalSlotIndexes, categoricalSlotNames, catl2, chunkSize, dataRandomSeed, dataTransferMode, defaultListenPort, deterministic, driverListenPort, dropRate, dropSeed, earlyStoppingRound, executionMode, extraSeed, featureFraction, featureFractionByNode, featureFractionSeed, featuresCol, featuresShapCol, fobj, improvementTolerance, initScoreCol, isEnableSparse, isProvideTrainingMetric, isUnbalance, labelCol, lambdaL1, lambdaL2, leafPredictionCol, learningRate, matrixType, maxBin, maxBinByFeature, maxCatThreshold, maxCatToOnehot, maxDeltaStep, maxDepth, maxDrop, maxNumClasses, maxStreamingOMPThreads, metric, microBatchSize, minDataInLeaf, minDataPerBin, minDataPerGroup, minGainToSplit, minSumHessianInLeaf, modelString, monotoneConstraints, monotoneConstraintsMethod, monotonePenalty, negBaggingFraction, numBatches, numIterations, numLeaves, numTasks, numThreads, objective, objectiveSeed, otherRate, parallelism, passThroughArgs, posBaggingFraction, predictDisableShapeCheck, predictionCol, probabilityCol, rawPredictionCol, referenceDataset, repartitionByGroupingColumn, samplingMode, samplingSubsetSize, seed, skipDrop, slotNames, thresholds, timeout, topK, topRate, uniformDrop, useBarrierExecutionMode, useMissing, useSingleDatasetMode, validationIndicatorCol, verbosity, weightCol, xGBoostDartMode, zeroAsMissing)
    412 super(LightGBMClassifier, self).__init__()
    413 if java_obj is None:
--> 414     self._java_obj = self._new_java_obj("com.microsoft.azure.synapse.ml.lightgbm.LightGBMClassifier", self.uid)
    415 else:
    416     self._java_obj = java_obj

# ============================================================================
# IMPROVEMENT 2: Create Product Transition Probability Features
# ============================================================================
print("=" * 70)
print("CREATING PRODUCT TRANSITION PROBABILITY FEATURES")
print("=" * 70)

# Calculate transition probabilities from training data
# This gives us P(second_product | first_product) for each combination
transition_probs_df = train_data.groupBy("product_category", "second_product_category").count()

# Calculate total counts per first product
first_prod_totals = train_data.groupBy("product_category").count().withColumnRenamed("count", "total_first_prod")

# Join and calculate probabilities
transition_probs_with_totals = transition_probs_df.join(
    first_prod_totals, 
    on="product_category", 
    how="left"
)

# Calculate probability: count / total for that first product
transition_probs_final = transition_probs_with_totals.withColumn(
    "transition_probability",
    F.col("count") / F.col("total_first_prod")
).select(
    F.col("product_category").alias("first_prod"),
    F.col("second_product_category").alias("second_prod"),
    "transition_probability"
)

print("\nSample Transition Probabilities:")
display(transition_probs_final.orderBy(F.desc("transition_probability")).limit(20))

# Create lookup table for top 3 most likely next products for each first product
window_spec = Window.partitionBy("first_prod").orderBy(F.col("transition_probability").desc())
transition_probs_ranked = transition_probs_final.withColumn("rank", F.row_number().over(window_spec))

# Get top 3 transitions for each first product
top_transitions = transition_probs_ranked.filter(F.col("rank") <= 3).select(
    "first_prod",
    "second_prod",
    "transition_probability",
    "rank"
)

print("\nTop 3 Transition Probabilities per First Product:")
display(top_transitions.orderBy("first_prod", "rank"))

# Create features: probability of each second product given first product
# We'll pivot this to create features like: prob_RETIREMENT_given_LIFE_INSURANCE
# Since each (first_prod, second_prod) combination is unique, we can use max()
# Note: pivot will create columns for each unique value in "second_prod"
transition_features = transition_probs_final.groupBy("first_prod").pivot("second_prod").agg(
    F.max("transition_probability")
).fillna(0.0)

# Rename columns to avoid conflicts
# Get column names and rename (excluding first_prod)
cols_to_rename = [col for col in transition_features.columns if col != "first_prod"]
for col_name in cols_to_rename:
    new_name = f"prob_{col_name}_given_first"
    # Replace any special characters that might cause issues
    new_name = new_name.replace(" ", "_").replace("-", "_").replace("(", "").replace(")", "")
    transition_features = transition_features.withColumnRenamed(col_name, new_name)

print("\nTransition Probability Features (sample):")
display(transition_features.limit(10))







# ============================================================================
# IMPROVEMENT 3: Add Transition Probability Features to Training Data
# ============================================================================
print("=" * 70)
print("ADDING TRANSITION PROBABILITY FEATURES TO DATASETS")
print("=" * 70)

# Function to add transition probability features
def add_transition_prob_features(df, transition_features_df):
    df_with_probs = df.join(
        transition_features_df,
        df["product_category"] == transition_features_df["first_prod"],
        how="left"
    )
    # Drop the join key column (first_prod) if it exists
    if "first_prod" in df_with_probs.columns:
        df_with_probs = df_with_probs.drop("first_prod")
    # Fill missing probabilities with 0
    prob_cols = [col for col in df_with_probs.columns if col.startswith("prob_")]
    if prob_cols:
        df_with_probs = df_with_probs.fillna(0.0, subset=prob_cols)
    return df_with_probs

# Add to training and validation sets
train_df_final_v2 = add_transition_prob_features(train_df_final, transition_features)
val_df_final_v2 = add_transition_prob_features(val_df_final, transition_features)

print(f"\nOriginal train_df_final columns: {len(train_df_final.columns)}")
print(f"Enhanced train_df_final_v2 columns: {len(train_df_final_v2.columns)}")
print(f"Added {len(train_df_final_v2.columns) - len(train_df_final.columns)} transition probability features")

# Update combined dataset
train_df_final_v2 = train_df_final_v2.withColumn("is_validation", F.lit(False))
val_df_final_v2 = val_df_final_v2.withColumn("is_validation", F.lit(True))
combined_train_val_v2 = train_df_final_v2.unionByName(val_df_final_v2)

print("\nâœ“ Transition probability features added successfully!")
