# ============================================================================
# STEP 5: Save Results with Talking Points
# ============================================================================

print("\n" + "=" * 80)
print("Saving Results with Talking Points")
print("=" * 80)

try:
    # Add extra columns from wealth_management_client_metrics on cont_id
    extra_cols = [
        "axa_party_id", "cont_id", "psn_age", "client_seg", "client_seg_1", "aum_band", "channel",
        "division_name", "branch_name", "business_city", "business_state_cod", "register_date",
        "prod_lob", "sub_product_level_1", "sub_product_level_2"  # Needed for product_category creation
    ]
    df_metrics = spark.table("dl_tenants_daas.us_wealth_management.wealth_management_client_metrics") \
        .select(*extra_cols).dropDuplicates(["cont_id"])
    
    # Create product_category column (same logic as in cell 4 and final_preprocessor.py)
    df_metrics = df_metrics.withColumn(
        "product_category",
        F.when(F.col("prod_lob") == "LIFE", "LIFE_INSURANCE")
        .when(F.col("sub_product_level_1").isin("VLI", "WL", "UL/IUL", "TERM", "PROTECTIVE PRODUCT"), "LIFE_INSURANCE")
        .when(F.col("sub_product_level_2").like("%LIFE%"), "LIFE_INSURANCE")
        .when(F.col("sub_product_level_2").isin(
            "VARIABLE UNIVERSAL LIFE", "WHOLE LIFE", "UNIVERSAL LIFE",
            "INDEX UNIVERSAL LIFE", "TERM PRODUCT", "VARIABLE LIFE",
            "SURVIVORSHIP WHOLE LIFE", "MONY PROTECTIVE PRODUCT"
        ), "LIFE_INSURANCE")
        .when(F.col("prod_lob").isin("GROUP RETIREMENT", "INDIVIDUAL RETIREMENT"), "RETIREMENT")
        .when(F.col("sub_product_level_1").isin(
            "EQUIVEST", "RETIREMENT 401K", "ACCUMULATOR",
            "RETIREMENT CORNERSTONE", "SCS", "INVESTMENT EDGE"
        ), "RETIREMENT")
        .when(
            (F.col("sub_product_level_2").like("%403B%")) |
            (F.col("sub_product_level_2").like("%401%")) |
            (F.col("sub_product_level_2").like("%IRA%")) |
            (F.col("sub_product_level_2").like("%SEP%")),
            "RETIREMENT"
        )
        .when(F.col("prod_lob") == "BROKER DEALER", "INVESTMENT")
        .when(F.col("sub_product_level_1").isin(
            "INVESTMENT PRODUCT - DIRECT", "INVESTMENT PRODUCT - BROKERAGE",
            "INVESTMENT PRODUCT - ADVISORY", "DIRECT", "BROKERAGE",
            "ADVISORY", "CASH SOLICITOR"
        ), "INVESTMENT")
        .when(
            (F.col("sub_product_level_2").like("%Investment%")) |
            (F.col("sub_product_level_2").like("%Brokerage%")) |
            (F.col("sub_product_level_2").like("%Advisory%")),
            "INVESTMENT"
        )
        .when(F.col("prod_lob") == "NETWORK", "NETWORK_PRODUCTS")
        .when(
            (F.col("sub_product_level_1") == "NETWORK PRODUCTS") |
            (F.col("sub_product_level_2") == "NETWORK PRODUCTS"),
            "NETWORK_PRODUCTS"
        )
        .when(
            (F.col("prod_lob") == "OTHERS") & (F.col("sub_product_level_1") == "HAS"),
            "DISABILITY"
        )
        .when(F.col("sub_product_level_2") == "HAS - DISABILITY", "DISABILITY")
        .when(F.col("prod_lob") == "OTHERS", "HEALTH")
        .when(F.col("sub_product_level_2") == "GROUP HEALTH PRODUCTS", "HEALTH")
        .otherwise("OTHER")
    )
    
    # Drop the helper columns used for product_category creation
    df_metrics = df_metrics.drop("prod_lob", "sub_product_level_1", "sub_product_level_2")
    
    # Add client_tenure as years between current date and register_date
    from pyspark.sql.functions import current_date, datediff, col
    df_metrics = df_metrics.withColumn(
        "client_tenure",
        (datediff(current_date(), col("register_date")) / 365.25)
    )
    
    # Convert predictions to Spark DataFrame if not already
    spark_df = spark.createDataFrame(predictions)
    
    # Join on cont_id
    spark_df = spark_df.join(df_metrics, on="cont_id", how="left")
    
    # output_table = "eda_smartlist.us_wealth_management_smartlist.ML_predictions_single_policy"
    output_table = "eda_smartlist.us_wealth_management_smartlist.exppp"
    spark_df.write.mode("overwrite").saveAsTable(output_table)
    print(f"✓ Results saved to table: {output_table}")
except Exception as e:
    print(f"⚠ Could not save results: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "=" * 80)
print("Complete!")
print("=" * 80)
