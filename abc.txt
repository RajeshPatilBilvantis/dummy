# ============================================================================
# Example: Predictions for axa_party_id with Only 1 Policy
# ============================================================================
# This cell filters the data to only include axa_party_id who have exactly 1 policy
# and then makes predictions using the preprocess_and_predict_model_lgbm model

import mlflow
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.window import Window

# ============================================================================
# CONFIGURATION
# ============================================================================

CATALOG_NAME = "eda_smartlist"
SCHEMA_NAME = "models"
MODEL_NAME = "preprocess_and_predict_model_lgbm"
MODEL_VERSION = "5"

MODEL_URI = f"models:/{CATALOG_NAME}.{SCHEMA_NAME}.{MODEL_NAME}/{MODEL_VERSION}"

# Table and filter parameters
TABLE_NAME = "dl_tenants_daas.us_wealth_management.wealth_management_client_metrics"
BRANCHOFFICE_CODE = "83"  # Optional: set to None if you want all branch offices
BUSINESS_MONTH = 202510  # Optional: set to None if you want all months

# ============================================================================
# STEP 1: Load Model from Unity Catalog
# ============================================================================

print("=" * 80)
print("Loading Model from Unity Catalog")
print("=" * 80)

try:
    model = mlflow.pyfunc.load_model(MODEL_URI)
    print(f"✓ Model loaded successfully from {MODEL_URI}")
except Exception as e:
    print(f"✗ Failed to load model: {e}")
    import traceback
    traceback.print_exc()
    raise

# ============================================================================
# STEP 2: Load and Filter Data - Only axa_party_id with Exactly 1 Policy
# ============================================================================

print("\n" + "=" * 80)
print("Loading and Filtering Data")
print("=" * 80)

spark = SparkSession.builder.getOrCreate()

# Load data from table
df_raw = spark.table(TABLE_NAME)

# Apply optional filters
if BRANCHOFFICE_CODE:
    df_raw = df_raw.filter(F.col("branchoffice_code") == BRANCHOFFICE_CODE)
    print(f"✓ Filtered by branchoffice_code: {BRANCHOFFICE_CODE}")

if BUSINESS_MONTH:
    df_raw = df_raw.filter(F.col("business_month") == BUSINESS_MONTH)
    print(f"✓ Filtered by business_month = {BUSINESS_MONTH}")

# Filter to only Active policies (required for the model)
df_raw = df_raw.filter(F.col("policy_status") == "Active")
print("✓ Filtered to Active policies only")

# Count distinct policies per axa_party_id
# Assuming each row represents a policy, we count distinct cont_id (or policy_id if available) per axa_party_id
# If there's a policy_id column, use that; otherwise, we'll use cont_id as proxy
# Note: Adjust the column name based on your actual schema

# Option 1: If you have a policy_id column
policy_count_df = df_raw.groupBy("axa_party_id").agg(
    F.countDistinct("policy_no").alias("policy_count")
).filter(F.col("policy_count") == 1)

# # Option 2: If cont_id represents policies (one cont_id = one policy)
# policy_count_df = df_raw.groupBy("axa_party_id").agg(
#     F.countDistinct("cont_id").alias("policy_count")
# ).filter(F.col("policy_count") == 1)

print(f"✓ Found {policy_count_df.count():,} axa_party_id with exactly 1 policy")

# Get the list of axa_party_id with exactly 1 policy
single_policy_party_ids = [row["axa_party_id"] for row in policy_count_df.select("axa_party_id").collect()]
print(f"✓ Extracted {len(single_policy_party_ids):,} unique axa_party_id")

# Filter the original data to only include these axa_party_id
df_filtered = df_raw.filter(F.col("axa_party_id").isin(single_policy_party_ids))

print(f"✓ Filtered data: {df_filtered.count():,} rows")
print(f"  Unique axa_party_id: {df_filtered.select('axa_party_id').distinct().count():,}")
print(f"  Unique cont_id: {df_filtered.select('cont_id').distinct().count():,}")

# ============================================================================
# STEP 3: Make Predictions
# ============================================================================

print("\n" + "=" * 80)
print("Making Predictions")
print("=" * 80)

try:
    # MLflow's schema validation expects a dict with table_name, branchoffice_code, business_month
    # but our model's predict method can handle Spark DataFrames directly
    # We need to bypass MLflow's schema validation by accessing the underlying PythonModel
    print("Accessing model's underlying PythonModel to bypass schema validation...")
    
    # Access the underlying PythonModel directly to bypass MLflow's schema validation
    # The model's load_context has already loaded all artifacts, so we can use the existing context
    python_model = model._model_impl.python_model
    
    # Use the model's existing context (artifacts are already loaded)
    # Pass None as context - our predict method handles this, but artifacts are already loaded in load_context
    predictions = python_model.predict(None, df_filtered)
    
    print(f"✓ Predictions successful")
    print(f"  Shape: {predictions.shape}")
    print(f"  Columns: {list(predictions.columns)}")
    print(f"\n  Sample predictions:")
    display(predictions.head(20))
    
    # Summary statistics
    print(f"\n  Summary:")
    print(f"    Total predictions: {len(predictions):,}")
    print(f"    Unique clients (cont_id): {predictions['cont_id'].nunique():,}")
    print(f"\n  Top predicted products:")
    top_products = predictions['pred_product'].value_counts().head(10)
    for product, count in top_products.items():
        print(f"    {product}: {count:,}")
        
except Exception as e:
    print(f"✗ Prediction failed: {e}")
    import traceback
    traceback.print_exc()
    raise

# ============================================================================
# STEP 4: Save Results (Optional)
# ============================================================================

print("\n" + "=" * 80)
print("Saving Results")
print("=" * 80)

try:
    # Convert to Spark DataFrame and save
    spark_df = spark.createDataFrame(predictions)
    output_table = "eda_smartlist.us_wealth_management_smartlist.predictions_single_policy"
    spark_df.write.mode("overwrite").saveAsTable(output_table)
    print(f"✓ Results saved to table: {output_table}")
except Exception as e:
    print(f"⚠ Could not save results: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "=" * 80)
print("Complete!")
print("=" * 80)
