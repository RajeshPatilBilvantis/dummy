"""
Preprocessing Pipeline for Wealth Management Product Recommendation Model

This module contains all preprocessing functions needed to transform raw client data
into features suitable for the LightGBM product recommendation model.

The preprocessing logic is based on Cell 4 from the training notebook, which focuses on
clients with 2+ policies and uses first policy features to predict the second product.
"""

from pyspark.sql import functions as F, Window
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional


# ============================================================================
# CONSTANTS
# ============================================================================

MIN_POLICIES = 2  # Minimum number of policies required for training examples


# ============================================================================
# PRODUCT CATEGORY MAPPING
# ============================================================================

def create_product_category_column(df):
    """
    Create product_category column from prod_lob, sub_product_level_1, and sub_product_level_2.
    
    Args:
        df: Spark DataFrame with prod_lob, sub_product_level_1, sub_product_level_2 columns
        
    Returns:
        Spark DataFrame with product_category column added
    """
    return df.withColumn(
        "product_category",
        F.when(F.col("prod_lob") == "LIFE", "LIFE_INSURANCE")
        .when(F.col("sub_product_level_1").isin("VLI", "WL", "UL/IUL", "TERM", "PROTECTIVE PRODUCT"), "LIFE_INSURANCE")
        .when(F.col("sub_product_level_2").like("%LIFE%"), "LIFE_INSURANCE")
        .when(F.col("sub_product_level_2").isin(
            "VARIABLE UNIVERSAL LIFE", "WHOLE LIFE", "UNIVERSAL LIFE",
            "INDEX UNIVERSAL LIFE", "TERM PRODUCT", "VARIABLE LIFE",
            "SURVIVORSHIP WHOLE LIFE", "MONY PROTECTIVE PRODUCT"
        ), "LIFE_INSURANCE")
        .when(F.col("prod_lob").isin("GROUP RETIREMENT", "INDIVIDUAL RETIREMENT"), "RETIREMENT")
        .when(F.col("sub_product_level_1").isin(
            "EQUIVEST", "RETIREMENT 401K", "ACCUMULATOR",
            "RETIREMENT CORNERSTONE", "SCS", "INVESTMENT EDGE"
        ), "RETIREMENT")
        .when(
            (F.col("sub_product_level_2").like("%403B%")) |
            (F.col("sub_product_level_2").like("%401%")) |
            (F.col("sub_product_level_2").like("%IRA%")) |
            (F.col("sub_product_level_2").like("%SEP%")),
            "RETIREMENT"
        )
        .when(F.col("prod_lob") == "BROKER DEALER", "INVESTMENT")
        .when(F.col("sub_product_level_1").isin(
            "INVESTMENT PRODUCT - DIRECT", "INVESTMENT PRODUCT - BROKERAGE",
            "INVESTMENT PRODUCT - ADVISORY", "DIRECT", "BROKERAGE",
            "ADVISORY", "CASH SOLICITOR"
        ), "INVESTMENT")
        .when(
            (F.col("sub_product_level_2").like("%Investment%")) |
            (F.col("sub_product_level_2").like("%Brokerage%")) |
            (F.col("sub_product_level_2").like("%Advisory%")),
            "INVESTMENT"
        )
        .when(F.col("prod_lob") == "NETWORK", "NETWORK_PRODUCTS")
        .when(
            (F.col("sub_product_level_1") == "NETWORK PRODUCTS") |
            (F.col("sub_product_level_2") == "NETWORK PRODUCTS"),
            "NETWORK_PRODUCTS"
        )
        .when(
            (F.col("prod_lob") == "OTHERS") & (F.col("sub_product_level_1") == "HAS"),
            "DISABILITY"
        )
        .when(F.col("sub_product_level_2") == "HAS - DISABILITY", "DISABILITY")
        .when(F.col("prod_lob") == "OTHERS", "HEALTH")
        .when(F.col("sub_product_level_2") == "GROUP HEALTH PRODUCTS", "HEALTH")
        .otherwise("OTHER")
    )


# ============================================================================
# FEATURE ENGINEERING
# ============================================================================

def add_asset_allocation_ratios(df):
    """
    Add asset allocation ratio features based on first policy state.
    
    Args:
        df: Spark DataFrame with wc_assetmix_* and wc_total_assets columns
        
    Returns:
        Spark DataFrame with allocation ratio columns added
    """
    df = df.withColumn(
        "stock_allocation_ratio",
        F.col("wc_assetmix_stocks") / F.when(F.col("wc_total_assets") != 0, F.col("wc_total_assets")).otherwise(F.lit(None))
    )
    df = df.withColumn(
        "bond_allocation_ratio",
        F.col("wc_assetmix_bonds") / F.when(F.col("wc_total_assets") != 0, F.col("wc_total_assets")).otherwise(F.lit(None))
    )
    df = df.withColumn(
        "annuity_allocation_ratio",
        F.col("wc_assetmix_annuity") / F.when(F.col("wc_total_assets") != 0, F.col("wc_total_assets")).otherwise(F.lit(None))
    )
    df = df.withColumn(
        "mutual_fund_allocation_ratio",
        F.col("wc_assetmix_mutual_funds") / F.when(F.col("wc_total_assets") != 0, F.col("wc_total_assets")).otherwise(F.lit(None))
    )
    return df


def add_temporal_features(df):
    """
    Add temporal features: season_of_first_policy, age_at_first_policy, years_to_second_policy.
    
    Args:
        df: Spark DataFrame with first_register_ts, second_register_ts, birth_ts columns
        
    Returns:
        Spark DataFrame with temporal feature columns added
    """
    # Season of first policy (Q1-Q4)
    df = df.withColumn(
        "season_of_first_policy",
        F.when(F.month("first_register_ts").between(1, 3), "Q1")
        .when(F.month("first_register_ts").between(4, 6), "Q2")
        .when(F.month("first_register_ts").between(7, 9), "Q3")
        .when(F.month("first_register_ts").between(10, 12), "Q4")
        .otherwise("Unknown")
    )
    
    # Age at first policy (in years)
    df = df.withColumn(
        "age_at_first_policy",
        F.datediff(F.col("first_register_ts"), F.col("birth_ts")) / 365.25
    )
    
    # Years to second policy (time between first and second purchase)
    df = df.withColumn(
        "years_to_second_policy",
        F.datediff(F.col("second_register_ts"), F.col("first_register_ts")) / 365.25
    )
    
    return df


# ============================================================================
# CATEGORICAL ENCODING
# ============================================================================

def encode_categorical_features(df, categorical_cols, spark_context, categorical_mappings=None):
    """
    Encode categorical features to integer indices.
    
    Args:
        df: Spark DataFrame
        categorical_cols: List of categorical column names
        spark_context: SparkContext for broadcasting
        categorical_mappings: Optional dict of {col_name: {value: index}} mappings.
                            If None, mappings will be created from data.
        
    Returns:
        Tuple of (encoded_df, mappings_dict)
    """
    if categorical_mappings is None:
        categorical_mappings = {}
    
    result_df = df
    
    for c in categorical_cols:
        if c in categorical_mappings:
            # Use provided mapping
            mapping = categorical_mappings[c]
            b = spark_context.broadcast(mapping)
            result_df = result_df.withColumn(
                c + "_idx",
                F.udf(lambda s: int(b.value.get(str(s), 0)), IntegerType())(
                    F.coalesce(F.col(c).cast("string"), F.lit("UNKNOWN"))
                )
            )
        else:
            # Create mapping from data
            vals = [r[0] for r in result_df.select(c).distinct().collect()]
            mapping = {v: i for i, v in enumerate(sorted([str(x) for x in vals]))}
            categorical_mappings[c] = mapping
            
            b = spark_context.broadcast(mapping)
            result_df = result_df.withColumn(
                c + "_idx",
                F.udf(lambda s: int(b.value.get(str(s), 0)), IntegerType())(
                    F.coalesce(F.col(c).cast("string"), F.lit("UNKNOWN"))
                )
            )
    
    return result_df, categorical_mappings


# ============================================================================
# MISSING VALUE IMPUTATION
# ============================================================================

def impute_missing_values(df, categorical_cols, numeric_fill=0, categorical_mode="UNKNOWN"):
    """
    Impute missing values in DataFrame.
    
    Args:
        df: Spark DataFrame
        categorical_cols: List of categorical column names
        numeric_fill: Value to fill numeric nulls (default: 0)
        categorical_mode: Value to fill categorical nulls (default: "UNKNOWN")
        
    Returns:
        Spark DataFrame with imputed values
    """
    # Numeric columns to fill
    numeric_cols = [
        "first_acct_val_amt", "first_face_amt", "first_cash_val_amt", "wc_total_assets",
        "wc_assetmix_stocks", "wc_assetmix_bonds", "wc_assetmix_mutual_funds",
        "wc_assetmix_annuity", "wc_assetmix_deposits", "wc_assetmix_other_assets",
        "psn_age", "stock_allocation_ratio", "bond_allocation_ratio",
        "annuity_allocation_ratio", "mutual_fund_allocation_ratio",
        "age_at_first_policy", "years_to_second_policy"
    ]
    
    # Fill numeric nulls
    fill_dict = {c: numeric_fill for c in numeric_cols if c in df.columns}
    result_df = df.fillna(fill_dict)
    
    # Fill categorical nulls with mode
    for c in categorical_cols:
        if c in result_df.columns:
            try:
                mode_val = result_df.groupBy(c).count().orderBy(F.desc("count")).first()
                mode_val = mode_val[0] if mode_val and mode_val[0] is not None else categorical_mode
            except:
                mode_val = categorical_mode
            
            result_df = result_df.withColumn(
                c,
                F.when(F.col(c).isNull(), F.lit(mode_val)).otherwise(F.col(c))
            )
    
    return result_df


# ============================================================================
# MAIN PREPROCESSING FUNCTION FOR TRAINING
# ============================================================================

def preprocess_for_training(
    spark,
    df_raw,
    sample_fraction: Optional[float] = None,
    random_seed: int = 42
):
    """
    Preprocess raw data for training (clients with 2+ policies).
    
    Args:
        spark: SparkSession
        df_raw: Raw Spark DataFrame with client metrics
        sample_fraction: Optional fraction to sample (for faster processing)
        random_seed: Random seed for sampling
        
    Returns:
        Tuple of (train_pd, val_pd, test_pd, model_feature_cols, prod2id, id2prod, 
                 label_map, categorical_mappings, num_classes)
    """
    from pyspark.sql.types import IntegerType
    
    # Step 1: Create product category column
    df = create_product_category_column(df_raw)
    
    # Step 2: Filter and prepare events
    df_events = df.select(
        "cont_id", "product_category", "register_date", "isrd_brth_date",
        "acct_val_amt", "face_amt", "cash_val_amt", "wc_total_assets",
        "wc_assetmix_stocks", "wc_assetmix_bonds", "wc_assetmix_mutual_funds",
        "wc_assetmix_annuity", "wc_assetmix_deposits", "wc_assetmix_other_assets",
        "psn_age", "client_seg", "client_seg_1", "aum_band", "channel", "agent_segment",
        "branchoffice_code", "policy_status"
    ).filter(
        (F.col("cont_id").isNotNull()) &
        (F.col("register_date").isNotNull()) &
        (F.col("product_category").isNotNull()) &
        (F.col("policy_status") == "Active")
    )
    
    if sample_fraction is not None:
        df_events = df_events.sample(withReplacement=False, fraction=float(sample_fraction), seed=random_seed)
    
    # Step 3: Convert dates and order events per client
    df_events = df_events.withColumn("register_ts", F.to_timestamp("register_date"))
    df_events = df_events.withColumn("birth_ts", F.to_timestamp("isrd_brth_date"))
    
    w = Window.partitionBy("cont_id").orderBy("register_ts")
    df_events = df_events.withColumn("event_idx", F.row_number().over(w))
    
    # Step 4: Filter to clients with 2+ policies
    w_count = Window.partitionBy("cont_id")
    df_events = df_events.withColumn("total_policies", F.count("*").over(w_count))
    df_events_multi = df_events.filter(F.col("total_policies") >= MIN_POLICIES)
    
    # Step 5: Separate first and second policy data
    df_first = df_events_multi.filter(F.col("event_idx") == 1).select(
        F.col("cont_id"),
        F.col("product_category").alias("first_product_category"),
        F.col("register_ts").alias("first_register_ts"),
        F.col("birth_ts"),
        F.col("acct_val_amt").alias("first_acct_val_amt"),
        F.col("face_amt").alias("first_face_amt"),
        F.col("cash_val_amt").alias("first_cash_val_amt"),
        F.col("wc_total_assets"),
        F.col("wc_assetmix_stocks"),
        F.col("wc_assetmix_bonds"),
        F.col("wc_assetmix_mutual_funds"),
        F.col("wc_assetmix_annuity"),
        F.col("wc_assetmix_deposits"),
        F.col("wc_assetmix_other_assets"),
        F.col("psn_age"),
        F.col("client_seg"),
        F.col("client_seg_1"),
        F.col("aum_band"),
        F.col("channel"),
        F.col("agent_segment"),
        F.col("branchoffice_code")
    )
    
    df_second = df_events_multi.filter(F.col("event_idx") == 2).select(
        F.col("cont_id"),
        F.col("product_category").alias("second_product_category"),
        F.col("register_ts").alias("second_register_ts")
    )
    
    # Step 6: Join first and second policy on cont_id
    df_combined = df_first.join(df_second, on="cont_id", how="inner")
    
    # Step 7: Add new features
    df_combined = add_asset_allocation_ratios(df_combined)
    df_combined = add_temporal_features(df_combined)
    
    # Step 8: Build product vocabulary for SECOND product (target)
    prod_list = df_combined.select("second_product_category").distinct().rdd.map(lambda r: r[0]).collect()
    prod_list = sorted([p for p in prod_list if p is not None])
    prod2id = {p: i for i, p in enumerate(prod_list)}  # 0-indexed labels
    id2prod = {v: k for k, v in prod2id.items()}
    num_classes = len(prod2id)
    
    # Map second_product_category to label
    df_combined = df_combined.withColumn(
        "label",
        F.when(F.col("second_product_category").isNotNull(), 
               F.udf(lambda x: prod2id.get(x, 0), IntegerType())(F.col("second_product_category")))
        .otherwise(0)
    )
    
    # Create label_map (for compatibility with other code)
    label_map = {prod2id[p]: prod2id[p] for p in prod_list}  # Identity mapping in this case
    
    # Step 9: Fill missing values
    categorical_cols = ["first_product_category", "client_seg", "client_seg_1", "aum_band", 
                        "channel", "agent_segment", "branchoffice_code", "season_of_first_policy"]
    df_combined = impute_missing_values(df_combined, categorical_cols)
    
    # Step 10: Encode categorical features
    df_combined, categorical_mappings = encode_categorical_features(
        df_combined, categorical_cols, spark.sparkContext, categorical_mappings=None
    )
    
    # Step 11: Define model feature columns
    model_feature_cols = [
        # First policy numeric features
        "first_acct_val_amt", "first_face_amt", "first_cash_val_amt", "wc_total_assets",
        "wc_assetmix_stocks", "wc_assetmix_bonds", "wc_assetmix_mutual_funds",
        "wc_assetmix_annuity", "wc_assetmix_deposits", "wc_assetmix_other_assets",
        "psn_age",
        # NEW allocation ratio features
        "stock_allocation_ratio", "bond_allocation_ratio",
        "annuity_allocation_ratio", "mutual_fund_allocation_ratio",
        # NEW temporal features
        "age_at_first_policy", "years_to_second_policy",
        # Encoded categorical features (including FIRST product as feature)
        "first_product_category_idx", "client_seg_idx", "client_seg_1_idx", "aum_band_idx",
        "channel_idx", "agent_segment_idx", "branchoffice_code_idx", "season_of_first_policy_idx"
    ]
    
    # Step 12: Split train/val/test and convert to Pandas
    train_spark, val_spark, test_spark = df_combined.randomSplit([0.8, 0.1, 0.1], seed=random_seed)
    
    train_pd = train_spark.select(["cont_id", "label"] + model_feature_cols).toPandas()
    val_pd = val_spark.select(["cont_id", "label"] + model_feature_cols).toPandas()
    test_pd = test_spark.select(["cont_id", "label"] + model_feature_cols).toPandas()
    
    # Step 13: Final data cleanup
    train_pd.fillna(0, inplace=True)
    val_pd.fillna(0, inplace=True)
    test_pd.fillna(0, inplace=True)
    
    return (train_pd, val_pd, test_pd, model_feature_cols, prod2id, id2prod, 
            label_map, categorical_mappings, num_classes)


# ============================================================================
# MAIN PREPROCESSING FUNCTION FOR PREDICTION
# ============================================================================

def preprocess_for_prediction(
    spark,
    df_raw,
    prod2id: Dict[str, int],
    num_classes: int,
    categorical_cols: Optional[List[str]] = None,
    categorical_mappings: Optional[Dict] = None,
    branchoffice_code: Optional[str] = None,
    business_month: Optional[int] = None,
    single_policy_only: bool = True
):
    """
    Preprocess raw data for prediction.
    
    For single_policy_only=True: Processes clients with only ONE policy (treats it as first policy).
    For single_policy_only=False: Processes clients with 2+ policies (uses first policy features).
    
    Args:
        spark: SparkSession
        df_raw: Raw Spark DataFrame with client metrics
        prod2id: Dictionary mapping product categories to IDs
        num_classes: Number of product classes
        categorical_cols: List of categorical column names
        categorical_mappings: Optional dict of categorical mappings
        branchoffice_code: Optional branch office code filter
        business_month: Optional business month filter
        single_policy_only: If True, process only single-policy clients; if False, process multi-policy clients
        
    Returns:
        Tuple of (preprocessed_pandas_df, feature_columns_list, categorical_mappings)
    """
    from pyspark.sql.types import IntegerType
    
    if categorical_cols is None:
        categorical_cols = ["first_product_category", "client_seg", "client_seg_1", "aum_band", 
                            "channel", "agent_segment", "branchoffice_code", "season_of_first_policy"]
    
    # Step 1: Create product category column
    df = create_product_category_column(df_raw)
    
    # Step 2: Apply filters
    if branchoffice_code:
        df = df.filter(F.col("branchoffice_code") == branchoffice_code)
    
    if business_month:
        df = df.filter(F.col("business_month") <= business_month)
    
    # Step 3: Prepare events data
    df_events = df.select(
        "cont_id", "product_category", "register_date", "isrd_brth_date",
        "acct_val_amt", "face_amt", "cash_val_amt", "wc_total_assets",
        "wc_assetmix_stocks", "wc_assetmix_bonds", "wc_assetmix_mutual_funds",
        "wc_assetmix_annuity", "wc_assetmix_deposits", "wc_assetmix_other_assets",
        "psn_age", "client_seg", "client_seg_1", "aum_band", "channel", "agent_segment",
        "branchoffice_code", "policy_status", "policy_no"
    ).filter(
        (F.col("cont_id").isNotNull()) &
        (F.col("register_date").isNotNull()) &
        (F.col("product_category").isNotNull()) &
        (F.col("policy_status") == "Active")
    )
    
    # Step 4: Convert dates and order events per client
    df_events = df_events.withColumn("register_ts", F.to_timestamp("register_date"))
    df_events = df_events.withColumn("birth_ts", F.to_timestamp("isrd_brth_date"))
    
    w = Window.partitionBy("cont_id").orderBy("register_ts")
    df_events = df_events.withColumn("event_idx", F.row_number().over(w))
    
    # Step 5: Filter based on policy count
    w_count = Window.partitionBy("cont_id")
    df_events = df_events.withColumn("total_policies", F.count("*").over(w_count))
    
    if single_policy_only:
        # Keep only clients with exactly 1 policy
        df_pred_events = df_events.filter(F.col("total_policies") == 1)
    else:
        # Keep clients with 2+ policies
        df_pred_events = df_events.filter(F.col("total_policies") >= MIN_POLICIES)
    
    # Step 6: Prepare first policy data
    df_first = df_pred_events.filter(F.col("event_idx") == 1).select(
        F.col("cont_id"),
        F.col("product_category").alias("first_product_category"),
        F.col("register_ts").alias("first_register_ts"),
        F.col("birth_ts"),
        F.col("acct_val_amt").alias("first_acct_val_amt"),
        F.col("face_amt").alias("first_face_amt"),
        F.col("cash_val_amt").alias("first_cash_val_amt"),
        F.col("wc_total_assets"),
        F.col("wc_assetmix_stocks"),
        F.col("wc_assetmix_bonds"),
        F.col("wc_assetmix_mutual_funds"),
        F.col("wc_assetmix_annuity"),
        F.col("wc_assetmix_deposits"),
        F.col("wc_assetmix_other_assets"),
        F.col("psn_age"),
        F.col("client_seg"),
        F.col("client_seg_1"),
        F.col("aum_band"),
        F.col("channel"),
        F.col("agent_segment"),
        F.col("branchoffice_code")
    )
    
    # Step 7: Add new features
    df_first = add_asset_allocation_ratios(df_first)
    
    # For temporal features, handle single vs multi-policy differently
    if single_policy_only:
        # For single policy clients, set years_to_second_policy to 0
        df_first = df_first.withColumn(
            "season_of_first_policy",
            F.when(F.month("first_register_ts").between(1, 3), "Q1")
            .when(F.month("first_register_ts").between(4, 6), "Q2")
            .when(F.month("first_register_ts").between(7, 9), "Q3")
            .when(F.month("first_register_ts").between(10, 12), "Q4")
            .otherwise("Unknown")
        )
        df_first = df_first.withColumn(
            "age_at_first_policy",
            F.datediff(F.col("first_register_ts"), F.col("birth_ts")) / 365.25
        )
        df_first = df_first.withColumn("years_to_second_policy", F.lit(0.0))
    else:
        # For multi-policy clients, we need second_register_ts
        # This would require joining with second policy data
        # For now, we'll use the same logic as single policy
        df_first = df_first.withColumn(
            "season_of_first_policy",
            F.when(F.month("first_register_ts").between(1, 3), "Q1")
            .when(F.month("first_register_ts").between(4, 6), "Q2")
            .when(F.month("first_register_ts").between(7, 9), "Q3")
            .when(F.month("first_register_ts").between(10, 12), "Q4")
            .otherwise("Unknown")
        )
        df_first = df_first.withColumn(
            "age_at_first_policy",
            F.datediff(F.col("first_register_ts"), F.col("birth_ts")) / 365.25
        )
        # For multi-policy, we'd need to join with second policy to get years_to_second_policy
        # For simplicity, set to 0 (this should be calculated properly in production)
        df_first = df_first.withColumn("years_to_second_policy", F.lit(0.0))
    
    # Step 8: Fill missing values
    df_first = impute_missing_values(df_first, categorical_cols)
    
    # Step 9: Encode categorical features
    df_first, categorical_mappings = encode_categorical_features(
        df_first, categorical_cols, spark.sparkContext, categorical_mappings
    )
    
    # Step 10: Build feature column list
    model_feature_cols = [
        # First policy numeric features
        "first_acct_val_amt", "first_face_amt", "first_cash_val_amt", "wc_total_assets",
        "wc_assetmix_stocks", "wc_assetmix_bonds", "wc_assetmix_mutual_funds",
        "wc_assetmix_annuity", "wc_assetmix_deposits", "wc_assetmix_other_assets",
        "psn_age",
        # NEW allocation ratio features
        "stock_allocation_ratio", "bond_allocation_ratio",
        "annuity_allocation_ratio", "mutual_fund_allocation_ratio",
        # NEW temporal features
        "age_at_first_policy", "years_to_second_policy",
        # Encoded categorical features (including FIRST product as feature)
        "first_product_category_idx", "client_seg_idx", "client_seg_1_idx", "aum_band_idx",
        "channel_idx", "agent_segment_idx", "branchoffice_code_idx", "season_of_first_policy_idx"
    ]
    
    # Step 11: Convert to Pandas and final cleanup
    pred_pd = df_first.select(["cont_id"] + model_feature_cols).toPandas()
    pred_pd.fillna(0, inplace=True)
    
    return pred_pd, model_feature_cols, categorical_mappings


# ============================================================================
# HELPER FUNCTIONS FOR MODEL ARTIFACTS
# ============================================================================

def get_feature_columns(categorical_cols: Optional[List[str]] = None) -> List[str]:
    """
    Get the list of feature columns in the correct order.
    
    Args:
        categorical_cols: List of categorical column names
        
    Returns:
        List of feature column names
    """
    if categorical_cols is None:
        categorical_cols = ["first_product_category", "client_seg", "client_seg_1", "aum_band", 
                            "channel", "agent_segment", "branchoffice_code", "season_of_first_policy"]
    
    return [
        # First policy numeric features
        "first_acct_val_amt", "first_face_amt", "first_cash_val_amt", "wc_total_assets",
        "wc_assetmix_stocks", "wc_assetmix_bonds", "wc_assetmix_mutual_funds",
        "wc_assetmix_annuity", "wc_assetmix_deposits", "wc_assetmix_other_assets",
        "psn_age",
        # NEW allocation ratio features
        "stock_allocation_ratio", "bond_allocation_ratio",
        "annuity_allocation_ratio", "mutual_fund_allocation_ratio",
        # NEW temporal features
        "age_at_first_policy", "years_to_second_policy",
        # Encoded categorical features (including FIRST product as feature)
        "first_product_category_idx", "client_seg_idx", "client_seg_1_idx", "aum_band_idx",
        "channel_idx", "agent_segment_idx", "branchoffice_code_idx", "season_of_first_policy_idx"
    ]

