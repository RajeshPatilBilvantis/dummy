# ===== ENSEMBLE MODELS (OPTIONAL - UNCOMMENT TO USE) =====
# Ensemble can improve performance by 5-10% over single model
# Uncomment below to train ensemble of CatBoost + XGBoost + LightGBM

# Install additional libraries
%pip install xgboost lightgbm

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.metrics import f1_score
import pandas as pd

# Prepare data for XGBoost and LightGBM (need label encoding for categoricals)
le = LabelEncoder()
y_train_encoded = le.fit_transform(train_df['second_product_category'])
y_val_encoded = le.transform(val_df['second_product_category'])

# Get feature columns (exclude target)
X_train = train_df[pool_columns].copy()
X_val = val_df[pool_columns].copy()

# Encode categorical features for XGBoost/LightGBM
cat_indices = [i for i, col in enumerate(pool_columns) if col in categorical_feature_cols]
# Convert object columns to category dtype for XGBoost
for col in X_train.select_dtypes(include='object').columns:
    X_train[col] = X_train[col].astype('category')
    X_val[col] = X_val[col].astype('category')

# For CatBoost, we need column names (not indices) of categorical features
cat_feature_names_ensemble = [col for col in categorical_feature_cols if col in pool_columns]

# 1. XGBoost
xgb_model = XGBClassifier(
    n_estimators=1000,
    max_depth=7,
    learning_rate=0.05,
    objective='multi:softprob',
    eval_metric='mlogloss',
    random_state=42,
    early_stopping_rounds=100,
    use_label_encoder=False, enable_categorical=True
)

# 2. LightGBM
lgb_model = LGBMClassifier(
    n_estimators=1000,
    max_depth=7,
    learning_rate=0.05,
    objective='multiclass',
    random_state=42,
    verbose=-1,
    early_stopping_rounds=100
)

# --- FIX: Create a fresh CatBoost model for ensemble (don't reuse trained model) ---
from catboost import CatBoostClassifier
cat_model_ensemble = CatBoostClassifier(
    iterations=1200,
    depth=7,
    learning_rate=0.05,
    loss_function="MultiClass",
    eval_metric="TotalF1",
    random_seed=42,
    task_type="CPU",
    l2_leaf_reg=3.0,
    subsample=0.85,
    colsample_bylevel=0.85,
    min_data_in_leaf=50,
    early_stopping_rounds=100,
    verbose=100,
    bootstrap_type="Bernoulli",
    class_weights=class_weights_dict,
    grow_policy="SymmetricTree",
    boosting_type="Plain",
    use_best_model=False  # Set to False for ensemble (no eval_set from VotingClassifier)
)

# --- FIX: Wrap CatBoost to always pass cat_features and handle label encoding properly ---
class CatBoostWithCatFeatures(BaseEstimator, ClassifierMixin):
    def __init__(self, model, cat_features, original_classes=None):
        self.model = model
        self.cat_features = cat_features
        self.original_classes = original_classes  # Store original class labels
    
    def __getstate__(self):
        # Ensure original_classes is preserved during pickling/cloning
        return {
            'model': self.model,
            'cat_features': self.cat_features,
            'original_classes': self.original_classes
        }
    
    def __setstate__(self, state):
        # Restore state after unpickling/cloning
        self.model = state['model']
        self.cat_features = state['cat_features']
        self.original_classes = state.get('original_classes', None)
        
    def fit(self, X, y, eval_set=None, **kwargs):
        # Handle numpy arrays directly first
        import numpy as np
        if isinstance(y, np.ndarray):
            if np.issubdtype(y.dtype, np.integer):
                is_integer = True
                y_series = pd.Series(y)
            else:
                is_integer = False
                y_series = pd.Series(y)
        else:
            # Convert to pandas Series for easier handling
            y_series = pd.Series(y) if not isinstance(y, pd.Series) else y
            
            # Check if y contains integers (from VotingClassifier's LabelEncoder)
            # Handle both pandas Series and numpy arrays
            is_integer = False
            if pd.api.types.is_integer_dtype(y_series):
                is_integer = True
            elif y_series.dtype == 'object' and len(y_series) > 0:
                # Check if all values are string representations of integers
                try:
                    is_integer = y_series.str.isdigit().all()
                except:
                    is_integer = False
            elif hasattr(y_series, 'dtype') and 'int' in str(y_series.dtype):
                is_integer = True
        
        # If y contains integers (encoded labels from VotingClassifier), map them back to original string labels
        if is_integer:
            # These are encoded labels from VotingClassifier
            # Map encoded integers back to original string labels
            if self.original_classes is not None and len(self.original_classes) > 0:
                # Create mapping: encoded_int -> original_string
                # VotingClassifier's LabelEncoder encodes in sorted order, so we can map directly
                y = y_series.map(lambda x: self.original_classes[int(x)] if 0 <= int(x) < len(self.original_classes) else str(x)).values
            else:
                # This should not happen if wrapper is created correctly, but provide helpful error
                raise ValueError(
                    f"Integer labels detected but original_classes not provided or empty. "
                    f"Please provide original_classes when initializing the wrapper. "
                    f"Example: CatBoostWithCatFeatures(model, cat_features, original_classes=sorted(train_df['target'].unique()))"
                )
        else:
            # Original string labels - use as-is
            y = y_series.astype(str).values
            # Always update original_classes from the labels (in case VotingClassifier cloned the estimator)
            # This ensures we have the correct mapping even if the estimator was cloned
            unique_classes = sorted(y_series.unique())
            if self.original_classes is None:
                self.original_classes = unique_classes
            # Also verify the classes match (in case there's a mismatch)
            elif set(self.original_classes) != set(unique_classes):
                # If classes don't match, update to the union (shouldn't happen, but be safe)
                self.original_classes = sorted(set(self.original_classes) | set(unique_classes))
        
        if eval_set is not None:
            X_eval, y_eval = eval_set
            y_eval_series = pd.Series(y_eval) if not isinstance(y_eval, pd.Series) else y_eval
            
            # Check if eval_set labels are integers (same logic as above)
            is_eval_integer = False
            if pd.api.types.is_integer_dtype(y_eval_series):
                is_eval_integer = True
            elif y_eval_series.dtype == 'object' and len(y_eval_series) > 0:
                try:
                    is_eval_integer = y_eval_series.str.isdigit().all()
                except:
                    is_eval_integer = False
            elif hasattr(y_eval_series, 'dtype') and 'int' in str(y_eval_series.dtype):
                is_eval_integer = True
            
            # Handle eval_set labels the same way
            if is_eval_integer:
                if self.original_classes is not None:
                    y_eval = y_eval_series.map(lambda x: self.original_classes[int(x)] if 0 <= int(x) < len(self.original_classes) else str(x)).values
                else:
                    raise ValueError("Integer labels detected in eval_set but original_classes not provided.")
            else:
                y_eval = y_eval_series.astype(str).values
                
            return self.model.fit(
                X, y,
                cat_features=self.cat_features,
                eval_set=(X_eval, y_eval),
                **kwargs
            )
        return self.model.fit(
            X, y,
            cat_features=self.cat_features,
            **kwargs
        )
    def predict(self, X):
        return self.model.predict(X)
    def predict_proba(self, X):
        return self.model.predict_proba(X)
    def get_params(self, deep=True):
        return {"model": self.model, "cat_features": self.cat_features}
    def set_params(self, **params):
        for key, value in params.items():
            setattr(self, key, value)
        return self

# Get original class labels before encoding (for mapping encoded labels back)
# VotingClassifier's LabelEncoder encodes in sorted order, so we sort here to match
original_classes = sorted(train_df['second_product_category'].unique())
catboost_wrapper = CatBoostWithCatFeatures(cat_model_ensemble, cat_feature_names_ensemble, original_classes=original_classes)

# Train individual models
print("Training XGBoost...")
xgb_model.fit(
    X_train, y_train_encoded,
    eval_set=[(X_val, y_val_encoded)],
    verbose=False
)

print("Training LightGBM...")
lgb_model.fit(
    X_train, y_train_encoded,
    eval_set=[(X_val, y_val_encoded)],
    categorical_feature=cat_indices
)

# For CatBoost, use string labels (not encoded)
catboost_wrapper.fit(X_train, train_df['second_product_category'])

# Create ensemble
ensemble = VotingClassifier(
    estimators=[
        ('catboost', catboost_wrapper),
        ('xgboost', xgb_model),
        ('lightgbm', lgb_model)
    ],
    voting='soft',
    weights=[2, 1, 1]
)
ensemble.fit(X_train, train_df['second_product_category'])

# Evaluate ensemble
ensemble_pred = ensemble.predict(X_val)
# For XGBoost/LightGBM, predictions are encoded, but for CatBoost and ensemble, they are string labels
# So, no need to decode
f1_macro_ensemble = f1_score(val_df['second_product_category'], ensemble_pred, average='macro')
print(f"\n=== Ensemble Performance ===")
print(f"Ensemble Macro F1: {f1_macro_ensemble:.4f}")
print(f"Single CatBoost Macro F1: {f1_macro:.4f}")
print(f"Improvement: {f1_macro_ensemble - f1_macro:.4f}")

# Note: CatBoost now always uses string labels, fixing the CatBoostError.
