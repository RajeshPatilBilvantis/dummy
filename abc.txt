# ============================================================================
# GLOBAL SHAP PLOTS AND FEATURE IMPORTANCE
# ============================================================================

import matplotlib.pyplot as plt
import seaborn as sns

print("\n" + "=" * 80)
print("GLOBAL SHAP-BASED FEATURE IMPORTANCE")
print("=" * 80)

# Create a SHAP summary plot (requires a notebook backend that can display figures)
try:
    shap.summary_plot(shap_array, pred_with_features[feature_cols], feature_names=feature_cols, show=True)
except Exception as e:
    print(f"âš  Could not generate SHAP summary plot: {e}")

# Compute mean absolute SHAP value per feature for a bar plot
mean_abs_shap = np.mean(np.abs(shap_array), axis=0)
feature_importance_df = pd.DataFrame({
    "feature": feature_cols,
    "mean_abs_shap": mean_abs_shap,
}).sort_values("mean_abs_shap", ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(
    data=feature_importance_df.head(20),
    x="mean_abs_shap",
    y="feature",
    palette="viridis"
)
plt.title("Top 20 Features by Mean |SHAP| (Global Importance)")
plt.xlabel("Mean |SHAP value|")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

print("\nTop 20 features by mean |SHAP|:")
display(feature_importance_df.head(20))

==================


# ============================================================================
# PREDICTION CONFIDENCE AND PRODUCT-LEVEL PATTERNS
# ============================================================================

print("\n" + "=" * 80)
print("PREDICTION CONFIDENCE AND PRODUCT PATTERNS")
print("=" * 80)

# Basic confidence histogram
plt.figure(figsize=(8, 5))
sns.histplot(pred_pd["pred_prob"], bins=30, kde=True)
plt.title("Prediction Confidence Distribution")
plt.xlabel("Predicted probability of recommended product")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

# Confidence by predicted product (boxplot)
plt.figure(figsize=(10, 6))
# Use predicted_product or pred_product depending on column name
pred_product_col = "predicted_product" if "predicted_product" in pred_pd.columns else "pred_product"

sns.boxplot(
    data=pred_pd,
    x=pred_product_col,
    y="pred_prob",
)
plt.title("Prediction Confidence by Product")
plt.xlabel("Predicted product")
plt.ylabel("Predicted probability")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()

# Count of predictions by product
plt.figure(figsize=(10, 5))
prod_counts = pred_pd[pred_product_col].value_counts().reset_index()
prod_counts.columns = ["product", "count"]

sns.barplot(data=prod_counts, x="product", y="count")
plt.title("Number of Predictions by Product")
plt.xlabel("Predicted product")
plt.ylabel("Count")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()

print("\nTop products by prediction volume:")
display(prod_counts.head(10))


==========================


# ============================================================================
# GENERATE DETAILED REASONING FOR EACH PREDICTION
# ============================================================================

print("\n" + "=" * 80)
print("GENERATING DETAILED REASONING")
print("=" * 80)


def format_feature_value(feat_name: str, feat_value: Any, categorical_mappings: Dict) -> str:
    """Format feature value for human-readable display."""
    if feat_name.endswith("_idx"):
        base_name = feat_name.replace("_idx", "")
        if categorical_mappings and base_name in categorical_mappings:
            mapping = categorical_mappings[base_name]
            inv_mapping = {v: k for k, v in mapping.items()}
            try:
                idx_val = int(float(feat_value))
                return inv_mapping.get(idx_val, str(feat_value))
            except:
                return str(feat_value)
    
    # Format numeric values
    if isinstance(feat_value, (int, float)):
        if "ratio" in feat_name or "allocation" in feat_name:
            return f"{feat_value:.1%}"
        elif "age" in feat_name or "years" in feat_name:
            return f"{feat_value:.1f}"
        elif "amt" in feat_name or "assets" in feat_name or "mix" in feat_name:
            return f"${feat_value:,.0f}"
        else:
            return f"{feat_value:,.2f}"
    
    return str(feat_value)


def generate_reasoning(
    cont_id: str,
    pred_product: str,
    pred_prob: float,
    all_probs: np.ndarray,
    feature_values: pd.Series,
    shap_contributions: np.ndarray,
    shap_all_classes: List[np.ndarray],
    feature_cols: List[str],
    final_id2prod: Dict[int, str],
    categorical_mappings: Dict,
) -> Dict[str, Any]:
    """Generate detailed reasoning for a single prediction."""
    
    # Get top contributing features (positive and negative)
    feature_contributions = list(zip(feature_cols, shap_contributions))
    feature_contributions.sort(key=lambda x: abs(float(x[1])), reverse=True)
    
    top_positive = [fc for fc in feature_contributions if fc[1] > 0][:5]
    top_negative = [fc for fc in feature_contributions if fc[1] < 0][:5]
    
    # Get top 3 alternative products (by probability)
    prob_with_class = [(all_probs[i], i) for i in range(len(all_probs))]
    prob_with_class.sort(reverse=True)
    
    top_alternatives = []
    for prob, class_id in prob_with_class[1:4]:  # Skip the predicted one
        product_name = final_id2prod.get(class_id, f"CLASS_{class_id}")
        top_alternatives.append({
            "product": product_name,
            "probability": float(prob),
            "class_id": int(class_id),
        })
    
    # Get SHAP contributions for top alternative
    alternative_reasons = []
    if top_alternatives and len(shap_all_classes) > 1:
        alt_class_id = top_alternatives[0]["class_id"]
        if alt_class_id < len(shap_all_classes):
            # Get SHAP values for alternative class
            if isinstance(shap_all_classes[alt_class_id], np.ndarray):
                if len(shap_all_classes[alt_class_id].shape) == 2:
                    alt_shap = shap_all_classes[alt_class_id][0]  # First sample
                else:
                    alt_shap = shap_all_classes[alt_class_id]
            else:
                alt_shap = shap_all_classes[alt_class_id]
            
            # Features that favor alternative over predicted
            diff_contributions = [
                (feature_cols[i], alt_shap[i] - shap_contributions[i]) 
                for i in range(len(feature_cols))
            ]
            diff_contributions.sort(key=lambda x: abs(float(x[1])), reverse=True)
            
            alternative_reasons = [
                {
                    "feature": fc[0],
                    "value": format_feature_value(fc[0], feature_values[fc[0]], categorical_mappings),
                    "contribution_diff": float(fc[1]),
                }
                for fc in diff_contributions[:3]
            ]
    
    # Build reasoning text
    reasoning_parts = []
    
    # Why this product was recommended
    reasoning_parts.append("## Why This Product Was Recommended")
    reasoning_parts.append(f"The model predicts **{pred_product}** with {pred_prob:.1%} confidence.")
    reasoning_parts.append("\n**Top Contributing Features (Supporting Prediction):**")
    
    for i, (feat_name, shap_val) in enumerate(top_positive[:5], 1):
        feat_value = format_feature_value(feat_name, feature_values[feat_name], categorical_mappings)
        impact = "ðŸ”¥ High" if abs(shap_val) > 0.1 else "â­ Moderate" if abs(shap_val) > 0.05 else "ðŸ“Š Low"
        reasoning_parts.append(f"{i}. **{feat_name}** = {feat_value} ({impact} impact: +{shap_val:.4f})")
    
    if top_negative:
        reasoning_parts.append("\n**Features Reducing Confidence:**")
        for i, (feat_name, shap_val) in enumerate(top_negative[:3], 1):
            feat_value = format_feature_value(feat_name, feature_values[feat_name], categorical_mappings)
            reasoning_parts.append(f"{i}. **{feat_name}** = {feat_value} (reducing confidence: {shap_val:.4f})")
    
    # Confidence score explanation
    reasoning_parts.append("\n## Confidence Score Analysis")
    if pred_prob >= 0.7:
        reasoning_parts.append(f"**High Confidence ({pred_prob:.1%})**: Strong feature alignment with {pred_product} patterns.")
    elif pred_prob >= 0.5:
        reasoning_parts.append(f"**Moderate Confidence ({pred_prob:.1%})**: Reasonable alignment, but some uncertainty remains.")
    else:
        reasoning_parts.append(f"**Low Confidence ({pred_prob:.1%})**: Weak feature alignment; consider reviewing alternative products.")
    
    # Why other products weren't predicted
    reasoning_parts.append("\n## Why Other Products Were Not Predicted")
    reasoning_parts.append("**Top Alternative Products:**")
    
    for alt in top_alternatives:
        prob_diff = pred_prob - alt["probability"]
        reasoning_parts.append(f"- **{alt['product']}**: {alt['probability']:.1%} probability (gap: {prob_diff:.1%})")
    
    if alternative_reasons:
        reasoning_parts.append("\n**Key Differences from Top Alternative:**")
        for reason in alternative_reasons[:3]:
            reasoning_parts.append(f"- **{reason['feature']}** = {reason['value']} (contribution difference: {reason['contribution_diff']:+.4f})")
    
    return {
        "cont_id": cont_id,
        "predicted_product": pred_product,
        "prediction_confidence": pred_prob,
        "reasoning_text": "\n".join(reasoning_parts),
        "top_positive_features": [
            {
                "feature": feat_name,
                "value": format_feature_value(feat_name, feature_values[feat_name], categorical_mappings),
                "shap_contribution": float(shap_val),
            }
            for feat_name, shap_val in top_positive[:5]
        ],
        "top_negative_features": [
            {
                "feature": feat_name,
                "value": format_feature_value(feat_name, feature_values[feat_name], categorical_mappings),
                "shap_contribution": float(shap_val),
            }
            for feat_name, shap_val in top_negative[:3]
        ],
        "top_alternatives": top_alternatives,
        "alternative_reasons": alternative_reasons,
    }


# Generate reasoning for all predictions
reasoning_list = []
print(f"Generating reasoning for {len(pred_with_features):,} predictions...")

for idx in range(len(pred_with_features)):
    if (idx + 1) % 1000 == 0:
        print(f"  Processed {idx + 1:,} / {len(pred_with_features):,}...")
    
    row = pred_with_features.iloc[idx]
    
    # Get SHAP values for this prediction
    if isinstance(shap_all_classes, list) and len(shap_all_classes) > 0:
        pred_class_id = int(row["pred_class_id"]) if "pred_class_id" in row else pred_class_ids[idx]
        if pred_class_id < len(shap_all_classes):
            if isinstance(shap_all_classes[pred_class_id], np.ndarray):
                if len(shap_all_classes[pred_class_id].shape) == 2:
                    shap_for_pred = shap_all_classes[pred_class_id][idx]
                else:
                    shap_for_pred = shap_all_classes[pred_class_id]
            else:
                shap_for_pred = shap_array[idx]
        else:
            shap_for_pred = shap_array[idx]
    else:
        shap_for_pred = shap_array[idx]
    
    # Get all probabilities
    all_probs = pred_probs[idx]
    
    reasoning = generate_reasoning(
        cont_id=str(row["cont_id"]),
        pred_product=str(row.get("predicted_product", row.get("pred_product", "UNKNOWN"))),
        pred_prob=float(row["pred_prob"]),
        all_probs=all_probs,
        feature_values=row[feature_cols],
        shap_contributions=shap_for_pred,
        shap_all_classes=shap_all_classes,
        feature_cols=feature_cols,
        final_id2prod=final_id2prod,
        categorical_mappings=categorical_mappings,
    )
    
    reasoning_list.append(reasoning)

print(f"âœ“ Generated reasoning for {len(reasoning_list):,} predictions")
