{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7fe1adf-fbca-4861-b60c-f32210c6c0ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Query to extract client data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc671e02-b9aa-4a1e-8141-5f7a3be26c35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with base as (\n",
    "  select \n",
    "    r.axa_party_id,\n",
    "    r.policy_no,\n",
    "    r.register_date,\n",
    "    r.trmn_eff_date,\n",
    "    r.wti_lob_txt,\n",
    "    r.prod_lob,\n",
    "    r.agt_class,\n",
    "    r.isrd_brth_date,\n",
    "    r.psn_age,\n",
    "    r.acct_val_amt,\n",
    "    r.face_amt,\n",
    "    r.cash_val_amt,\n",
    "    r.wc_total_assets,\n",
    "    r.wc_assetmix_stocks,\n",
    "    r.wc_assetmix_bonds,\n",
    "    r.wc_assetmix_mutual_funds,\n",
    "    r.wc_assetmix_annuity,\n",
    "    r.wc_assetmix_deposits,\n",
    "    r.wc_assetmix_other_assets,\n",
    "    r.division_name,\n",
    "    r.mkt_prod_hier,\n",
    "    r.policy_status,\n",
    "    r.agent_segment,\n",
    "    r.channel,\n",
    "    r.client_seg,\n",
    "    r.client_seg_1,\n",
    "    r.aum_band,\n",
    "    r.business_month,\n",
    "    r.branchoffice_code,\n",
    "    r.agt_no,\n",
    "    h.sub_product_level_1,\n",
    "    h.sub_product_level_2,\n",
    "    h.Product,\n",
    "    row_number() over (partition by r.axa_party_id order by r.register_date asc) as rn,\n",
    "    row_number() over (partition by r.axa_party_id order by r.register_date asc) = 1 as is_first_policy\n",
    "  from dl_tenants_daas.us_wealth_management.wealth_management_client_metrics r\n",
    "  left join (\n",
    "    select distinct source_sys_id, idb_plan_cd, idb_sub_plan_cd, \n",
    "      trim(stmt_plan_typ_txt) as Product, sub_product_level_1, sub_product_level_2\n",
    "    from dl_tenants_daas.us_wealth_management.wealth_management_sub_product_group\n",
    "  ) h \n",
    "    on upper(r.source_sys_id) = upper(h.source_sys_id)\n",
    "    and trim(upper(REPLACE(LTRIM(REPLACE(r.plan_code,'0',' ')),' ','0'))) = trim(upper(h.idb_plan_cd))\n",
    "    and trim(upper(REPLACE(LTRIM(REPLACE(r.plan_subcd_code,'0',' ')),' ','0'))) = trim(upper(h.idb_sub_plan_cd))\n",
    "  where r.business_month = (select max(business_month) from dl_tenants_daas.us_wealth_management.wealth_management_client_metrics)\n",
    "    and r.axa_party_id is not null\n",
    "    and r.policy_no is not null\n",
    "),\n",
    "first_second as (\n",
    "  select\n",
    "    axa_party_id,\n",
    "    -- First policy fields\n",
    "    max(case when rn = 1 then policy_no end) as policy_no,\n",
    "    max(case when rn = 1 then register_date end) as register_date,\n",
    "    max(case when rn = 1 then trmn_eff_date end) as trmn_eff_date,\n",
    "    max(case when rn = 1 then wti_lob_txt end) as wti_lob_txt,\n",
    "    max(case when rn = 1 then prod_lob end) as prod_lob,\n",
    "    max(case when rn = 1 then agt_class end) as agt_class,\n",
    "    max(case when rn = 1 then isrd_brth_date end) as isrd_brth_date,\n",
    "    max(case when rn = 1 then psn_age end) as psn_age,\n",
    "    max(case when rn = 1 then acct_val_amt end) as acct_val_amt,\n",
    "    max(case when rn = 1 then face_amt end) as face_amt,\n",
    "    max(case when rn = 1 then cash_val_amt end) as cash_val_amt,\n",
    "    max(case when rn = 1 then wc_total_assets end) as wc_total_assets,\n",
    "    max(case when rn = 1 then wc_assetmix_stocks end) as wc_assetmix_stocks,\n",
    "    max(case when rn = 1 then wc_assetmix_bonds end) as wc_assetmix_bonds,\n",
    "    max(case when rn = 1 then wc_assetmix_mutual_funds end) as wc_assetmix_mutual_funds,\n",
    "    max(case when rn = 1 then wc_assetmix_annuity end) as wc_assetmix_annuity,\n",
    "    max(case when rn = 1 then wc_assetmix_deposits end) as wc_assetmix_deposits,\n",
    "    max(case when rn = 1 then wc_assetmix_other_assets end) as wc_assetmix_other_assets,\n",
    "    max(case when rn = 1 then client_seg end) as client_seg,\n",
    "    max(case when rn = 1 then client_seg_1 end) as client_seg_1,\n",
    "    max(case when rn = 1 then aum_band end) as aum_band,\n",
    "    max(case when rn = 1 then sub_product_level_1 end) as sub_product_level_1,\n",
    "    max(case when rn = 1 then sub_product_level_2 end) as sub_product_level_2,\n",
    "    max(case when rn = 1 then Product end) as Product,\n",
    "    max(case when rn = 1 then business_month end) as business_month,\n",
    "    max(case when rn = 1 then branchoffice_code end) as branchoffice_code,\n",
    "    max(case when rn = 1 then agt_no end) as agt_no,\n",
    "    max(case when rn = 1 then division_name end) as division_name,\n",
    "    max(case when rn = 1 then mkt_prod_hier end) as mkt_prod_hier,\n",
    "    max(case when rn = 1 then policy_status end) as policy_status ,\n",
    "    max(case when rn = 1 then channel end) as channel,\n",
    "    max(case when rn = 1 then agent_segment end) as agent_segment,\n",
    "    -- Second policy fields\n",
    "    max(case when rn = 2 then policy_no end) as second_policy_no,\n",
    "    max(case when rn = 2 then register_date end) as second_register_date,\n",
    "    max(case when rn = 2 then trmn_eff_date end) as second_trmn_eff_date,\n",
    "    max(case when rn = 2 then wti_lob_txt end) as second_wti_lob_txt,\n",
    "    max(case when rn = 2 then prod_lob end) as second_prod_lob,\n",
    "    max(case when rn = 2 then sub_product_level_1 end) as second_sub_product_level_1,\n",
    "    max(case when rn = 2 then sub_product_level_2 end) as second_sub_product_level_2,\n",
    "    max(case when rn = 2 then Product end) as second_Product\n",
    "  from base\n",
    "  where rn <= 2\n",
    "  group by axa_party_id\n",
    ")\n",
    "select *,\n",
    "  wc_assetmix_stocks / NULLIF(wc_total_assets, 0) AS stock_allocation_ratio,\n",
    "  wc_assetmix_bonds / NULLIF(wc_total_assets, 0) AS bond_allocation_ratio,\n",
    "  wc_assetmix_annuity / NULLIF(wc_total_assets, 0) AS annuity_allocation_ratio,\n",
    "  wc_assetmix_mutual_funds / NULLIF(wc_total_assets, 0) AS mutual_fund_allocation_ratio,\n",
    "  acct_val_amt / NULLIF(wc_total_assets, 0) AS aum_to_asset_ratio,\n",
    "  face_amt / NULLIF(wc_total_assets, 0) AS policy_value_to_assets_ratio,\n",
    "  \n",
    "  CASE \n",
    "    WHEN prod_lob = 'LIFE' THEN 'LIFE_INSURANCE'\n",
    "    WHEN sub_product_level_1 IN ('VLI', 'WL', 'UL/IUL', 'TERM', 'PROTECTIVE PRODUCT') THEN 'LIFE_INSURANCE'\n",
    "    WHEN sub_product_level_2 LIKE '%LIFE%' THEN 'LIFE_INSURANCE'\n",
    "    WHEN sub_product_level_2 IN ('VARIABLE UNIVERSAL LIFE', 'WHOLE LIFE', 'UNIVERSAL LIFE', \n",
    "                                'INDEX UNIVERSAL LIFE', 'TERM PRODUCT', 'VARIABLE LIFE', \n",
    "                                'SURVIVORSHIP WHOLE LIFE', 'MONY PROTECTIVE PRODUCT') THEN 'LIFE_INSURANCE'\n",
    "    WHEN prod_lob IN ('GROUP RETIREMENT', 'INDIVIDUAL RETIREMENT') THEN 'RETIREMENT'\n",
    "    WHEN sub_product_level_1 IN ('EQUIVEST', 'RETIREMENT 401K', 'ACCUMULATOR', \n",
    "                                'RETIREMENT CORNERSTONE', 'SCS', 'INVESTMENT EDGE') THEN 'RETIREMENT'\n",
    "    WHEN sub_product_level_2 LIKE '%403B%' OR sub_product_level_2 LIKE '%401%' \n",
    "         OR sub_product_level_2 LIKE '%IRA%' OR sub_product_level_2 LIKE '%SEP%' THEN 'RETIREMENT'\n",
    "    WHEN Product LIKE '%IRA%' OR Product LIKE '%401%' OR Product LIKE '%403%' \n",
    "         OR Product LIKE '%SEP%' OR Product LIKE '%Accumulator%' \n",
    "         OR Product LIKE '%Retirement%' THEN 'RETIREMENT'\n",
    "    WHEN prod_lob = 'BROKER DEALER' THEN 'INVESTMENT'\n",
    "    WHEN sub_product_level_1 IN ('INVESTMENT PRODUCT - DIRECT', 'INVESTMENT PRODUCT - BROKERAGE', \n",
    "                                'INVESTMENT PRODUCT - ADVISORY', 'DIRECT', 'BROKERAGE', \n",
    "                                'ADVISORY', 'CASH SOLICITOR') THEN 'INVESTMENT'\n",
    "    WHEN sub_product_level_2 LIKE '%Investment%' OR sub_product_level_2 LIKE '%Brokerage%' \n",
    "         OR sub_product_level_2 LIKE '%Advisory%' THEN 'INVESTMENT'\n",
    "    WHEN prod_lob = 'NETWORK' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN sub_product_level_1 = 'NETWORK PRODUCTS' OR sub_product_level_2 = 'NETWORK PRODUCTS' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN Product LIKE '%Network%' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN prod_lob = 'OTHERS' AND sub_product_level_1 = 'HAS' THEN 'DISABILITY'\n",
    "    WHEN sub_product_level_2 = 'HAS - DISABILITY' THEN 'DISABILITY'\n",
    "    WHEN Product LIKE '%Disability%' OR Product LIKE '%DI -%' THEN 'DISABILITY'\n",
    "    WHEN prod_lob = 'OTHERS' THEN 'HEALTH'\n",
    "    WHEN sub_product_level_2 = 'GROUP HEALTH PRODUCTS' THEN 'HEALTH'\n",
    "    WHEN Product LIKE '%Health%' OR Product LIKE '%Medical%' OR Product LIKE '%Hospital%' THEN 'HEALTH'\n",
    "    ELSE 'OTHER'\n",
    "  END AS product_category,\n",
    "  CASE \n",
    "    WHEN second_prod_lob IS NULL THEN NULL\n",
    "    WHEN second_prod_lob = 'LIFE' THEN 'LIFE_INSURANCE'\n",
    "    WHEN second_sub_product_level_1 IN ('VLI', 'WL', 'UL/IUL', 'TERM', 'PROTECTIVE PRODUCT') THEN 'LIFE_INSURANCE'\n",
    "    WHEN second_sub_product_level_2 LIKE '%LIFE%' THEN 'LIFE_INSURANCE'\n",
    "    WHEN second_sub_product_level_2 IN ('VARIABLE UNIVERSAL LIFE', 'WHOLE LIFE', 'UNIVERSAL LIFE', \n",
    "                                'INDEX UNIVERSAL LIFE', 'TERM PRODUCT', 'VARIABLE LIFE', \n",
    "                                'SURVIVORSHIP WHOLE LIFE', 'MONY PROTECTIVE PRODUCT') THEN 'LIFE_INSURANCE'\n",
    "    WHEN second_prod_lob IN ('GROUP RETIREMENT', 'INDIVIDUAL RETIREMENT') THEN 'RETIREMENT'\n",
    "    WHEN second_sub_product_level_1 IN ('EQUIVEST', 'RETIREMENT 401K', 'ACCUMULATOR', \n",
    "                                'RETIREMENT CORNERSTONE', 'SCS', 'INVESTMENT EDGE') THEN 'RETIREMENT'\n",
    "    WHEN second_sub_product_level_2 LIKE '%403B%' OR second_sub_product_level_2 LIKE '%401%' \n",
    "         OR second_sub_product_level_2 LIKE '%IRA%' OR second_sub_product_level_2 LIKE '%SEP%' THEN 'RETIREMENT'\n",
    "    WHEN second_Product LIKE '%IRA%' OR second_Product LIKE '%401%' OR second_Product LIKE '%403%' \n",
    "         OR second_Product LIKE '%SEP%' OR second_Product LIKE '%Accumulator%' \n",
    "         OR second_Product LIKE '%Retirement%' THEN 'RETIREMENT'\n",
    "    WHEN second_prod_lob = 'BROKER DEALER' THEN 'INVESTMENT'\n",
    "    WHEN second_sub_product_level_1 IN ('INVESTMENT PRODUCT - DIRECT', 'INVESTMENT PRODUCT - BROKERAGE', \n",
    "                                'INVESTMENT PRODUCT - ADVISORY', 'DIRECT', 'BROKERAGE', \n",
    "                                'ADVISORY', 'CASH SOLICITOR') THEN 'INVESTMENT'\n",
    "    WHEN second_sub_product_level_2 LIKE '%Investment%' OR second_sub_product_level_2 LIKE '%Brokerage%' \n",
    "         OR second_sub_product_level_2 LIKE '%Advisory%' THEN 'INVESTMENT'\n",
    "    WHEN second_prod_lob = 'NETWORK' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN second_sub_product_level_1 = 'NETWORK PRODUCTS' OR second_sub_product_level_2 = 'NETWORK PRODUCTS' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN second_Product LIKE '%Network%' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN second_prod_lob = 'OTHERS' AND second_sub_product_level_1 = 'HAS' THEN 'DISABILITY'\n",
    "    WHEN second_sub_product_level_2 = 'HAS - DISABILITY' THEN 'DISABILITY'\n",
    "    WHEN second_Product LIKE '%Disability%' OR second_Product LIKE '%DI -%' THEN 'DISABILITY'\n",
    "    WHEN second_prod_lob = 'OTHERS' THEN 'HEALTH'\n",
    "    WHEN second_sub_product_level_2 = 'GROUP HEALTH PRODUCTS' THEN 'HEALTH'\n",
    "    WHEN second_Product LIKE '%Health%' OR second_Product LIKE '%Medical%' OR second_Product LIKE '%Hospital%' THEN 'HEALTH'\n",
    "    ELSE 'OTHER'\n",
    "  END AS second_product_category,\n",
    "  CASE\n",
    "    WHEN MONTH(register_date) BETWEEN 1 AND 3 THEN 'Q1'\n",
    "    WHEN MONTH(register_date) BETWEEN 4 AND 6 THEN 'Q2'\n",
    "    WHEN MONTH(register_date) BETWEEN 7 AND 9 THEN 'Q3'\n",
    "    WHEN MONTH(register_date) BETWEEN 10 AND 12 THEN 'Q4'\n",
    "    ELSE 'Unknown'\n",
    "  END AS season_of_first_policy\n",
    "  \n",
    "from first_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afc69e2e-c7a4-4a27-b565-c721d9ff3d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = _sqldf.toPandas()\n",
    "# df = pd.read_csv('improve_metrics_JOB.csv')\n",
    "\n",
    "# age at first policy (calculated from dates)\n",
    "df['register_date'] = pd.to_datetime(df['register_date'], errors='coerce')\n",
    "df['isrd_brth_date'] = pd.to_datetime(df['isrd_brth_date'], errors='coerce')\n",
    "df['age_at_first_policy'] = (df['register_date'] - df['isrd_brth_date']).dt.days / 365.25\n",
    "\n",
    "# age at second policy\n",
    "df['second_register_date'] = pd.to_datetime(df['second_register_date'], errors='coerce')\n",
    "df['age_at_second_policy'] = (df['second_register_date'] - df['isrd_brth_date']).dt.days / 365.25\n",
    "\n",
    "# time gap between first and second policy\n",
    "df['years_to_second'] = (df['second_register_date'] - df['register_date']).dt.days / 365.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b60f0f9-da6a-41da-a8a9-c3b59f589cc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>axa_party_id</th>\n",
       "      <th>policy_no</th>\n",
       "      <th>register_date</th>\n",
       "      <th>trmn_eff_date</th>\n",
       "      <th>wti_lob_txt</th>\n",
       "      <th>prod_lob</th>\n",
       "      <th>agt_class</th>\n",
       "      <th>isrd_brth_date</th>\n",
       "      <th>psn_age</th>\n",
       "      <th>acct_val_amt</th>\n",
       "      <th>...</th>\n",
       "      <th>annuity_allocation_ratio</th>\n",
       "      <th>mutual_fund_allocation_ratio</th>\n",
       "      <th>aum_to_asset_ratio</th>\n",
       "      <th>policy_value_to_assets_ratio</th>\n",
       "      <th>product_category</th>\n",
       "      <th>second_product_category</th>\n",
       "      <th>season_of_first_policy</th>\n",
       "      <th>age_at_first_policy</th>\n",
       "      <th>age_at_second_policy</th>\n",
       "      <th>years_to_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00BK05RY274T637VXXXX</td>\n",
       "      <td>011492461</td>\n",
       "      <td>1942-02-07</td>\n",
       "      <td>2025-02-07</td>\n",
       "      <td>Life Insurance</td>\n",
       "      <td>LIFE</td>\n",
       "      <td>ESF - EXPERIENCED SALESFORCE</td>\n",
       "      <td>1928-11-26</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031985</td>\n",
       "      <td>0.240678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>LIFE_INSURANCE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q1</td>\n",
       "      <td>13.199179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00BK05RY27EMUV5SXXXX</td>\n",
       "      <td>011995497</td>\n",
       "      <td>1945-07-19</td>\n",
       "      <td>2025-07-19</td>\n",
       "      <td>Life Insurance</td>\n",
       "      <td>LIFE</td>\n",
       "      <td>ESF - EXPERIENCED SALESFORCE</td>\n",
       "      <td>1944-04-07</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021641</td>\n",
       "      <td>0.090285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021939</td>\n",
       "      <td>LIFE_INSURANCE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q3</td>\n",
       "      <td>1.281314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00BK05RY27JF1H4DXXXX</td>\n",
       "      <td>012258775</td>\n",
       "      <td>1946-09-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Life Insurance</td>\n",
       "      <td>LIFE</td>\n",
       "      <td>ESF - EXPERIENCED SALESFORCE</td>\n",
       "      <td>1939-04-21</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1818.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043612</td>\n",
       "      <td>0.546774</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>LIFE_INSURANCE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q3</td>\n",
       "      <td>7.373032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00BK05RY27L0GNKUXXXX</td>\n",
       "      <td>012370262</td>\n",
       "      <td>1947-03-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Life Insurance</td>\n",
       "      <td>LIFE</td>\n",
       "      <td>ESF - EXPERIENCED SALESFORCE</td>\n",
       "      <td>1947-01-16</td>\n",
       "      <td>79.0</td>\n",
       "      <td>869.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LIFE_INSURANCE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q1</td>\n",
       "      <td>0.175222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00BK05RY27QPLN0IXXXX</td>\n",
       "      <td>012710157</td>\n",
       "      <td>1948-06-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Life Insurance</td>\n",
       "      <td>LIFE</td>\n",
       "      <td>DSF - DEVELOPED SALESFORCE</td>\n",
       "      <td>1940-09-02</td>\n",
       "      <td>85.0</td>\n",
       "      <td>941.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031777</td>\n",
       "      <td>0.265780</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.002738</td>\n",
       "      <td>LIFE_INSURANCE</td>\n",
       "      <td>RETIREMENT</td>\n",
       "      <td>Q2</td>\n",
       "      <td>7.819302</td>\n",
       "      <td>64.722793</td>\n",
       "      <td>56.903491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           axa_party_id  policy_no register_date trmn_eff_date  \\\n",
       "0  00BK05RY274T637VXXXX  011492461    1942-02-07    2025-02-07   \n",
       "1  00BK05RY27EMUV5SXXXX  011995497    1945-07-19    2025-07-19   \n",
       "2  00BK05RY27JF1H4DXXXX  012258775    1946-09-04           NaN   \n",
       "3  00BK05RY27L0GNKUXXXX  012370262    1947-03-21           NaN   \n",
       "4  00BK05RY27QPLN0IXXXX  012710157    1948-06-28           NaN   \n",
       "\n",
       "      wti_lob_txt prod_lob                     agt_class isrd_brth_date  \\\n",
       "0  Life Insurance     LIFE  ESF - EXPERIENCED SALESFORCE     1928-11-26   \n",
       "1  Life Insurance     LIFE  ESF - EXPERIENCED SALESFORCE     1944-04-07   \n",
       "2  Life Insurance     LIFE  ESF - EXPERIENCED SALESFORCE     1939-04-21   \n",
       "3  Life Insurance     LIFE  ESF - EXPERIENCED SALESFORCE     1947-01-16   \n",
       "4  Life Insurance     LIFE    DSF - DEVELOPED SALESFORCE     1940-09-02   \n",
       "\n",
       "   psn_age  acct_val_amt  ...  annuity_allocation_ratio  \\\n",
       "0     97.0           0.0  ...                  0.031985   \n",
       "1     82.0           0.0  ...                  0.021641   \n",
       "2     87.0        1818.0  ...                  0.043612   \n",
       "3     79.0         869.0  ...                       NaN   \n",
       "4     85.0         941.0  ...                  0.031777   \n",
       "\n",
       "   mutual_fund_allocation_ratio  aum_to_asset_ratio  \\\n",
       "0                      0.240678            0.000000   \n",
       "1                      0.090285            0.000000   \n",
       "2                      0.546774            0.000271   \n",
       "3                           NaN                 NaN   \n",
       "4                      0.265780            0.002576   \n",
       "\n",
       "   policy_value_to_assets_ratio  product_category  second_product_category  \\\n",
       "0                      0.000235    LIFE_INSURANCE                      NaN   \n",
       "1                      0.021939    LIFE_INSURANCE                      NaN   \n",
       "2                      0.000298    LIFE_INSURANCE                      NaN   \n",
       "3                           NaN    LIFE_INSURANCE                      NaN   \n",
       "4                      0.002738    LIFE_INSURANCE               RETIREMENT   \n",
       "\n",
       "   season_of_first_policy  age_at_first_policy  age_at_second_policy  \\\n",
       "0                      Q1            13.199179                   NaN   \n",
       "1                      Q3             1.281314                   NaN   \n",
       "2                      Q3             7.373032                   NaN   \n",
       "3                      Q1             0.175222                   NaN   \n",
       "4                      Q2             7.819302             64.722793   \n",
       "\n",
       "  years_to_second  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4       56.903491  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aff859e-33b2-44fe-9c28-2c2ef01c0b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "\n",
    "numerical_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "exclude_cols = ['years_to_second', 'days_to_second', 'age_at_first_policy', 'age_at_second_policy']\n",
    "numerical_features = [col for col in numerical_features if col not in exclude_cols]\n",
    "\n",
    "for col in ['acct_val_amt', 'face_amt', 'cash_val_amt']:\n",
    "    if col in df.columns and col not in numerical_features:\n",
    "        numerical_features = list(numerical_features) + [col]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "260338b5-8729-4e12-89d2-8de3aee2c2f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **DATA CLEANING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2f878379-c952-4c3c-aa69-47699480b878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['axa_party_id', 'policy_no', 'register_date', 'trmn_eff_date',\n",
       "       'wti_lob_txt', 'prod_lob', 'agt_class', 'isrd_brth_date', 'psn_age',\n",
       "       'acct_val_amt', 'face_amt', 'cash_val_amt', 'wc_total_assets',\n",
       "       'wc_assetmix_stocks', 'wc_assetmix_bonds', 'wc_assetmix_mutual_funds',\n",
       "       'wc_assetmix_annuity', 'wc_assetmix_deposits',\n",
       "       'wc_assetmix_other_assets', 'client_seg', 'client_seg_1', 'aum_band',\n",
       "       'sub_product_level_1', 'sub_product_level_2', 'Product',\n",
       "       'business_month', 'branchoffice_code', 'agt_no', 'division_name',\n",
       "       'mkt_prod_hier', 'policy_status', 'channel', 'agent_segment',\n",
       "       'second_policy_no', 'second_register_date', 'second_trmn_eff_date',\n",
       "       'second_wti_lob_txt', 'second_prod_lob', 'second_sub_product_level_1',\n",
       "       'second_sub_product_level_2', 'second_Product',\n",
       "       'stock_allocation_ratio', 'bond_allocation_ratio',\n",
       "       'annuity_allocation_ratio', 'mutual_fund_allocation_ratio',\n",
       "       'aum_to_asset_ratio', 'policy_value_to_assets_ratio',\n",
       "       'product_category', 'second_product_category', 'season_of_first_policy',\n",
       "       'age_at_first_policy', 'age_at_second_policy', 'years_to_second'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26bbc16a-4ec4-4427-b658-3e3e5246bfd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Dropping duplicates is important in ML to prevent data leakage, reduce bias, and ensure the model does not overfit to repeated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44626a45-ad25-4613-8085-3eed57922a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e87d65e2-9297-4fdc-aebf-ee44a5715932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Handle missing values - missing values in critical features can lead to unreliable model training, errors during fitting, or biased results.\n",
    "Removing such rows ensures data quality and model integrity.\n",
    "\n",
    "Separating numerical and categorical columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e79dd5-2284-4d28-8dd2-26e3a1ef3881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['psn_age', 'acct_val_amt', 'face_amt', 'cash_val_amt',\n",
       "       'wc_total_assets', 'wc_assetmix_stocks', 'wc_assetmix_bonds',\n",
       "       'wc_assetmix_mutual_funds', 'wc_assetmix_annuity',\n",
       "       'wc_assetmix_deposits', 'wc_assetmix_other_assets', 'business_month',\n",
       "       'branchoffice_code', 'agt_no', 'stock_allocation_ratio',\n",
       "       'bond_allocation_ratio', 'annuity_allocation_ratio',\n",
       "       'mutual_fund_allocation_ratio', 'aum_to_asset_ratio',\n",
       "       'policy_value_to_assets_ratio', 'age_at_first_policy',\n",
       "       'age_at_second_policy', 'years_to_second'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['axa_party_id', 'policy_no', 'trmn_eff_date', 'wti_lob_txt', 'prod_lob',\n",
       "       'agt_class', 'client_seg', 'client_seg_1', 'aum_band',\n",
       "       'sub_product_level_1', 'sub_product_level_2', 'Product',\n",
       "       'division_name', 'mkt_prod_hier', 'policy_status', 'channel',\n",
       "       'agent_segment', 'second_policy_no', 'second_trmn_eff_date',\n",
       "       'second_wti_lob_txt', 'second_prod_lob', 'second_sub_product_level_1',\n",
       "       'second_sub_product_level_2', 'second_Product', 'product_category',\n",
       "       'second_product_category', 'season_of_first_policy'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Handle missing values\n",
    "\n",
    "# drop rows with missing target or critical features\n",
    "critical_cols = ['product_category']\n",
    "df = df.dropna(subset=critical_cols)\n",
    "\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "display(num_cols)\n",
    "\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "display(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c775a516-eb80-4ff2-a63d-3e4d501ff326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wc_total_assets</th>\n",
       "      <th>wc_assetmix_stocks</th>\n",
       "      <th>wc_assetmix_bonds</th>\n",
       "      <th>wc_assetmix_mutual_funds</th>\n",
       "      <th>wc_assetmix_annuity</th>\n",
       "      <th>wc_assetmix_deposits</th>\n",
       "      <th>wc_assetmix_other_assets</th>\n",
       "      <th>face_amt</th>\n",
       "      <th>cash_val_amt</th>\n",
       "      <th>acct_val_amt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.299423</td>\n",
       "      <td>3.949428</td>\n",
       "      <td>7.215579</td>\n",
       "      <td>3.776952</td>\n",
       "      <td>4.721003</td>\n",
       "      <td>6.720008</td>\n",
       "      <td>9.29926</td>\n",
       "      <td>17.432612</td>\n",
       "      <td>11.072645</td>\n",
       "      <td>8.773947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wc_total_assets  wc_assetmix_stocks  wc_assetmix_bonds  \\\n",
       "0         3.299423            3.949428           7.215579   \n",
       "\n",
       "   wc_assetmix_mutual_funds  wc_assetmix_annuity  wc_assetmix_deposits  \\\n",
       "0                  3.776952             4.721003              6.720008   \n",
       "\n",
       "   wc_assetmix_other_assets   face_amt  cash_val_amt  acct_val_amt  \n",
       "0                   9.29926  17.432612     11.072645      8.773947  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajesh/.pyenv/versions/3.11.6/envs/my-vscode-env/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_wc_total_assets</th>\n",
       "      <th>log_wc_assetmix_stocks</th>\n",
       "      <th>log_wc_assetmix_bonds</th>\n",
       "      <th>log_wc_assetmix_mutual_funds</th>\n",
       "      <th>log_wc_assetmix_annuity</th>\n",
       "      <th>log_wc_assetmix_deposits</th>\n",
       "      <th>log_wc_assetmix_other_assets</th>\n",
       "      <th>log_face_amt</th>\n",
       "      <th>log_cash_val_amt</th>\n",
       "      <th>log_acct_val_amt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.252003</td>\n",
       "      <td>-1.65398</td>\n",
       "      <td>-1.124151</td>\n",
       "      <td>-2.121249</td>\n",
       "      <td>-2.238858</td>\n",
       "      <td>-5.091397</td>\n",
       "      <td>-1.165425</td>\n",
       "      <td>-1.503723</td>\n",
       "      <td>0.535675</td>\n",
       "      <td>-0.563845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_wc_total_assets  log_wc_assetmix_stocks  log_wc_assetmix_bonds  \\\n",
       "0            -3.252003                -1.65398              -1.124151   \n",
       "\n",
       "   log_wc_assetmix_mutual_funds  log_wc_assetmix_annuity  \\\n",
       "0                     -2.121249                -2.238858   \n",
       "\n",
       "   log_wc_assetmix_deposits  log_wc_assetmix_other_assets  log_face_amt  \\\n",
       "0                 -5.091397                     -1.165425     -1.503723   \n",
       "\n",
       "   log_cash_val_amt  log_acct_val_amt  \n",
       "0          0.535675         -0.563845  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "# List of financial columns\n",
    "financial_cols = [col for col in df.columns if col.startswith('wc_')] + ['face_amt', 'cash_val_amt', 'acct_val_amt']\n",
    "financial_cols = [col for col in financial_cols if col in df.columns]\n",
    "\n",
    "# Compute skewness for each financial column\n",
    "skewness_dict = {col: stats.skew(df[col].dropna()) for col in financial_cols}\n",
    "skew_df = pd.DataFrame([skewness_dict])\n",
    "display(skew_df)\n",
    "\n",
    "# Apply log1p transformation to reduce skewness\n",
    "for col in financial_cols:\n",
    "    df[f'log_{col}'] = np.log1p(df[col])\n",
    "\n",
    "# Compute skewness for each log-transformed financial column\n",
    "log_skewness_dict = {f'log_{col}': stats.skew(df[f'log_{col}'].dropna()) for col in financial_cols}\n",
    "log_skew_df = pd.DataFrame([log_skewness_dict])\n",
    "display(log_skew_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f85864d-6188-45a2-8a03-3a37fe872a62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Standardizing date columns ensures all date fields are in a consistent datetime format,\n",
    "which is necessary for reliable feature engineering (e.g., calculating durations, extracting year/month).\n",
    "\n",
    "Removing outliers in numerical features (e.g., age_at_first_policy) prevents extreme values from distracting the model during training,\n",
    "leading to more robust and generalizable models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "716c67eb-0fef-4b35-8edd-de975c7d5ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standardize date columns\n",
    "date_cols = ['register_date', 'second_register_date', 'isrd_brth_date', 'trmn_eff_date', 'second_trmn_eff_date']\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# Remove outliers in numerical features\n",
    "df = df[(df['age_at_first_policy'] >= 0) & (df['age_at_first_policy'] <= 100)]\n",
    "\n",
    "# Categorical encoding (LabelEncoder is correct for tree models, but for Spark MLlib, use StringIndexer)\n",
    "cat_cols = [\n",
    "    'product_category', 'prod_lob', 'client_seg', 'aum_band', 'agt_class', 'season_of_first_policy', 'client_seg_1', 'division_name','mkt_prod_hier', 'policy_status', 'channel', 'agent_segment']\n",
    "for col in cat_cols + ['second_product_category']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6448a3cd-4542-479a-b67d-93054c92a998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['axa_party_id', 'policy_no', 'register_date', 'trmn_eff_date',\n",
       "       'wti_lob_txt', 'prod_lob', 'agt_class', 'isrd_brth_date', 'psn_age',\n",
       "       'acct_val_amt', 'face_amt', 'cash_val_amt', 'wc_total_assets',\n",
       "       'wc_assetmix_stocks', 'wc_assetmix_bonds', 'wc_assetmix_mutual_funds',\n",
       "       'wc_assetmix_annuity', 'wc_assetmix_deposits',\n",
       "       'wc_assetmix_other_assets', 'client_seg', 'client_seg_1', 'aum_band',\n",
       "       'sub_product_level_1', 'sub_product_level_2', 'Product',\n",
       "       'business_month', 'branchoffice_code', 'agt_no', 'division_name',\n",
       "       'mkt_prod_hier', 'policy_status', 'channel', 'agent_segment',\n",
       "       'second_policy_no', 'second_register_date', 'second_trmn_eff_date',\n",
       "       'second_wti_lob_txt', 'second_prod_lob', 'second_sub_product_level_1',\n",
       "       'second_sub_product_level_2', 'second_Product',\n",
       "       'stock_allocation_ratio', 'bond_allocation_ratio',\n",
       "       'annuity_allocation_ratio', 'mutual_fund_allocation_ratio',\n",
       "       'aum_to_asset_ratio', 'policy_value_to_assets_ratio',\n",
       "       'product_category', 'second_product_category', 'season_of_first_policy',\n",
       "       'age_at_first_policy', 'age_at_second_policy', 'years_to_second',\n",
       "       'log_wc_total_assets', 'log_wc_assetmix_stocks',\n",
       "       'log_wc_assetmix_bonds', 'log_wc_assetmix_mutual_funds',\n",
       "       'log_wc_assetmix_annuity', 'log_wc_assetmix_deposits',\n",
       "       'log_wc_assetmix_other_assets', 'log_face_amt', 'log_cash_val_amt',\n",
       "       'log_acct_val_amt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ba69537-7e5e-4906-b35f-421358009ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Dropping highly correlated numerical features is important because:\n",
    "1. It reduces multicollinearity, which can negatively impact model interpretability and stability.\n",
    "2. It prevents redundant information, making models more efficient and less prone to overfitting.\n",
    "3. It improves training speed and can enhance generalization by reducing noise from duplicate signals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55aa2a54-488e-43f0-9dea-dc6a73cf7e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\n",
    "    'log_wc_assetmix_stocks',\n",
    "    'log_wc_assetmix_bonds',\n",
    "    'log_wc_assetmix_mutual_funds',\n",
    "    'log_wc_assetmix_deposits',\n",
    "    'log_wc_assetmix_other_assets',\n",
    "    'log_acct_val_amt'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d4e72f-f50c-45ea-8780-dc5115943a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Correlation matrix after removing highly correlated features\n",
    "\n",
    "numerical_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "exclude_cols = ['years_to_second', 'days_to_second', 'age_at_first_policy', 'age_at_second_policy', 'log_wc_assetmix_stocks', 'log_wc_assetmix_bonds', 'log_wc_assetmix_mutual_funds', 'log_wc_assetmix_deposits', 'log_wc_assetmix_other_assets']\n",
    "numerical_features = [col for col in numerical_features if col not in exclude_cols and not col.startswith('wc')]\n",
    "\n",
    "for col in ['face_amt', 'cash_val_amt']:\n",
    "    if col in df.columns and col not in numerical_features:\n",
    "        numerical_features = list(numerical_features) + [col]\n",
    "\n",
    "corr_matrix = df[numerical_features].corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e32c395c-4868-4482-9881-e8942a8fd0dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['axa_party_id', 'policy_no', 'register_date', 'trmn_eff_date',\n",
       "       'wti_lob_txt', 'prod_lob', 'agt_class', 'isrd_brth_date', 'psn_age',\n",
       "       'acct_val_amt', 'face_amt', 'cash_val_amt', 'wc_total_assets',\n",
       "       'wc_assetmix_stocks', 'wc_assetmix_bonds', 'wc_assetmix_mutual_funds',\n",
       "       'wc_assetmix_annuity', 'wc_assetmix_deposits',\n",
       "       'wc_assetmix_other_assets', 'client_seg', 'client_seg_1', 'aum_band',\n",
       "       'sub_product_level_1', 'sub_product_level_2', 'Product',\n",
       "       'business_month', 'branchoffice_code', 'agt_no', 'division_name',\n",
       "       'mkt_prod_hier', 'policy_status', 'channel', 'agent_segment',\n",
       "       'second_policy_no', 'second_register_date', 'second_trmn_eff_date',\n",
       "       'second_wti_lob_txt', 'second_prod_lob', 'second_sub_product_level_1',\n",
       "       'second_sub_product_level_2', 'second_Product',\n",
       "       'stock_allocation_ratio', 'bond_allocation_ratio',\n",
       "       'annuity_allocation_ratio', 'mutual_fund_allocation_ratio',\n",
       "       'aum_to_asset_ratio', 'policy_value_to_assets_ratio',\n",
       "       'product_category', 'second_product_category', 'season_of_first_policy',\n",
       "       'age_at_first_policy', 'age_at_second_policy', 'years_to_second',\n",
       "       'log_wc_total_assets', 'log_wc_assetmix_annuity', 'log_face_amt',\n",
       "       'log_cash_val_amt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de5ab69b-7ee9-4b97-b6bc-3ab635bc44bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Medium Imputation performed for the numerical (ratio) columns - this ensures that all records are usable, improving training efficiency and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install synapseml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db84aa4b-6461-488d-9848-24b5ca834ac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Spark context is not available. Please run this cell in a Databricks notebook with an active cluster.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     11\u001b[39m allocation_cols = [\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mstock_allocation_ratio\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mbond_allocation_ratio\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mannuity_allocation_ratio\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmutual_fund_allocation_ratio\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33maum_to_asset_ratio\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpolicy_value_to_assets_ratio\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     14\u001b[39m ]\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mspark\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mor\u001b[39;00m spark \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSpark context is not available. Please run this cell in a Databricks notebook with an active cluster.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(df, \u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdf is not defined or not a pandas DataFrame.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Spark context is not available. Please run this cell in a Databricks notebook with an active cluster."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, Imputer\n",
    "from pyspark.ml.classification import GBTClassifier, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from synapse.ml.lightgbm import LightGBMClassifier\n",
    "\n",
    "# Median imputation for allocation ratio columns\n",
    "allocation_cols = [\n",
    "    'stock_allocation_ratio', 'bond_allocation_ratio', 'annuity_allocation_ratio',\n",
    "    'mutual_fund_allocation_ratio', 'aum_to_asset_ratio', 'policy_value_to_assets_ratio'\n",
    "]\n",
    "\n",
    "if 'spark' not in globals() or spark is None:\n",
    "    raise RuntimeError(\"Spark context is not available. Please run this cell in a Databricks notebook with an active cluster.\")\n",
    "\n",
    "if not hasattr(df, 'columns'):\n",
    "    raise RuntimeError(\"df is not defined or not a pandas DataFrame.\")\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=[c for c in allocation_cols if c in df.columns],\n",
    "    outputCols=[c for c in allocation_cols if c in df.columns],\n",
    "    strategy=\"median\"\n",
    ")\n",
    "spark_df = spark.createDataFrame(df)\n",
    "df_imputed = imputer.fit(spark_df).transform(spark_df)\n",
    "spark_df = df_imputed\n",
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0052324e-bd56-4f14-bd3a-718dbae47e74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Function for adding Propensity features - we're creating propensity features using train data and adding them to train and unseen data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f40d72d5-af74-426f-b3ef-c70455de256e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_propensity_features(df, lookup_prod, lookup_mode, lookup_agent, lookup_branch):\n",
    "    df_with_features = df.withColumnRenamed(\"product_category\", \"prod_code\")\n",
    "    df_with_features = df_with_features.join(lookup_prod, on=\"prod_code\", how=\"left\")\n",
    "    df_with_features = df_with_features.join(lookup_mode, on=\"prod_code\", how=\"left\")\n",
    "    df_with_features = df_with_features.join(lookup_agent, on=[\"agt_no\", \"prod_code\"], how=\"left\")\n",
    "    df_with_features = df_with_features.join(lookup_branch, on=[\"branchoffice_code\", \"prod_code\"], how=\"left\")\n",
    "    df_with_features = df_with_features.na.fill(0, [\n",
    "        \"p1_cross_sell_popularity\",\n",
    "        \"agent_p1_cross_sell_count\",\n",
    "        \"branch_p1_cross_sell_count\"\n",
    "    ])\n",
    "    df_with_features = df_with_features.na.fill(\"UNKNOWN\", [\"p1_most_common_next_prod\"])\n",
    "    df_with_features = df_with_features.withColumnRenamed(\"prod_code\", \"product_category\")\n",
    "    return df_with_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74ff44fe-f99c-450e-9ab8-1907dd72e640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Using rows with a second policy for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "038d0563-5d47-46cf-b636-402f33b43396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_financial_cols = [\n",
    "    'log_wc_total_assets', 'log_wc_assetmix_annuity', 'log_wc_assetmix_other_assets',\n",
    "    'log_face_amt', 'log_cash_val_amt'\n",
    "]\n",
    "\n",
    "# Train only on rows with second_product_category (i.e., with a second policy)\n",
    "train_df = spark_df.filter(col(\"second_product_category\").isNotNull())\n",
    "for colname in ['age_at_first_policy', 'years_to_second'] + [col for col in log_financial_cols if col in df.columns]:\n",
    "    train_df = train_df.filter(col(colname).isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35d831f7-17e8-4832-9f3a-e1dcaee551eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split train_df into training and validation sets\n",
    "train_data, val_data = train_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b1a7aa-0efd-46e7-8b09-6ef8fec9b8ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:434)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:434)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining a new df for creating propensity features\n",
    "\n",
    "cross_sell_history_df = train_data.select(\n",
    "    F.col(\"product_category\").alias(\"first_prod_code\"),\n",
    "    F.col(\"second_product_category\").alias(\"second_prod_code\"),\n",
    "    \"agt_no\",\n",
    "    \"branchoffice_code\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84dd9175-39c7-4ceb-93df-ebca45026de8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "FEATURE ENGINEERING - PROPENSITY FEATURES\n",
    "- Product propensity - Given product A was sold first, how likely is product B is to be sold next  --- Tells the global cross sell tendency\n",
    "- Agent Propensity - How often does this agent sell product A and then any product B --- incorporates seller behavior and identifies agents with strong patterns\n",
    "- Branch Propensity - How often does a specific branch sell product A and then product B --- captures branch influence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1658a8e0-1aac-4459-92b7-7f00a27d3d7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature: Most common next product (mode)\n",
    "prod_total_cross_sells = cross_sell_history_df.groupBy(\"first_prod_code\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"p1_total_cross_sells\")\n",
    "propensity_prod_df = prod_total_cross_sells.withColumnRenamed(\"first_prod_code\", \"prod_code\") \\\n",
    "    .withColumnRenamed(\"p1_total_cross_sells\", \"p1_cross_sell_popularity\")\n",
    "display(propensity_prod_df)\n",
    "\n",
    "path_counts = cross_sell_history_df.groupBy(\"first_prod_code\", \"second_prod_code\").count()\n",
    "window_spec = Window.partitionBy(\"first_prod_code\").orderBy(F.col(\"count\").desc())\n",
    "most_common_path_df = path_counts.withColumn(\"rank\", F.row_number().over(window_spec)) \\\n",
    "    .filter(F.col(\"rank\") == 1) \\\n",
    "    .select(\n",
    "        F.col(\"first_prod_code\").alias(\"prod_code\"),\n",
    "        F.col(\"second_prod_code\").alias(\"p1_most_common_next_prod\")\n",
    "    )\n",
    "display(most_common_path_df)\n",
    "\n",
    "# Feature: Agent-level cross-sell count\n",
    "propensity_agent_df = cross_sell_history_df.groupBy(\"agt_no\", \"first_prod_code\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"agent_p1_cross_sell_count\") \\\n",
    "    .withColumnRenamed(\"first_prod_code\", \"prod_code\")\n",
    "display(propensity_agent_df)\n",
    "\n",
    "# Feature: Branch-level cross-sell count\n",
    "propensity_branch_df = cross_sell_history_df.groupBy(\"branchoffice_code\", \"first_prod_code\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"branch_p1_cross_sell_count\") \\\n",
    "    .withColumnRenamed(\"first_prod_code\", \"prod_code\")\n",
    "display(propensity_branch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "414b0615-3362-4b54-a135-5163f0db3903",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add propensity features to train and validation sets\n",
    "train_df_final = add_propensity_features(\n",
    "    train_data, propensity_prod_df, most_common_path_df, propensity_agent_df, propensity_branch_df\n",
    ")\n",
    "val_df_final = add_propensity_features(\n",
    "    val_data, propensity_prod_df, most_common_path_df, propensity_agent_df, propensity_branch_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "060b4496-0b38-4d66-8080-d224fd073691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Gather columns for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a94d3c74-75fc-4aeb-80d2-524fdce90553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These features are not used in the model, but are used for validationIndicatorCol param in LightGBM ONLY\n",
    "train_df_final = train_df_final.withColumn(\"is_validation\", F.lit(False))\n",
    "val_df_final = val_df_final.withColumn(\"is_validation\", F.lit(True))\n",
    "combined_train_val = train_df_final.unionByName(val_df_final)\n",
    "#\n",
    "\n",
    "# Define categorical columns (add p1_most_common_next_prod)\n",
    "cat_cols = cat_cols + ['p1_most_common_next_prod']\n",
    "\n",
    "# Define feature columns\n",
    "feature_cols = (\n",
    "    [f\"{c}_idx\" for c in cat_cols] +\n",
    "    [\n",
    "        'age_at_first_policy', 'years_to_second',\n",
    "        'stock_allocation_ratio', 'bond_allocation_ratio', 'annuity_allocation_ratio',\n",
    "        'mutual_fund_allocation_ratio', 'aum_to_asset_ratio', 'policy_value_to_assets_ratio'\n",
    "    ] +\n",
    "    [col for col in log_financial_cols if col in df.columns] +\n",
    "    [\n",
    "        \"p1_cross_sell_popularity\",\n",
    "        \"agent_p1_cross_sell_count\",\n",
    "        \"branch_p1_cross_sell_count\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f363ee4-4c56-41d2-8327-0a67fea066fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Index categorical columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    for c in cat_cols\n",
    "]\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"second_product_category\", outputCol=\"label\", handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"keep\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69d857e5-9c70-43ed-90c1-07fdd2fc51dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### GBT CLASSIFIER TRAINING\n",
    "\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=50, maxDepth=5)\n",
    "ovr = OneVsRest(classifier=gbt, labelCol=\"label\", featuresCol=\"features\")\n",
    "pipeline_ovr = Pipeline(stages=indexers + [label_indexer, assembler, ovr])\n",
    "\n",
    "model = pipeline_ovr.fit(train_df_final)\n",
    "\n",
    "# Get feature importances from the first binary GBT model in OneVsRest\n",
    "gbt_model = model.stages[-1].models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed235020-ec56-4388-8a84-d809cbe6061a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The parameter 'isUnbalance' handles the class imbalance in LightGBM when set to _True_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9716cbc2-db16-4136-9665-5e5ff7e638ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### LIGHTGBM TRAINING\n",
    "\n",
    "lgbm = LightGBMClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    isUnbalance=True,\n",
    "    validationIndicatorCol=\"is_validation\"\n",
    ")\n",
    "lgbm.setParams(\n",
    "    maxDepth=7,\n",
    "    objective=\"multiclass\",\n",
    "    numClass=6,\n",
    "    learningRate  =0.05,\n",
    "    numIterations=1000,\n",
    "    earlyStoppingRound=50,\n",
    "    numLeaves=40,\n",
    "    baggingFraction=0.8,\n",
    "    baggingFreq=1,\n",
    "    featureFraction=0.8\n",
    ")\n",
    "\n",
    "pipeline_ovr = Pipeline(stages=indexers + [label_indexer, assembler, lgbm])\n",
    "model = pipeline_ovr.fit(combined_train_val)\n",
    "\n",
    "# Get feature importances for LightGBM\n",
    "gbt_model = model.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67815f27-028e-427b-9cf3-ac333e53fbcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FEATURE IMPORTANCE AND DISPLAY PREDICTED SECOND PRODUCT\n",
    "\n",
    "importances = gbt_model.getFeatureImportances()\n",
    "import pandas as pd\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": importances.toArray()\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "display(spark.createDataFrame(feature_importance))\n",
    "\n",
    "predictions_val = model.transform(val_df_final)\n",
    "from pyspark.ml.feature import IndexToString\n",
    "label_converter = IndexToString(\n",
    "    inputCol=\"prediction\",\n",
    "    outputCol=\"predicted_second_product_category\",\n",
    "    labels=model.stages[-3].labels\n",
    ")\n",
    "final_predictions = label_converter.transform(predictions_val)\n",
    "display(final_predictions.select(\"axa_party_id\", \"policy_no\", \"product_category\", \"predicted_second_product_category\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af926810-eb7c-4e1c-af5c-77f178ae082b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Storing the Propensity features in the catalog explorer for pipeline preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c395771-3fdf-4c9f-ba54-6d60a5627408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "propensity_prod_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"eda_smartlist.us_wealth_management_smartlist.propensity_prod_df\")\n",
    "\n",
    "most_common_path_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"eda_smartlist.us_wealth_management_smartlist.most_common_path_df\")\n",
    "\n",
    "propensity_agent_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"eda_smartlist.us_wealth_management_smartlist.propensity_agent_df\")\n",
    "\n",
    "propensity_branch_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"eda_smartlist.us_wealth_management_smartlist.propensity_branch_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a9142df-0377-4b34-af18-7426a7d64089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Registering the model in the catalog explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3772054a-54ee-42cc-a0e9-099723cf729e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Get a sample input (as a pandas DataFrame)\n",
    "input_example = train_df_final.limit(5).toPandas()\n",
    "\n",
    "# Get model predictions for the sample input\n",
    "predictions = model.transform(train_df_final.limit(5))\n",
    "output_example = predictions.select(\"prediction\").toPandas()\n",
    "\n",
    "# Infer the signature\n",
    "signature = infer_signature(input_example, output_example)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=model,\n",
    "        artifact_path=\"gbt_propfeat_lightgbm\",\n",
    "        registered_model_name=\"eda_smartlist.models.gbt_propfeat_lightgbm\",\n",
    "        signature=signature,\n",
    "        input_example=input_example\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d231348-dee3-470e-b160-d2c61b06a06b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Evaluating the model performance using Accuracy, Precision, Recall and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "500727f9-9309-4bc0-9be3-98b2b69720c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:434)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:434)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:911)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:937)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:936)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:991)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:776)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Overall metrics\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(predictions_val)\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "f1 = evaluator_f1.evaluate(predictions_val)\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "precision = evaluator_precision.evaluate(predictions_val)\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "recall = evaluator_recall.evaluate(predictions_val)\n",
    "\n",
    "metrics_df = spark.createDataFrame(\n",
    "    [{\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}]\n",
    ")\n",
    "print(\"=\" * 60)\n",
    "print(\"OVERALL MODEL PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "display(metrics_df)\n",
    "\n",
    "# Convert predictions to pandas for detailed analysis\n",
    "pred_pandas = final_predictions.select(\"label\", \"prediction\", \"second_product_category\", \"predicted_second_product_category\").toPandas()\n",
    "\n",
    "# Get label mapping\n",
    "label_mapping = {i: label for i, label in enumerate(model.stages[-3].labels)}\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 60)\n",
    "cm = confusion_matrix(pred_pandas['label'], pred_pandas['prediction'])\n",
    "cm_df = pd.DataFrame(cm, \n",
    "                     index=[label_mapping.get(i, f'Class_{i}') for i in range(len(cm))],\n",
    "                     columns=[label_mapping.get(i, f'Class_{i}') for i in range(len(cm))])\n",
    "display(cm_df)\n",
    "\n",
    "# Per-class metrics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PER-CLASS PERFORMANCE METRICS\")\n",
    "print(\"=\" * 60)\n",
    "report = classification_report(pred_pandas['label'], pred_pandas['prediction'], \n",
    "                               target_names=[label_mapping.get(i, f'Class_{i}') for i in range(len(label_mapping))],\n",
    "                               output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "display(report_df)\n",
    "\n",
    "# Class distribution analysis\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASS DISTRIBUTION IN VALIDATION SET\")\n",
    "print(\"=\" * 60)\n",
    "class_dist = pred_pandas['label'].value_counts().sort_index()\n",
    "class_dist_df = pd.DataFrame({\n",
    "    'Class': [label_mapping.get(i, f'Class_{i}') for i in class_dist.index],\n",
    "    'Count': class_dist.values,\n",
    "    'Percentage': (class_dist.values / len(pred_pandas) * 100).round(2)\n",
    "})\n",
    "display(class_dist_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 1: COMPREHENSIVE DATA ANALYSIS FOR MODEL IMPROVEMENT**\n",
    "\n",
    "Let's analyze the data systematically to identify improvement opportunities:\n",
    "1. Class distribution and imbalance\n",
    "2. Feature quality and distributions\n",
    "3. Missing value patterns\n",
    "4. Feature importance analysis\n",
    "5. Error pattern analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYSIS 1: CLASS DISTRIBUTION IN TRAINING DATA\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS - TRAINING DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get class distribution in training set\n",
    "train_class_dist = train_df_final.groupBy(\"second_product_category\").count().orderBy(F.desc(\"count\"))\n",
    "train_class_dist_pd = train_class_dist.toPandas()\n",
    "train_class_dist_pd['percentage'] = (train_class_dist_pd['count'] / train_class_dist_pd['count'].sum() * 100).round(2)\n",
    "train_class_dist_pd.columns = ['Class', 'Count', 'Percentage']\n",
    "print(\"\\nTraining Set Class Distribution:\")\n",
    "display(train_class_dist_pd)\n",
    "\n",
    "# Get class distribution in validation set\n",
    "val_class_dist = val_df_final.groupBy(\"second_product_category\").count().orderBy(F.desc(\"count\"))\n",
    "val_class_dist_pd = val_class_dist.toPandas()\n",
    "val_class_dist_pd['percentage'] = (val_class_dist_pd['count'] / val_class_dist_pd['count'].sum() * 100).round(2)\n",
    "val_class_dist_pd.columns = ['Class', 'Count', 'Percentage']\n",
    "print(\"\\nValidation Set Class Distribution:\")\n",
    "display(val_class_dist_pd)\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "min_class = train_class_dist_pd['Count'].min()\n",
    "max_class = train_class_dist_pd['Count'].max()\n",
    "imbalance_ratio = max_class / min_class\n",
    "print(f\"\\nClass Imbalance Ratio (max/min): {imbalance_ratio:.2f}\")\n",
    "print(f\"Most common class: {train_class_dist_pd.iloc[0]['Class']} ({train_class_dist_pd.iloc[0]['Percentage']}%)\")\n",
    "print(f\"Least common class: {train_class_dist_pd.iloc[-1]['Class']} ({train_class_dist_pd.iloc[-1]['Percentage']}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYSIS 2: MISSING VALUES ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Convert to pandas for easier analysis\n",
    "train_pd = train_df_final.toPandas()\n",
    "\n",
    "# Check missing values in key features\n",
    "key_features = feature_cols + ['product_category', 'second_product_category', 'age_at_first_policy', \n",
    "                               'years_to_second', 'agt_no', 'branchoffice_code']\n",
    "available_features = [f for f in key_features if f in train_pd.columns]\n",
    "\n",
    "missing_analysis = []\n",
    "for col in available_features:\n",
    "    missing_count = train_pd[col].isna().sum()\n",
    "    missing_pct = (missing_count / len(train_pd) * 100) if len(train_pd) > 0 else 0\n",
    "    missing_analysis.append({\n",
    "        'Feature': col,\n",
    "        'Missing_Count': missing_count,\n",
    "        'Missing_Percentage': round(missing_pct, 2),\n",
    "        'Available_Count': len(train_pd) - missing_count\n",
    "    })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_analysis).sort_values('Missing_Percentage', ascending=False)\n",
    "print(\"\\nMissing Values by Feature:\")\n",
    "display(missing_df[missing_df['Missing_Percentage'] > 0])\n",
    "\n",
    "if len(missing_df[missing_df['Missing_Percentage'] > 0]) == 0:\n",
    "    print(\"\\nâœ“ No missing values found in key features!\")\n",
    "else:\n",
    "    print(f\"\\nâš  Found {len(missing_df[missing_df['Missing_Percentage'] > 0])} features with missing values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYSIS 3: FEATURE DISTRIBUTION AND OUTLIER ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"NUMERICAL FEATURE DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select numerical features\n",
    "numerical_cols = ['age_at_first_policy', 'years_to_second'] + \\\n",
    "                 [col for col in ['stock_allocation_ratio', 'bond_allocation_ratio', \n",
    "                                  'annuity_allocation_ratio', 'mutual_fund_allocation_ratio',\n",
    "                                  'aum_to_asset_ratio', 'policy_value_to_assets_ratio'] \n",
    "                  if col in train_pd.columns] + \\\n",
    "                 [col for col in log_financial_cols if col in train_pd.columns] + \\\n",
    "                 ['p1_cross_sell_popularity', 'agent_p1_cross_sell_count', 'branch_p1_cross_sell_count']\n",
    "\n",
    "available_numerical = [col for col in numerical_cols if col in train_pd.columns]\n",
    "\n",
    "# Statistical summary\n",
    "stats_summary = train_pd[available_numerical].describe()\n",
    "print(\"\\nStatistical Summary of Numerical Features:\")\n",
    "display(stats_summary)\n",
    "\n",
    "# Check for outliers using IQR method\n",
    "outlier_analysis = []\n",
    "for col in available_numerical:\n",
    "    Q1 = train_pd[col].quantile(0.25)\n",
    "    Q3 = train_pd[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = train_pd[(train_pd[col] < lower_bound) | (train_pd[col] > upper_bound)]\n",
    "    outlier_count = len(outliers)\n",
    "    outlier_pct = (outlier_count / len(train_pd) * 100) if len(train_pd) > 0 else 0\n",
    "    \n",
    "    outlier_analysis.append({\n",
    "        'Feature': col,\n",
    "        'Outlier_Count': outlier_count,\n",
    "        'Outlier_Percentage': round(outlier_pct, 2),\n",
    "        'Min': round(train_pd[col].min(), 2),\n",
    "        'Q1': round(Q1, 2),\n",
    "        'Median': round(train_pd[col].median(), 2),\n",
    "        'Q3': round(Q3, 2),\n",
    "        'Max': round(train_pd[col].max(), 2)\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_analysis).sort_values('Outlier_Percentage', ascending=False)\n",
    "print(\"\\nOutlier Analysis (IQR Method):\")\n",
    "display(outlier_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYSIS 4: FEATURE IMPORTANCE ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get feature importances from the model\n",
    "importances = gbt_model.getFeatureImportances()\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": importances.toArray()\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features:\")\n",
    "display(feature_importance.head(20))\n",
    "\n",
    "print(\"\\nBottom 10 Least Important Features:\")\n",
    "display(feature_importance.tail(10))\n",
    "\n",
    "# Calculate cumulative importance\n",
    "feature_importance['cumulative_importance'] = feature_importance['importance'].cumsum()\n",
    "feature_importance['cumulative_pct'] = (feature_importance['cumulative_importance'] / \n",
    "                                        feature_importance['importance'].sum() * 100)\n",
    "\n",
    "# Find how many features account for 80% of importance\n",
    "features_for_80pct = len(feature_importance[feature_importance['cumulative_pct'] <= 80])\n",
    "print(f\"\\nNumber of features accounting for 80% of importance: {features_for_80pct} out of {len(feature_importance)}\")\n",
    "print(f\"Percentage of features needed: {(features_for_80pct/len(feature_importance)*100):.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYSIS 5: ERROR PATTERN ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"ERROR PATTERN ANALYSIS - WHERE IS THE MODEL FAILING?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze misclassifications\n",
    "pred_pandas['is_correct'] = pred_pandas['label'] == pred_pandas['prediction']\n",
    "misclassified = pred_pandas[~pred_pandas['is_correct']].copy()\n",
    "\n",
    "print(f\"\\nTotal predictions: {len(pred_pandas)}\")\n",
    "print(f\"Correct predictions: {pred_pandas['is_correct'].sum()}\")\n",
    "print(f\"Misclassified: {len(misclassified)} ({len(misclassified)/len(pred_pandas)*100:.2f}%)\")\n",
    "\n",
    "# Most common misclassification patterns\n",
    "if len(misclassified) > 0:\n",
    "    misclassified['actual'] = misclassified['label'].map(label_mapping)\n",
    "    misclassified['predicted'] = misclassified['prediction'].map(label_mapping)\n",
    "    \n",
    "    error_patterns = misclassified.groupby(['actual', 'predicted']).size().reset_index(name='count')\n",
    "    error_patterns = error_patterns.sort_values('count', ascending=False)\n",
    "    error_patterns.columns = ['Actual_Class', 'Predicted_Class', 'Error_Count']\n",
    "    \n",
    "    print(\"\\nTop 10 Most Common Misclassification Patterns:\")\n",
    "    display(error_patterns.head(10))\n",
    "    \n",
    "    # Analyze which classes are most confused\n",
    "    print(\"\\nClasses Most Often Confused (Actual -> Predicted):\")\n",
    "    confusion_summary = error_patterns.groupby('Actual_Class')['Error_Count'].sum().sort_values(ascending=False)\n",
    "    confusion_summary_df = pd.DataFrame({\n",
    "        'Actual_Class': confusion_summary.index,\n",
    "        'Total_Errors': confusion_summary.values\n",
    "    })\n",
    "    display(confusion_summary_df)\n",
    "else:\n",
    "    print(\"\\nâœ“ No misclassifications found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYSIS 6: CROSS-SELL PATTERN ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"CROSS-SELL PATTERN ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze actual cross-sell patterns in training data\n",
    "cross_sell_patterns = train_pd.groupby(['product_category', 'second_product_category']).size().reset_index(name='count')\n",
    "cross_sell_patterns = cross_sell_patterns.sort_values('count', ascending=False)\n",
    "cross_sell_patterns.columns = ['First_Product', 'Second_Product', 'Count']\n",
    "cross_sell_patterns['Percentage'] = (cross_sell_patterns['Count'] / cross_sell_patterns['Count'].sum() * 100).round(2)\n",
    "\n",
    "print(\"\\nTop 20 Most Common Cross-Sell Patterns:\")\n",
    "display(cross_sell_patterns.head(20))\n",
    "\n",
    "# Create a pivot table for better visualization\n",
    "pivot_table = cross_sell_patterns.pivot_table(\n",
    "    index='First_Product', \n",
    "    columns='Second_Product', \n",
    "    values='Count', \n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(\"\\nCross-Sell Pattern Matrix (First Product -> Second Product):\")\n",
    "display(pivot_table)\n",
    "\n",
    "# Calculate transition probabilities\n",
    "transition_probs = train_pd.groupby('product_category')['second_product_category'].apply(\n",
    "    lambda x: x.value_counts(normalize=True)\n",
    ").reset_index()\n",
    "transition_probs.columns = ['First_Product', 'Second_Product', 'Probability']\n",
    "transition_probs = transition_probs.sort_values(['First_Product', 'Probability'], ascending=[True, False])\n",
    "\n",
    "print(\"\\nTop 3 Most Likely Next Products for Each First Product:\")\n",
    "for first_prod in transition_probs['First_Product'].unique():\n",
    "    top3 = transition_probs[transition_probs['First_Product'] == first_prod].head(3)\n",
    "    print(f\"\\n{first_prod}:\")\n",
    "    for _, row in top3.iterrows():\n",
    "        print(f\"  -> {row['Second_Product']}: {row['Probability']*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **STEP 2: IDENTIFYING IMPROVEMENT OPPORTUNITIES**\n",
    "\n",
    "Based on the analysis above, we'll identify specific areas for improvement:\n",
    "1. **Class Imbalance Handling**: If severe imbalance exists, we may need SMOTE, class weights, or different sampling\n",
    "2. **Feature Engineering**: Create new features based on patterns discovered\n",
    "3. **Hyperparameter Tuning**: Optimize model parameters\n",
    "4. **Ensemble Methods**: Combine multiple models\n",
    "5. **Feature Selection**: Remove low-importance features or add domain-specific features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **PHASE 3: IMPLEMENTING TARGETED IMPROVEMENTS**\n",
    "\n",
    "Based on the analysis, we'll implement improvements in this order:\n",
    "1. **Handle Class Imbalance** (HIGH PRIORITY - Severe imbalance detected)\n",
    "2. **Improve Feature Engineering** (Add transition probability features)\n",
    "3. **Hyperparameter Tuning** (Optimize for F1 score)\n",
    "4. **Feature Selection** (Remove low-importance features)\n",
    "\n",
    "Let's start with the most critical issue: Class Imbalance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMPROVEMENT 1: Handle Severe Class Imbalance**\n",
    "\n",
    "**Strategy**: Use class weights in LightGBM to give more importance to minority classes.\n",
    "\n",
    "**Approach**: Calculate balanced class weights based on class frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPROVEMENT 1: Calculate Class Weights for Imbalanced Classes\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"CALCULATING CLASS WEIGHTS FOR IMBALANCED DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get class distribution from training data\n",
    "train_class_counts = train_df_final.groupBy(\"second_product_category\").count().orderBy(F.desc(\"count\"))\n",
    "train_class_counts_pd = train_class_counts.toPandas()\n",
    "\n",
    "# Calculate total samples\n",
    "total_samples = train_class_counts_pd['count'].sum()\n",
    "n_classes = len(train_class_counts_pd)\n",
    "\n",
    "# Calculate class weights using balanced method: n_samples / (n_classes * class_count)\n",
    "class_weights = {}\n",
    "for _, row in train_class_counts_pd.iterrows():\n",
    "    class_name = row['second_product_category']\n",
    "    class_count = row['count']\n",
    "    # Balanced weight: inverse of class frequency\n",
    "    weight = total_samples / (n_classes * class_count)\n",
    "    class_weights[class_name] = weight\n",
    "\n",
    "print(\"\\nClass Weights (higher = more important):\")\n",
    "for class_name, weight in sorted(class_weights.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {class_name}: {weight:.4f}\")\n",
    "\n",
    "# Store for later use\n",
    "print(f\"\\nTotal classes: {n_classes}\")\n",
    "print(f\"Total samples: {total_samples}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMPROVEMENT 2: Add Product Transition Probability Features**\n",
    "\n",
    "Based on the cross-sell pattern analysis, we can create powerful features that capture the probability of transitioning from one product to another.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPROVEMENT 2: Create Product Transition Probability Features\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"CREATING PRODUCT TRANSITION PROBABILITY FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate transition probabilities from training data\n",
    "# This gives us P(second_product | first_product) for each combination\n",
    "transition_probs_df = train_data.groupBy(\"product_category\", \"second_product_category\").count()\n",
    "\n",
    "# Calculate total counts per first product\n",
    "first_prod_totals = train_data.groupBy(\"product_category\").count().withColumnRenamed(\"count\", \"total_first_prod\")\n",
    "\n",
    "# Join and calculate probabilities\n",
    "transition_probs_with_totals = transition_probs_df.join(\n",
    "    first_prod_totals, \n",
    "    on=\"product_category\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Calculate probability: count / total for that first product\n",
    "transition_probs_final = transition_probs_with_totals.withColumn(\n",
    "    \"transition_probability\",\n",
    "    F.col(\"count\") / F.col(\"total_first_prod\")\n",
    ").select(\n",
    "    F.col(\"product_category\").alias(\"first_prod\"),\n",
    "    F.col(\"second_product_category\").alias(\"second_prod\"),\n",
    "    \"transition_probability\"\n",
    ")\n",
    "\n",
    "print(\"\\nSample Transition Probabilities:\")\n",
    "display(transition_probs_final.orderBy(F.desc(\"transition_probability\")).limit(20))\n",
    "\n",
    "# Create lookup table for top 3 most likely next products for each first product\n",
    "window_spec = Window.partitionBy(\"first_prod\").orderBy(F.col(\"transition_probability\").desc())\n",
    "transition_probs_ranked = transition_probs_final.withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "\n",
    "# Get top 3 transitions for each first product\n",
    "top_transitions = transition_probs_ranked.filter(F.col(\"rank\") <= 3).select(\n",
    "    \"first_prod\",\n",
    "    \"second_prod\",\n",
    "    \"transition_probability\",\n",
    "    \"rank\"\n",
    ")\n",
    "\n",
    "print(\"\\nTop 3 Transition Probabilities per First Product:\")\n",
    "display(top_transitions.orderBy(\"first_prod\", \"rank\"))\n",
    "\n",
    "# Create features: probability of each second product given first product\n",
    "# We'll pivot this to create features like: prob_RETIREMENT_given_LIFE_INSURANCE\n",
    "transition_features = transition_probs_final.groupBy(\"first_prod\").pivot(\"second_prod\").agg(\n",
    "    F.first(\"transition_probability\")\n",
    ").fillna(0.0)\n",
    "\n",
    "# Rename columns to avoid conflicts\n",
    "for col_name in transition_features.columns:\n",
    "    if col_name != \"first_prod\":\n",
    "        transition_features = transition_features.withColumnRenamed(\n",
    "            col_name, \n",
    "            f\"prob_{col_name}_given_first\"\n",
    "        )\n",
    "\n",
    "print(\"\\nTransition Probability Features (sample):\")\n",
    "display(transition_features.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPROVEMENT 3: Add Transition Probability Features to Training Data\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"ADDING TRANSITION PROBABILITY FEATURES TO DATASETS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Function to add transition probability features\n",
    "def add_transition_prob_features(df, transition_features_df):\n",
    "    df_with_probs = df.join(\n",
    "        transition_features_df,\n",
    "        on=df[\"product_category\"] == transition_features_df[\"first_prod\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    # Drop the join key column (first_prod) if it exists\n",
    "    if \"first_prod\" in df_with_probs.columns:\n",
    "        df_with_probs = df_with_probs.drop(\"first_prod\")\n",
    "    # Fill missing probabilities with 0\n",
    "    prob_cols = [col for col in df_with_probs.columns if col.startswith(\"prob_\")]\n",
    "    if prob_cols:\n",
    "        df_with_probs = df_with_probs.fillna(0.0, subset=prob_cols)\n",
    "    return df_with_probs\n",
    "\n",
    "# Add to training and validation sets\n",
    "train_df_final_v2 = add_transition_prob_features(train_df_final, transition_features)\n",
    "val_df_final_v2 = add_transition_prob_features(val_df_final, transition_features)\n",
    "\n",
    "print(f\"\\nOriginal train_df_final columns: {len(train_df_final.columns)}\")\n",
    "print(f\"Enhanced train_df_final_v2 columns: {len(train_df_final_v2.columns)}\")\n",
    "print(f\"Added {len(train_df_final_v2.columns) - len(train_df_final.columns)} transition probability features\")\n",
    "\n",
    "# Update combined dataset\n",
    "train_df_final_v2 = train_df_final_v2.withColumn(\"is_validation\", F.lit(False))\n",
    "val_df_final_v2 = val_df_final_v2.withColumn(\"is_validation\", F.lit(True))\n",
    "combined_train_val_v2 = train_df_final_v2.unionByName(val_df_final_v2)\n",
    "\n",
    "print(\"\\nâœ“ Transition probability features added successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMPROVEMENT 4: Update Feature List and Retrain with Class Weights**\n",
    "\n",
    "Now we'll:\n",
    "1. Add transition probability features to feature list\n",
    "2. Use class weights in LightGBM\n",
    "3. Optimize hyperparameters for better F1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPROVEMENT 4: Update Feature Columns with Transition Probabilities\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"UPDATING FEATURE COLUMNS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get transition probability column names\n",
    "prob_cols = [col for col in train_df_final_v2.columns if col.startswith(\"prob_\")]\n",
    "print(f\"\\nTransition probability features to add: {len(prob_cols)}\")\n",
    "print(\"Sample features:\", prob_cols[:5] if len(prob_cols) > 5 else prob_cols)\n",
    "\n",
    "# Update feature columns to include transition probabilities\n",
    "feature_cols_v2 = (\n",
    "    [f\"{c}_idx\" for c in cat_cols] +\n",
    "    [\n",
    "        'age_at_first_policy', 'years_to_second',\n",
    "        'stock_allocation_ratio', 'bond_allocation_ratio', 'annuity_allocation_ratio',\n",
    "        'mutual_fund_allocation_ratio', 'aum_to_asset_ratio', 'policy_value_to_assets_ratio'\n",
    "    ] +\n",
    "    [col for col in log_financial_cols if col in train_df_final_v2.columns] +\n",
    "    [\n",
    "        \"p1_cross_sell_popularity\",\n",
    "        \"agent_p1_cross_sell_count\",\n",
    "        \"branch_p1_cross_sell_count\"\n",
    "    ] +\n",
    "    prob_cols  # Add transition probability features\n",
    ")\n",
    "\n",
    "print(f\"\\nOriginal feature count: {len(feature_cols)}\")\n",
    "print(f\"Updated feature count: {len(feature_cols_v2)}\")\n",
    "print(f\"Added {len(feature_cols_v2) - len(feature_cols)} new features\")\n",
    "\n",
    "# Update indexers and assembler for new features\n",
    "indexers_v2 = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
    "    for c in cat_cols\n",
    "]\n",
    "label_indexer_v2 = StringIndexer(\n",
    "    inputCol=\"second_product_category\", outputCol=\"label\", handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "assembler_v2 = VectorAssembler(\n",
    "    inputCols=feature_cols_v2, outputCol=\"features\", handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Feature columns updated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPROVEMENT 5: Retrain LightGBM with Class Weights and New Features\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"RETRAINING LIGHTGBM WITH IMPROVEMENTS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Improvements applied:\")\n",
    "print(\"  1. Class weights for imbalanced classes\")\n",
    "print(\"  2. Transition probability features\")\n",
    "print(\"  3. Optimized hyperparameters for F1 score\")\n",
    "\n",
    "# Note: LightGBM's isUnbalance=True handles class imbalance automatically\n",
    "# But we can also use class_weight parameter if available\n",
    "# For now, we'll use isUnbalance=True and optimize other parameters\n",
    "\n",
    "lgbm_v2 = LightGBMClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    isUnbalance=True,  # Handles class imbalance\n",
    "    validationIndicatorCol=\"is_validation\"\n",
    ")\n",
    "\n",
    "# Optimized hyperparameters for better F1 score\n",
    "lgbm_v2.setParams(\n",
    "    maxDepth=8,  # Slightly deeper for more complex patterns\n",
    "    objective=\"multiclass\",\n",
    "    numClass=7,  # Updated to 7 classes (including None)\n",
    "    learningRate=0.03,  # Lower learning rate for better convergence\n",
    "    numIterations=1500,  # More iterations\n",
    "    earlyStoppingRound=100,  # More patience\n",
    "    numLeaves=50,  # More leaves for complex patterns\n",
    "    baggingFraction=0.85,  # Slightly higher\n",
    "    baggingFreq=1,\n",
    "    featureFraction=0.75,  # Slightly lower to reduce overfitting\n",
    "    minDataInLeaf=20,  # Prevent overfitting on minority classes\n",
    "    lambdaL1=0.1,  # L1 regularization\n",
    "    lambdaL2=0.1,  # L2 regularization\n",
    "    minGainToSplit=0.1  # Minimum gain to split\n",
    ")\n",
    "\n",
    "pipeline_v2 = Pipeline(stages=indexers_v2 + [label_indexer_v2, assembler_v2, lgbm_v2])\n",
    "\n",
    "print(\"\\nTraining model with improvements...\")\n",
    "model_v2 = pipeline_v2.fit(combined_train_val_v2)\n",
    "\n",
    "print(\"âœ“ Model training completed!\")\n",
    "\n",
    "# Get feature importances\n",
    "lgbm_model_v2 = model_v2.stages[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPROVEMENT 6: Evaluate Improved Model\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATING IMPROVED MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Make predictions\n",
    "predictions_val_v2 = model_v2.transform(val_df_final_v2)\n",
    "\n",
    "# Convert predictions\n",
    "label_converter_v2 = IndexToString(\n",
    "    inputCol=\"prediction\",\n",
    "    outputCol=\"predicted_second_product_category\",\n",
    "    labels=model_v2.stages[-3].labels\n",
    ")\n",
    "final_predictions_v2 = label_converter_v2.transform(predictions_val_v2)\n",
    "\n",
    "# Calculate metrics\n",
    "evaluator_v2 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "accuracy_v2 = evaluator_v2.evaluate(predictions_val_v2)\n",
    "\n",
    "evaluator_f1_v2 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "f1_v2 = evaluator_f1_v2.evaluate(predictions_val_v2)\n",
    "\n",
    "evaluator_precision_v2 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "precision_v2 = evaluator_precision_v2.evaluate(predictions_val_v2)\n",
    "\n",
    "evaluator_recall_v2 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "recall_v2 = evaluator_recall_v2.evaluate(predictions_val_v2)\n",
    "\n",
    "# Compare with baseline\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "comparison_df = spark.createDataFrame([\n",
    "    {\"Metric\": \"Accuracy\", \"Baseline\": accuracy, \"Improved\": accuracy_v2, \"Improvement\": accuracy_v2 - accuracy},\n",
    "    {\"Metric\": \"F1 Score\", \"Baseline\": f1, \"Improved\": f1_v2, \"Improvement\": f1_v2 - f1},\n",
    "    {\"Metric\": \"Precision\", \"Baseline\": precision, \"Improved\": precision_v2, \"Improvement\": precision_v2 - precision},\n",
    "    {\"Metric\": \"Recall\", \"Baseline\": recall, \"Improved\": recall_v2, \"Improvement\": recall_v2 - recall}\n",
    "])\n",
    "display(comparison_df)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"F1 Score Improvement: {f1_v2 - f1:.4f} ({((f1_v2 - f1) / f1 * 100):.2f}%)\")\n",
    "print(f\"Current F1: {f1_v2:.4f} | Target: 0.85\")\n",
    "if f1_v2 >= 0.85:\n",
    "    print(\"ðŸŽ‰ TARGET ACHIEVED! F1 Score >= 0.85\")\n",
    "else:\n",
    "    print(f\"ðŸ“ˆ Still need improvement: {0.85 - f1_v2:.4f} to reach target\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPROVEMENT 7: Per-Class Performance Comparison\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"PER-CLASS PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get predictions for improved model\n",
    "pred_pandas_v2 = final_predictions_v2.select(\"label\", \"prediction\", \"second_product_category\", \"predicted_second_product_category\").toPandas()\n",
    "label_mapping_v2 = {i: label for i, label in enumerate(model_v2.stages[-3].labels)}\n",
    "\n",
    "# Per-class metrics for improved model\n",
    "report_v2 = classification_report(\n",
    "    pred_pandas_v2['label'], \n",
    "    pred_pandas_v2['prediction'], \n",
    "    target_names=[label_mapping_v2.get(i, f'Class_{i}') for i in range(len(label_mapping_v2))],\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "# Compare F1 scores per class\n",
    "print(\"\\nPer-Class F1 Score Comparison:\")\n",
    "class_comparison = []\n",
    "for class_name in label_mapping_v2.values():\n",
    "    if class_name in report_v2 and class_name in report:\n",
    "        baseline_f1 = report.get(class_name, {}).get('f1-score', 0)\n",
    "        improved_f1 = report_v2.get(class_name, {}).get('f1-score', 0)\n",
    "        improvement = improved_f1 - baseline_f1\n",
    "        class_comparison.append({\n",
    "            'Class': class_name,\n",
    "            'Baseline_F1': round(baseline_f1, 4),\n",
    "            'Improved_F1': round(improved_f1, 4),\n",
    "            'Improvement': round(improvement, 4),\n",
    "            'Support': report_v2.get(class_name, {}).get('support', 0)\n",
    "        })\n",
    "\n",
    "class_comparison_df = pd.DataFrame(class_comparison).sort_values('Support', ascending=False)\n",
    "display(class_comparison_df)\n",
    "\n",
    "# Highlight improvements\n",
    "improved_classes = class_comparison_df[class_comparison_df['Improvement'] > 0]\n",
    "if len(improved_classes) > 0:\n",
    "    print(f\"\\nâœ“ {len(improved_classes)} classes improved\")\n",
    "    print(f\"  Biggest improvement: {improved_classes.loc[improved_classes['Improvement'].idxmax(), 'Class']} (+{improved_classes['Improvement'].max():.4f})\")\n",
    "else:\n",
    "    print(\"\\nâš  No classes improved yet - may need further tuning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NEXT STEPS IF F1 < 0.85**\n",
    "\n",
    "If the F1 score is still below 0.85, try these additional improvements:\n",
    "\n",
    "1. **Stratified Sampling**: Use stratified train/val split to ensure balanced representation\n",
    "2. **SMOTE/Undersampling**: Apply synthetic oversampling for minority classes\n",
    "3. **Feature Engineering**: \n",
    "   - Interaction features (age Ã— product_category)\n",
    "   - Temporal features (month, day of week)\n",
    "   - Agent/branch performance metrics\n",
    "4. **Hyperparameter Tuning**: Use Optuna/Hyperopt for systematic tuning\n",
    "5. **Ensemble Methods**: Combine multiple models (LightGBM + XGBoost + CatBoost)\n",
    "6. **Remove/Combine Rare Classes**: Consider combining DISABILITY and HEALTH into \"OTHER\" if they're too rare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUMMARY OF FINDINGS AND RECOMMENDATIONS\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY OF FINDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "findings = []\n",
    "\n",
    "# Check class imbalance\n",
    "if 'imbalance_ratio' in locals():\n",
    "    if imbalance_ratio > 5:\n",
    "        findings.append({\n",
    "            'Issue': 'Severe Class Imbalance',\n",
    "            'Severity': 'HIGH',\n",
    "            'Impact': 'Model may be biased toward majority class',\n",
    "            'Recommendation': 'Use class weights, SMOTE, or stratified sampling'\n",
    "        })\n",
    "    elif imbalance_ratio > 2:\n",
    "        findings.append({\n",
    "            'Issue': 'Moderate Class Imbalance',\n",
    "            'Severity': 'MEDIUM',\n",
    "            'Impact': 'Minority classes may have lower recall',\n",
    "            'Recommendation': 'Consider class weights or balanced sampling'\n",
    "        })\n",
    "\n",
    "# Check missing values\n",
    "if 'missing_df' in locals() and len(missing_df[missing_df['Missing_Percentage'] > 5]) > 0:\n",
    "    high_missing = missing_df[missing_df['Missing_Percentage'] > 5]\n",
    "    findings.append({\n",
    "        'Issue': f'High Missing Values in {len(high_missing)} features',\n",
    "        'Severity': 'MEDIUM',\n",
    "        'Impact': 'May reduce model performance',\n",
    "        'Recommendation': 'Improve imputation strategy or feature engineering'\n",
    "    })\n",
    "\n",
    "# Check feature importance distribution\n",
    "if 'feature_importance' in locals():\n",
    "    low_importance_features = len(feature_importance[feature_importance['importance'] < 0.001])\n",
    "    if low_importance_features > 5:\n",
    "        findings.append({\n",
    "            'Issue': f'{low_importance_features} features with very low importance',\n",
    "            'Severity': 'LOW',\n",
    "            'Impact': 'Noise in model, potential overfitting',\n",
    "            'Recommendation': 'Consider feature selection to remove low-importance features'\n",
    "        })\n",
    "\n",
    "if findings:\n",
    "    findings_df = pd.DataFrame(findings)\n",
    "    display(findings_df)\n",
    "else:\n",
    "    print(\"\\nâœ“ No major issues identified in initial analysis!\")\n",
    "    print(\"Focus areas: Feature engineering and hyperparameter tuning\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"1. Review all analysis outputs above\")\n",
    "print(\"2. Identify specific improvement strategies based on findings\")\n",
    "print(\"3. Implement improvements incrementally\")\n",
    "print(\"4. Measure impact of each change\")\n",
    "print(\"5. Iterate until F1 > 0.85 is achieved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **IMPROVEMENT ROADMAP: F1 Score 0.71 â†’ >0.85**\n",
    "\n",
    "### **Phase 1: Data Understanding (COMPLETED) âœ…**\n",
    "We've added comprehensive analysis cells that will help us understand:\n",
    "- Current model performance (confusion matrix, per-class metrics)\n",
    "- Class distribution and imbalance\n",
    "- Feature quality and distributions\n",
    "- Missing value patterns\n",
    "- Feature importance\n",
    "- Error patterns\n",
    "- Cross-sell patterns\n",
    "\n",
    "### **Phase 2: Run Analysis & Review Results (NEXT)**\n",
    "1. Execute all analysis cells (cells 42-51)\n",
    "2. Review findings and identify specific issues\n",
    "3. Document insights from each analysis\n",
    "\n",
    "### **Phase 3: Targeted Improvements (Based on Findings)**\n",
    "Potential improvement strategies (to be prioritized based on analysis):\n",
    "\n",
    "#### **A. Handle Class Imbalance**\n",
    "- Use class weights in LightGBM\n",
    "- Implement SMOTE or other oversampling techniques\n",
    "- Use stratified sampling for train/val split\n",
    "\n",
    "#### **B. Feature Engineering**\n",
    "- Create interaction features (e.g., age Ã— product_category)\n",
    "- Add temporal features (month, day of week, time since first policy)\n",
    "- Create aggregated features (agent performance, branch performance)\n",
    "- Add product transition probability features\n",
    "- Create client lifetime value features\n",
    "\n",
    "#### **C. Model Improvements**\n",
    "- Hyperparameter tuning with Optuna/Hyperopt\n",
    "- Try different algorithms (XGBoost, CatBoost)\n",
    "- Ensemble multiple models\n",
    "- Use stacking or voting classifiers\n",
    "\n",
    "#### **D. Data Quality**\n",
    "- Better outlier handling\n",
    "- Improved imputation strategies\n",
    "- Feature selection (remove low-importance features)\n",
    "\n",
    "### **Phase 4: Iterative Improvement**\n",
    "- Implement one improvement at a time\n",
    "- Measure impact on F1 score\n",
    "- Keep improvements that help\n",
    "- Remove changes that don't help\n",
    "\n",
    "---\n",
    "\n",
    "**Let's start by running the analysis cells to understand what we're working with!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "533378aa-5fab-458f-ae30-326c75c3aee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Hyperparameter tuning for GBT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "774b6269-1564-4ea4-9e49-39dfc8d96fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Next Steps**\n",
    "### \n",
    "1. Train LightGBM\n",
    "2. Train XGBoost\n",
    "3. Hyperparameter Tuning using Hperopt"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3773137400068040,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ML_Process_flow",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "my-vscode-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
