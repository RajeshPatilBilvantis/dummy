

from pyspark.sql import functions as F

df_raw = df_raw.withColumn(
    "product_category",
    F.when(F.col("prod_lob") == "LIFE", "LIFE_INSURANCE")
    .when(F.col("sub_product_level_1").isin("VLI", "WL", "UL/IUL", "TERM", "PROTECTIVE PRODUCT"), "LIFE_INSURANCE")
    .when(F.col("sub_product_level_2").like("%LIFE%"), "LIFE_INSURANCE")
    .when(F.col("sub_product_level_2").isin(
        "VARIABLE UNIVERSAL LIFE", "WHOLE LIFE", "UNIVERSAL LIFE",
        "INDEX UNIVERSAL LIFE", "TERM PRODUCT", "VARIABLE LIFE",
        "SURVIVORSHIP WHOLE LIFE", "MONY PROTECTIVE PRODUCT"
    ), "LIFE_INSURANCE")
    .when(F.col("prod_lob").isin("GROUP RETIREMENT", "INDIVIDUAL RETIREMENT"), "RETIREMENT")
    .when(F.col("sub_product_level_1").isin(
        "EQUIVEST", "RETIREMENT 401K", "ACCUMULATOR",
        "RETIREMENT CORNERSTONE", "SCS", "INVESTMENT EDGE"
    ), "RETIREMENT")
    .when(
        (F.col("sub_product_level_2").like("%403B%")) |
        (F.col("sub_product_level_2").like("%401%")) |
        (F.col("sub_product_level_2").like("%IRA%")) |
        (F.col("sub_product_level_2").like("%SEP%")),
        "RETIREMENT"
    )
    .when(F.col("prod_lob") == "BROKER DEALER", "INVESTMENT")
    .when(F.col("sub_product_level_1").isin(
        "INVESTMENT PRODUCT - DIRECT", "INVESTMENT PRODUCT - BROKERAGE",
        "INVESTMENT PRODUCT - ADVISORY", "DIRECT", "BROKERAGE",
        "ADVISORY", "CASH SOLICITOR"
    ), "INVESTMENT")
    .when(
        (F.col("sub_product_level_2").like("%Investment%")) |
        (F.col("sub_product_level_2").like("%Brokerage%")) |
        (F.col("sub_product_level_2").like("%Advisory%")),
        "INVESTMENT"
    )
    .when(F.col("prod_lob") == "NETWORK", "NETWORK_PRODUCTS")
    .when(
        (F.col("sub_product_level_1") == "NETWORK PRODUCTS") |
        (F.col("sub_product_level_2") == "NETWORK PRODUCTS"),
        "NETWORK_PRODUCTS"
    )
    .when(
        (F.col("prod_lob") == "OTHERS") & (F.col("sub_product_level_1") == "HAS"),
        "DISABILITY"
    )
    .when(F.col("sub_product_level_2") == "HAS - DISABILITY", "DISABILITY"
    )
    .when(F.col("prod_lob") == "OTHERS", "HEALTH")
    .when(F.col("sub_product_level_2") == "GROUP HEALTH PRODUCTS", "HEALTH")
    .otherwise("OTHER")
)




# =====================
# 3. Exclude OTHER (not part of the 7-class problem)
# =====================
df_raw = df_raw.filter(F.col("product_category") != "OTHER")


# =====================
# 4. Basic cleaning - remove policies without dates or customer id
# =====================
df_raw = df_raw.filter(
    F.col("strt_date").isNotNull() & F.col("end_date").isNotNull() & F.col("axa_party_id").isNotNull()
)


# =====================
# 5. Cast dates correctly (if they are strings)
# =====================
df_raw = (
    df_raw
    .withColumn("strt_date", F.to_date("strt_date"))
    .withColumn("end_date", F.to_date("end_date"))
)


# =====================
# 6. Cache for performance
# =====================
df_raw = df_raw.cache()
df_raw.count()   # trigger caching
from pyspark.sql import functions as F
from pyspark.sql import Window

# STEP 2: keep only active policies (to avoid terminated ones)
df = df_raw.filter(F.col("policy_status") == "Active")

# STEP 3: find purchase order per client (sorted by register date)
w = Window.partitionBy("cont_id").orderBy("register_date")

df = df.withColumn("rn", F.row_number().over(w))

# STEP 4: first product and second product
df_pivot = df.withColumn("product_category_next",
                         F.lead("product_category").over(w))

# Keep only customers with at least 2 products
df_two = df_pivot.filter(F.col("product_category_next").isNotNull())

display(df_two.select("cont_id", "product_category", "product_category_next").limit(10))
print("Final training population:", df_two.count())
feature_cols = [
    "face_amt", "cash_val_amt", "acct_val_amt",
    "monthly_preminum_amount", "wc_total_assets",
    "wc_assetmix_stocks","wc_assetmix_bonds",
    "wc_assetmix_mutual_funds", "wc_assetmix_annuity",
    "wc_assetmix_deposits", "wc_assetmix_other_assets",
    "psn_age", "client_seg", "client_seg_1", "aum_band",
    "policy_type", "designation", "channel"
]

df_train = df_two.select(
    "cont_id",
   "product_category",
    "product_category_next",   # ★ LABEL WE WILL PREDICT
    *feature_cols
)

display(df_train)
print("Training DF Count:", df_train.count())


cont_id	product_category	product_category_next
100036768312101744	LIFE_INSURANCE	LIFE_INSURANCE
100036768312101744	LIFE_INSURANCE	LIFE_INSURANCE
100036768312101744	LIFE_INSURANCE	LIFE_INSURANCE
100036768312101744	LIFE_INSURANCE	LIFE_INSURANCE
100036768312101744	LIFE_INSURANCE	LIFE_INSURANCE
100036768312101744	LIFE_INSURANCE	LIFE_INSURANCE
100036768312101744	LIFE_INSURANCE	LIFE_INSURANCE
100036768312101744	LIFE_INSURANCE	LIFE_INSURANCE
100036768312101744	LIFE_INSURANCE	LIFE_INSURANCE
100036768312101744	LIFE_INSURANCE	LIFE_INSURANCE


cont_id	product_category	product_category_next	face_amt	cash_val_amt	acct_val_amt	monthly_preminum_amount	wc_total_assets	wc_assetmix_stocks	wc_assetmix_bonds	wc_assetmix_mutual_funds	wc_assetmix_annuity	wc_assetmix_deposits	wc_assetmix_other_assets	psn_age	client_seg	client_seg_1	aum_band	policy_type	designation	channel
100036768312101744	LIFE_INSURANCE	LIFE_INSURANCE	500000	11535.0595703125	11535.0595703125	100	8065506	3318836	1011487	3012448	150191	452278	120266	69	5m+	High Asset Customers	<$25K	Old Policy	Individual	Retail
100036768312101744	LIFE_INSURANCE	LIFE_INSURANCE	500000	10226.8095703125	10226.8095703125	100	8065506	3318836	1011487	3012448	150191	452278	120266	69	5m+	High Asset Customers	<$25K	Old Policy	Individual	Retail
100036768312101744	LIFE_INSURANCE	LIFE_INSURANCE	500000	10533.240234375	10533.240234375	100	8065506	3318836	1011487	3012448	150191	452278	120266	69	5m+	High Asset Customers	<$25K	Old Policy	Individual	Retail
100036768312101744	LIFE_INSURANCE	LIFE_INSURANCE	500000	8647.599609375	8647.599609375	100	8065506	3318836	1011487	3012448	150191	452278	120266	69	5m+	High Asset Customers	<$25K	Old Policy	Individual	Retail



Training DF Count: 223395252






# =====================
# 1. Load SQL into Spark DF
# =====================
query = """
SELECT *
FROM client_metrics   -- you already ran this and validated columns
"""
df_raw = spark.sql(query)


# =====================
# 2. Add product_category column
# =====================
from pyspark.sql import functions as F

df_raw = df_raw.withColumn(
    "product_category",
    F.when(F.col("prod_lob") == "LIFE", "LIFE_INSURANCE")
    .when(F.col("sub_product_level_1").isin("VLI", "WL", "UL/IUL", "TERM", "PROTECTIVE PRODUCT"), "LIFE_INSURANCE")
    .when(F.col("sub_product_level_2").like("%LIFE%"), "LIFE_INSURANCE")
    .when(F.col("sub_product_level_2").isin(
        "VARIABLE UNIVERSAL LIFE", "WHOLE LIFE", "UNIVERSAL LIFE",
        "INDEX UNIVERSAL LIFE", "TERM PRODUCT", "VARIABLE LIFE",
        "SURVIVORSHIP WHOLE LIFE", "MONY PROTECTIVE PRODUCT"
    ), "LIFE_INSURANCE")
    .when(F.col("prod_lob").isin("GROUP RETIREMENT", "INDIVIDUAL RETIREMENT"), "RETIREMENT")
    .when(F.col("sub_product_level_1").isin(
        "EQUIVEST", "RETIREMENT 401K", "ACCUMULATOR",
        "RETIREMENT CORNERSTONE", "SCS", "INVESTMENT EDGE"
    ), "RETIREMENT")
    .when(
        (F.col("sub_product_level_2").like("%403B%")) |
        (F.col("sub_product_level_2").like("%401%")) |
        (F.col("sub_product_level_2").like("%IRA%")) |
        (F.col("sub_product_level_2").like("%SEP%")),
        "RETIREMENT"
    )
    .when(
        (F.col("Product").like("%IRA%")) |
        (F.col("Product").like("%401%")) |
        (F.col("Product").like("%403%")) |
        (F.col("Product").like("%SEP%")) |
        (F.col("Product").like("%Accumulator%")) |
        (F.col("Product").like("%Retirement%")),
        "RETIREMENT"
    )
    .when(F.col("prod_lob") == "BROKER DEALER", "INVESTMENT")
    .when(F.col("sub_product_level_1").isin(
        "INVESTMENT PRODUCT - DIRECT", "INVESTMENT PRODUCT - BROKERAGE",
        "INVESTMENT PRODUCT - ADVISORY", "DIRECT", "BROKERAGE",
        "ADVISORY", "CASH SOLICITOR"
    ), "INVESTMENT")
    .when(
        (F.col("sub_product_level_2").like("%Investment%")) |
        (F.col("sub_product_level_2").like("%Brokerage%")) |
        (F.col("sub_product_level_2").like("%Advisory%")),
        "INVESTMENT"
    )
    .when(F.col("prod_lob") == "NETWORK", "NETWORK_PRODUCTS")
    .when(
        (F.col("sub_product_level_1") == "NETWORK PRODUCTS") |
        (F.col("sub_product_level_2") == "NETWORK PRODUCTS"),
        "NETWORK_PRODUCTS"
    )
    .when(F.col("Product").like("%Network%"), "NETWORK_PRODUCTS")
    .when(
        (F.col("prod_lob") == "OTHERS") & (F.col("sub_product_level_1") == "HAS"),
        "DISABILITY"
    )
    .when(F.col("sub_product_level_2") == "HAS - DISABILITY", "DISABILITY")
    .when(
        (F.col("Product").like("%Disability%")) |
        (F.col("Product").like("%DI -%")),
        "DISABILITY"
    )
    .when(F.col("prod_lob") == "OTHERS", "HEALTH")
    .when(F.col("sub_product_level_2") == "GROUP HEALTH PRODUCTS", "HEALTH")
    .when(
        (F.col("Product").like("%Health%")) |
        (F.col("Product").like("%Medical%")) |
        (F.col("Product").like("%Hospital%")),
        "HEALTH"
    )
    .otherwise("OTHER")
)


# =====================
# 3. Exclude OTHER (not part of the 7-class problem)
# =====================
df_raw = df_raw.filter(F.col("product_category") != "OTHER")


# =====================
# 4. Basic cleaning - remove policies without dates or customer id
# =====================
df_raw = df_raw.filter(
    F.col("strt_date").isNotNull() & F.col("end_date").isNotNull() & F.col("axa_party_id").isNotNull()
)


# =====================
# 5. Cast dates correctly (if they are strings)
# =====================
df_raw = (
    df_raw
    .withColumn("strt_date", F.to_date("strt_date"))
    .withColumn("end_date", F.to_date("end_date"))
)


# =====================
# 6. Cache for performance
# =====================
df_raw = df_raw.cache()
df_raw.count()   # trigger caching
------------
from pyspark.sql import functions as F
from pyspark.sql import Window

# STEP 2: keep only active policies (to avoid terminated ones)
df = df_raw.filter(F.col("policy_status") == "Active")

# STEP 3: find purchase order per client (sorted by register date)
w = Window.partitionBy("cont_id").orderBy("register_date")

df = df.withColumn("rn", F.row_number().over(w))

# STEP 4: first product and second product
df_pivot = df.withColumn("prod_lob_next",
                         F.lead("prod_lob").over(w))

# Keep only customers with at least 2 products
df_two = df_pivot.filter(F.col("prod_lob_next").isNotNull())

display(df_two.select("cont_id", "prod_lob", "prod_lob_next").limit(10))
print("Final training population:", df_two.count())


feature_cols = [
    "face_amt", "cash_val_amt", "acct_val_amt",
    "monthly_preminum_amount", "wc_total_assets",
    "wc_assetmix_stocks","wc_assetmix_bonds",
    "wc_assetmix_mutual_funds", "wc_assetmix_annuity",
    "wc_assetmix_deposits", "wc_assetmix_other_assets",
    "psn_age", "client_seg", "client_seg_1", "aum_band",
    "policy_type", "designation", "channel"
]

df_train = df_two.select(
    "cont_id",
    "prod_lob",
    "prod_lob_next",   # ★ LABEL WE WILL PREDICT
    *feature_cols
)

display(df_train)
print("Training DF Count:", df_train.count())



------------------------


cont_id	prod_lob	prod_lob_next
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE



Final training population: 297670503
