from pyspark.sql import functions as F
from pyspark.sql import Window

# STEP 2: keep only active policies (to avoid terminated ones)
df = df_raw.filter(F.col("policy_status") == "Active")

# STEP 3: find purchase order per client (sorted by register date)
w = Window.partitionBy("cont_id").orderBy("register_date")

df = df.withColumn("rn", F.row_number().over(w))

# STEP 4: first product and second product
df_pivot = df.withColumn("prod_lob_next",
                         F.lead("prod_lob").over(w))

# Keep only customers with at least 2 products
df_two = df_pivot.filter(F.col("prod_lob_next").isNotNull())

display(df_two.select("cont_id", "prod_lob", "prod_lob_next").limit(10))
print("Final training population:", df_two.count())


feature_cols = [
    "face_amt", "cash_val_amt", "acct_val_amt",
    "monthly_preminum_amount", "wc_total_assets",
    "wc_assetmix_stocks","wc_assetmix_bonds",
    "wc_assetmix_mutual_funds", "wc_assetmix_annuity",
    "wc_assetmix_deposits", "wc_assetmix_other_assets",
    "psn_age", "client_seg", "client_seg_1", "aum_band",
    "policy_type", "designation", "channel"
]

df_train = df_two.select(
    "cont_id",
    "prod_lob",
    "prod_lob_next",   # â˜… LABEL WE WILL PREDICT
    *feature_cols
)

display(df_train)
print("Training DF Count:", df_train.count())



------------------------


cont_id	prod_lob	prod_lob_next
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE
100036768312101744	LIFE	LIFE



Final training population: 297670503
