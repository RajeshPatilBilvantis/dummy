{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cbc2609d-8f3b-4e3c-b783-6c47aeb29b0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with product_lookup as (\n",
    "  -- Pre-process lookup table once for better performance\n",
    "  select distinct \n",
    "    upper(source_sys_id) as source_sys_id,\n",
    "    trim(upper(REPLACE(LTRIM(REPLACE(idb_plan_cd,'0',' ')),' ','0'))) as idb_plan_cd,\n",
    "    trim(upper(REPLACE(LTRIM(REPLACE(idb_sub_plan_cd,'0',' ')),' ','0'))) as idb_sub_plan_cd,\n",
    "    trim(stmt_plan_typ_txt) as Product, \n",
    "    sub_product_level_1, \n",
    "    sub_product_level_2\n",
    "  from dl_tenants_daas.us_wealth_management.wealth_management_sub_product_group\n",
    "),\n",
    "base as (\n",
    "  select \n",
    "    r.axa_party_id,\n",
    "    r.policy_no,\n",
    "    r.register_date,\n",
    "    r.trmn_eff_date,\n",
    "    r.wti_lob_txt,\n",
    "    r.prod_lob,\n",
    "    r.agt_class,\n",
    "    r.isrd_brth_date,\n",
    "    r.psn_age,\n",
    "    r.acct_val_amt,\n",
    "    r.face_amt,\n",
    "    r.cash_val_amt,\n",
    "    r.wc_total_assets,\n",
    "    r.wc_assetmix_stocks,\n",
    "    r.wc_assetmix_bonds,\n",
    "    r.wc_assetmix_mutual_funds,\n",
    "    r.wc_assetmix_annuity,\n",
    "    r.wc_assetmix_deposits,\n",
    "    r.wc_assetmix_other_assets,\n",
    "    r.division_name,\n",
    "    r.mkt_prod_hier,\n",
    "    r.policy_status,\n",
    "    r.agent_segment,\n",
    "    r.channel,\n",
    "    r.client_seg,\n",
    "    r.client_seg_1,\n",
    "    r.aum_band,\n",
    "    r.business_month,\n",
    "    r.branchoffice_code,\n",
    "    r.agt_no,\n",
    "    -- Pre-clean plan codes for better join performance\n",
    "    trim(upper(REPLACE(LTRIM(REPLACE(r.plan_code,'0',' ')),' ','0'))) as cleaned_plan_code,\n",
    "    trim(upper(REPLACE(LTRIM(REPLACE(r.plan_subcd_code,'0',' ')),' ','0'))) as cleaned_plan_subcd_code,\n",
    "    h.sub_product_level_1,\n",
    "    h.sub_product_level_2,\n",
    "    h.Product,\n",
    "    row_number() over (partition by r.axa_party_id order by r.register_date asc) as rn,\n",
    "    count(*) over (partition by r.axa_party_id) as total_policies_per_client\n",
    "  from dl_tenants_daas.us_wealth_management.wealth_management_client_metrics r\n",
    "  left join product_lookup h \n",
    "    on upper(r.source_sys_id) = h.source_sys_id\n",
    "    and trim(upper(REPLACE(LTRIM(REPLACE(r.plan_code,'0',' ')),' ','0'))) = h.idb_plan_cd\n",
    "    and trim(upper(REPLACE(LTRIM(REPLACE(r.plan_subcd_code,'0',' ')),' ','0'))) = h.idb_sub_plan_cd\n",
    "  where r.business_month = (select max(business_month) from dl_tenants_daas.us_wealth_management.wealth_management_client_metrics)\n",
    "    and r.axa_party_id is not null\n",
    "    and r.policy_no is not null\n",
    "),\n",
    "first_second as (\n",
    "  select\n",
    "    axa_party_id,\n",
    "    max(total_policies_per_client) as total_policies_per_client,  -- NEW: Total policy count\n",
    "    -- First policy fields\n",
    "    max(case when rn = 1 then policy_no end) as policy_no,\n",
    "    max(case when rn = 1 then register_date end) as register_date,\n",
    "    max(case when rn = 1 then trmn_eff_date end) as trmn_eff_date,\n",
    "    max(case when rn = 1 then wti_lob_txt end) as wti_lob_txt,\n",
    "    max(case when rn = 1 then prod_lob end) as prod_lob,\n",
    "    max(case when rn = 1 then agt_class end) as agt_class,\n",
    "    max(case when rn = 1 then isrd_brth_date end) as isrd_brth_date,\n",
    "    max(case when rn = 1 then psn_age end) as psn_age,\n",
    "    max(case when rn = 1 then acct_val_amt end) as acct_val_amt,\n",
    "    max(case when rn = 1 then face_amt end) as face_amt,\n",
    "    max(case when rn = 1 then cash_val_amt end) as cash_val_amt,\n",
    "    max(case when rn = 1 then wc_total_assets end) as wc_total_assets,\n",
    "    max(case when rn = 1 then wc_assetmix_stocks end) as wc_assetmix_stocks,\n",
    "    max(case when rn = 1 then wc_assetmix_bonds end) as wc_assetmix_bonds,\n",
    "    max(case when rn = 1 then wc_assetmix_mutual_funds end) as wc_assetmix_mutual_funds,\n",
    "    max(case when rn = 1 then wc_assetmix_annuity end) as wc_assetmix_annuity,\n",
    "    max(case when rn = 1 then wc_assetmix_deposits end) as wc_assetmix_deposits,\n",
    "    max(case when rn = 1 then wc_assetmix_other_assets end) as wc_assetmix_other_assets,\n",
    "    max(case when rn = 1 then client_seg end) as client_seg,\n",
    "    max(case when rn = 1 then client_seg_1 end) as client_seg_1,\n",
    "    max(case when rn = 1 then aum_band end) as aum_band,\n",
    "    max(case when rn = 1 then sub_product_level_1 end) as sub_product_level_1,\n",
    "    max(case when rn = 1 then sub_product_level_2 end) as sub_product_level_2,\n",
    "    max(case when rn = 1 then Product end) as Product,\n",
    "    max(case when rn = 1 then business_month end) as business_month,\n",
    "    max(case when rn = 1 then branchoffice_code end) as branchoffice_code,\n",
    "    max(case when rn = 1 then agt_no end) as agt_no,\n",
    "    max(case when rn = 1 then division_name end) as division_name,\n",
    "    max(case when rn = 1 then mkt_prod_hier end) as mkt_prod_hier,\n",
    "    max(case when rn = 1 then policy_status end) as policy_status ,\n",
    "    max(case when rn = 1 then channel end) as channel,\n",
    "    max(case when rn = 1 then agent_segment end) as agent_segment,\n",
    "    -- Second policy fields\n",
    "    max(case when rn = 2 then policy_no end) as second_policy_no,\n",
    "    max(case when rn = 2 then register_date end) as second_register_date,\n",
    "    max(case when rn = 2 then trmn_eff_date end) as second_trmn_eff_date,\n",
    "    max(case when rn = 2 then wti_lob_txt end) as second_wti_lob_txt,\n",
    "    max(case when rn = 2 then prod_lob end) as second_prod_lob,\n",
    "    max(case when rn = 2 then sub_product_level_1 end) as second_sub_product_level_1,\n",
    "    max(case when rn = 2 then sub_product_level_2 end) as second_sub_product_level_2,\n",
    "    max(case when rn = 2 then Product end) as second_Product\n",
    "  from base\n",
    "  where rn <= 2\n",
    "  group by axa_party_id\n",
    ")\n",
    "select *,\n",
    "  -- NEW: Time-based features\n",
    "  datediff(day, \n",
    "    max(case when rn = 1 then register_date end),\n",
    "    max(case when rn = 2 then register_date end)\n",
    "  ) as days_between_policies,\n",
    "  CASE \n",
    "    WHEN datediff(day, \n",
    "          max(case when rn = 1 then register_date end),\n",
    "          max(case when rn = 2 then register_date end)\n",
    "        ) IS NULL THEN NULL\n",
    "    WHEN datediff(day, \n",
    "          max(case when rn = 1 then register_date end),\n",
    "          max(case when rn = 2 then register_date end)\n",
    "        ) <= 90 THEN 'IMMEDIATE'\n",
    "    WHEN datediff(day, \n",
    "          max(case when rn = 1 then register_date end),\n",
    "          max(case when rn = 2 then register_date end)\n",
    "        ) <= 365 THEN 'WITHIN_YEAR'\n",
    "    WHEN datediff(day, \n",
    "          max(case when rn = 1 then register_date end),\n",
    "          max(case when rn = 2 then register_date end)\n",
    "        ) <= 1095 THEN 'WITHIN_3_YEARS'\n",
    "    ELSE 'LONG_TERM'\n",
    "  END AS cross_sell_timing_category,\n",
    "  -- Existing ratio calculations\n",
    "  wc_assetmix_stocks / NULLIF(wc_total_assets, 0) AS stock_allocation_ratio,\n",
    "  wc_assetmix_bonds / NULLIF(wc_total_assets, 0) AS bond_allocation_ratio,\n",
    "  wc_assetmix_annuity / NULLIF(wc_total_assets, 0) AS annuity_allocation_ratio,\n",
    "  wc_assetmix_mutual_funds / NULLIF(wc_total_assets, 0) AS mutual_fund_allocation_ratio,\n",
    "  acct_val_amt / NULLIF(wc_total_assets, 0) AS aum_to_asset_ratio,\n",
    "  face_amt / NULLIF(wc_total_assets, 0) AS policy_value_to_assets_ratio,\n",
    "  \n",
    "  CASE \n",
    "    WHEN prod_lob = 'LIFE' THEN 'LIFE_INSURANCE'\n",
    "    WHEN sub_product_level_1 IN ('VLI', 'WL', 'UL/IUL', 'TERM', 'PROTECTIVE PRODUCT') THEN 'LIFE_INSURANCE'\n",
    "    WHEN sub_product_level_2 LIKE '%LIFE%' THEN 'LIFE_INSURANCE'\n",
    "    WHEN sub_product_level_2 IN ('VARIABLE UNIVERSAL LIFE', 'WHOLE LIFE', 'UNIVERSAL LIFE', \n",
    "                                'INDEX UNIVERSAL LIFE', 'TERM PRODUCT', 'VARIABLE LIFE', \n",
    "                                'SURVIVORSHIP WHOLE LIFE', 'MONY PROTECTIVE PRODUCT') THEN 'LIFE_INSURANCE'\n",
    "    WHEN prod_lob IN ('GROUP RETIREMENT', 'INDIVIDUAL RETIREMENT') THEN 'RETIREMENT'\n",
    "    WHEN sub_product_level_1 IN ('EQUIVEST', 'RETIREMENT 401K', 'ACCUMULATOR', \n",
    "                                'RETIREMENT CORNERSTONE', 'SCS', 'INVESTMENT EDGE') THEN 'RETIREMENT'\n",
    "    WHEN sub_product_level_2 LIKE '%403B%' OR sub_product_level_2 LIKE '%401%' \n",
    "         OR sub_product_level_2 LIKE '%IRA%' OR sub_product_level_2 LIKE '%SEP%' THEN 'RETIREMENT'\n",
    "    WHEN Product LIKE '%IRA%' OR Product LIKE '%401%' OR Product LIKE '%403%' \n",
    "         OR Product LIKE '%SEP%' OR Product LIKE '%Accumulator%' \n",
    "         OR Product LIKE '%Retirement%' THEN 'RETIREMENT'\n",
    "    WHEN prod_lob = 'BROKER DEALER' THEN 'INVESTMENT'\n",
    "    WHEN sub_product_level_1 IN ('INVESTMENT PRODUCT - DIRECT', 'INVESTMENT PRODUCT - BROKERAGE', \n",
    "                                'INVESTMENT PRODUCT - ADVISORY', 'DIRECT', 'BROKERAGE', \n",
    "                                'ADVISORY', 'CASH SOLICITOR') THEN 'INVESTMENT'\n",
    "    WHEN sub_product_level_2 LIKE '%Investment%' OR sub_product_level_2 LIKE '%Brokerage%' \n",
    "         OR sub_product_level_2 LIKE '%Advisory%' THEN 'INVESTMENT'\n",
    "    WHEN prod_lob = 'NETWORK' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN sub_product_level_1 = 'NETWORK PRODUCTS' OR sub_product_level_2 = 'NETWORK PRODUCTS' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN Product LIKE '%Network%' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN prod_lob = 'OTHERS' AND sub_product_level_1 = 'HAS' THEN 'DISABILITY'\n",
    "    WHEN sub_product_level_2 = 'HAS - DISABILITY' THEN 'DISABILITY'\n",
    "    WHEN Product LIKE '%Disability%' OR Product LIKE '%DI -%' THEN 'DISABILITY'\n",
    "    WHEN prod_lob = 'OTHERS' THEN 'HEALTH'\n",
    "    WHEN sub_product_level_2 = 'GROUP HEALTH PRODUCTS' THEN 'HEALTH'\n",
    "    WHEN Product LIKE '%Health%' OR Product LIKE '%Medical%' OR Product LIKE '%Hospital%' THEN 'HEALTH'\n",
    "    ELSE 'OTHER'\n",
    "  END AS product_category,\n",
    "  CASE \n",
    "    WHEN second_prod_lob IS NULL OR second_prod_lob = '' THEN NULL\n",
    "    WHEN second_prod_lob = 'LIFE' THEN 'LIFE_INSURANCE'\n",
    "    WHEN second_sub_product_level_1 IN ('VLI', 'WL', 'UL/IUL', 'TERM', 'PROTECTIVE PRODUCT') THEN 'LIFE_INSURANCE'\n",
    "    WHEN second_sub_product_level_2 LIKE '%LIFE%' THEN 'LIFE_INSURANCE'\n",
    "    WHEN second_sub_product_level_2 IN ('VARIABLE UNIVERSAL LIFE', 'WHOLE LIFE', 'UNIVERSAL LIFE', \n",
    "                                'INDEX UNIVERSAL LIFE', 'TERM PRODUCT', 'VARIABLE LIFE', \n",
    "                                'SURVIVORSHIP WHOLE LIFE', 'MONY PROTECTIVE PRODUCT') THEN 'LIFE_INSURANCE'\n",
    "    WHEN second_prod_lob IN ('GROUP RETIREMENT', 'INDIVIDUAL RETIREMENT') THEN 'RETIREMENT'\n",
    "    WHEN second_sub_product_level_1 IN ('EQUIVEST', 'RETIREMENT 401K', 'ACCUMULATOR', \n",
    "                                'RETIREMENT CORNERSTONE', 'SCS', 'INVESTMENT EDGE') THEN 'RETIREMENT'\n",
    "    WHEN second_sub_product_level_2 LIKE '%403B%' OR second_sub_product_level_2 LIKE '%401%' \n",
    "         OR second_sub_product_level_2 LIKE '%IRA%' OR second_sub_product_level_2 LIKE '%SEP%' THEN 'RETIREMENT'\n",
    "    WHEN second_Product LIKE '%IRA%' OR second_Product LIKE '%401%' OR second_Product LIKE '%403%' \n",
    "         OR second_Product LIKE '%SEP%' OR second_Product LIKE '%Accumulator%' \n",
    "         OR second_Product LIKE '%Retirement%' THEN 'RETIREMENT'\n",
    "    WHEN second_prod_lob = 'BROKER DEALER' THEN 'INVESTMENT'\n",
    "    WHEN second_sub_product_level_1 IN ('INVESTMENT PRODUCT - DIRECT', 'INVESTMENT PRODUCT - BROKERAGE', \n",
    "                                'INVESTMENT PRODUCT - ADVISORY', 'DIRECT', 'BROKERAGE', \n",
    "                                'ADVISORY', 'CASH SOLICITOR') THEN 'INVESTMENT'\n",
    "    WHEN second_sub_product_level_2 LIKE '%Investment%' OR second_sub_product_level_2 LIKE '%Brokerage%' \n",
    "         OR second_sub_product_level_2 LIKE '%Advisory%' THEN 'INVESTMENT'\n",
    "    WHEN second_prod_lob = 'NETWORK' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN second_sub_product_level_1 = 'NETWORK PRODUCTS' OR second_sub_product_level_2 = 'NETWORK PRODUCTS' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN second_Product LIKE '%Network%' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN second_prod_lob = 'OTHERS' AND second_sub_product_level_1 = 'HAS' THEN 'DISABILITY'\n",
    "    WHEN second_sub_product_level_2 = 'HAS - DISABILITY' THEN 'DISABILITY'\n",
    "    WHEN second_Product LIKE '%Disability%' OR second_Product LIKE '%DI -%' THEN 'DISABILITY'\n",
    "    WHEN second_prod_lob = 'OTHERS' THEN 'HEALTH'\n",
    "    WHEN second_sub_product_level_2 = 'GROUP HEALTH PRODUCTS' THEN 'HEALTH'\n",
    "    WHEN second_Product LIKE '%Health%' OR second_Product LIKE '%Medical%' OR second_Product LIKE '%Hospital%' THEN 'HEALTH'\n",
    "    ELSE 'OTHER'\n",
    "  END AS second_product_category,\n",
    "  CASE\n",
    "    WHEN MONTH(register_date) BETWEEN 1 AND 3 THEN 'Q1'\n",
    "    WHEN MONTH(register_date) BETWEEN 4 AND 6 THEN 'Q2'\n",
    "    WHEN MONTH(register_date) BETWEEN 7 AND 9 THEN 'Q3'\n",
    "    WHEN MONTH(register_date) BETWEEN 10 AND 12 THEN 'Q4'\n",
    "    ELSE 'Unknown'\n",
    "  END AS season_of_first_policy\n",
    "  \n",
    "from first_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88b97b50-dea8-4486-8bcf-2658c8ea4d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-1506772837420440>, line 5\u001b[0m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
       "\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m _sqldf\u001b[38;5;241m.\u001b[39mtoPandas()\n",
       "\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# df = pd.read_csv('/Users/rajesh/Desktop/improve_metrics_JOB.csv')\u001b[39;00m\n",
       "\u001b[1;32m      7\u001b[0m \n",
       "\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# age at first policy (calculated from dates)\u001b[39;00m\n",
       "\u001b[1;32m      9\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregister_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregister_date\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
       "\n",
       "\u001b[0;31mNameError\u001b[0m: name '_sqldf' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\nFile \u001b[0;32m<command-1506772837420440>, line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m _sqldf\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# df = pd.read_csv('/Users/rajesh/Desktop/improve_metrics_JOB.csv')\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# age at first policy (calculated from dates)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregister_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregister_date\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\n\u001b[0;31mNameError\u001b[0m: name '_sqldf' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name '_sqldf' is not defined",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = _sqldf.toPandas()\n",
    "# df = pd.read_csv('/Users/rajesh/Desktop/improve_metrics_JOB.csv')\n",
    "\n",
    "# age at first policy (calculated from dates)\n",
    "df['register_date'] = pd.to_datetime(df['register_date'], errors='coerce')\n",
    "df['isrd_brth_date'] = pd.to_datetime(df['isrd_brth_date'], errors='coerce')\n",
    "df['age_at_first_policy'] = (df['register_date'] - df['isrd_brth_date']).dt.days / 365.25\n",
    "\n",
    "# age at second policy\n",
    "df['second_register_date'] = pd.to_datetime(df['second_register_date'], errors='coerce')\n",
    "df['age_at_second_policy'] = (df['second_register_date'] - df['isrd_brth_date']).dt.days / 365.25\n",
    "\n",
    "# time gap between first and second policy\n",
    "df['years_to_second'] = (df['second_register_date'] - df['register_date']).dt.days / 365.25\n",
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "# Handle missing values\n",
    "\n",
    "# drop rows with missing target or critical features\n",
    "critical_cols = ['product_category']\n",
    "df = df.dropna(subset=critical_cols)\n",
    "\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "display(num_cols)\n",
    "\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "display(cat_cols)\n",
    "\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "# List of financial columns\n",
    "financial_cols = [col for col in df.columns if col.startswith('wc_')] + ['face_amt', 'cash_val_amt', 'acct_val_amt']\n",
    "financial_cols = [col for col in financial_cols if col in df.columns]\n",
    "\n",
    "# Compute skewness for each financial column\n",
    "skewness_dict = {col: stats.skew(df[col].dropna()) for col in financial_cols}\n",
    "skew_df = pd.DataFrame([skewness_dict])\n",
    "display(skew_df)\n",
    "\n",
    "# Apply log1p transformation to reduce skewness\n",
    "for col in financial_cols:\n",
    "    df[f'log_{col}'] = np.log1p(df[col])\n",
    "\n",
    "# Compute skewness for each log-transformed financial column\n",
    "log_skewness_dict = {f'log_{col}': stats.skew(df[f'log_{col}'].dropna()) for col in financial_cols}\n",
    "log_skew_df = pd.DataFrame([log_skewness_dict])\n",
    "display(log_skew_df)\n",
    "\n",
    "# Standardize date columns\n",
    "date_cols = ['register_date', 'second_register_date', 'isrd_brth_date', 'trmn_eff_date', 'second_trmn_eff_date']\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# Remove outliers in numerical features\n",
    "df = df[(df['age_at_first_policy'] >= 0) & (df['age_at_first_policy'] <= 100)]\n",
    "\n",
    "# Categorical encoding (LabelEncoder is correct for tree models, but for Spark MLlib, use StringIndexer)\n",
    "cat_cols = [\n",
    "    'product_category', 'prod_lob', 'client_seg', 'aum_band', 'agt_class', 'season_of_first_policy', 'client_seg_1', 'division_name','mkt_prod_hier', 'policy_status', 'channel', 'agent_segment']\n",
    "for col in cat_cols + ['second_product_category']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "        \n",
    "# NOTE: Target encoding will be done AFTER train/test split to avoid data leakage\n",
    "# This cell is just identifying which columns need target encoding\n",
    "high_cardinality_cols = ['client_seg', 'client_seg_1', 'division_name', 'mkt_prod_hier']\n",
    "\n",
    "high_cardinality_cols = [col for col in high_cardinality_cols if col in df.columns]\n",
    "print(f\"Columns identified for target encoding: {high_cardinality_cols}\")\n",
    "# Interaction features to capture non-linear relationships (with proper NaN/inf handling)\n",
    "if {\"age_at_first_policy\", \"wc_total_assets\"}.issubset(df.columns):\n",
    "    df[\"age_assets\"] = (df[\"age_at_first_policy\"] * df[\"wc_total_assets\"]).replace([np.inf, -np.inf], np.nan)\n",
    "else:\n",
    "    df[\"age_assets\"] = np.nan\n",
    "\n",
    "if {\"age_at_first_policy\", \"stock_allocation_ratio\"}.issubset(df.columns):\n",
    "    df[\"age_equity_ratio\"] = (df[\"age_at_first_policy\"] * df[\"stock_allocation_ratio\"]).replace([np.inf, -np.inf], np.nan)\n",
    "else:\n",
    "    df[\"age_equity_ratio\"] = np.nan\n",
    "\n",
    "# NOTE: days_since_first_policy will be calculated AFTER train/test split to prevent data leakage\n",
    "# This calculation will use only training data reference date\n",
    "if \"register_date\" in df.columns:\n",
    "    # Placeholder - will be recalculated after split\n",
    "    df[\"days_since_first_policy\"] = np.nan\n",
    "    print(\"days_since_first_policy will be calculated after train/test split\")\n",
    "else:\n",
    "    df[\"days_since_first_policy\"] = np.nan\n",
    "\n",
    "if \"wc_total_assets\" in df.columns:\n",
    "    df[\"log_total_assets\"] = np.log1p(df[\"wc_total_assets\"].clip(lower=0))\n",
    "    df[\"log_total_assets\"] = df[\"log_total_assets\"].replace([np.inf, -np.inf], np.nan)\n",
    "else:\n",
    "    df[\"log_total_assets\"] = np.nan\n",
    "\n",
    "if {\"stock_allocation_ratio\", \"bond_allocation_ratio\"}.issubset(df.columns):\n",
    "    bond_ratio = df[\"bond_allocation_ratio\"].replace(0, np.nan)\n",
    "    df[\"equity_to_bond_ratio\"] = (df[\"stock_allocation_ratio\"] / bond_ratio).replace([np.inf, -np.inf], np.nan)\n",
    "else:\n",
    "    df[\"equity_to_bond_ratio\"] = np.nan\n",
    "\n",
    "# if {\"premium_amount\", \"income_estimate\"}.issubset(df.columns):\n",
    "#     income = df[\"income_estimate\"].replace(0, np.nan)\n",
    "#     df[\"premium_to_income\"] = (df[\"premium_amount\"] / income).replace([np.inf, -np.inf], np.nan)\n",
    "# else:\n",
    "#     df[\"premium_to_income\"] = np.nan\n",
    "\n",
    "\n",
    "# NOTE: Clustering will be done AFTER train/test split to avoid data leakage\n",
    "# This cell just identifies which features will be used for clustering\n",
    "cluster_features = [\n",
    "    \"age_at_first_policy\",\n",
    "    \"wc_total_assets\",\n",
    "    \"stock_allocation_ratio\",\n",
    "    \"bond_allocation_ratio\",\n",
    "    \"annuity_allocation_ratio\",\n",
    "    \"mutual_fund_allocation_ratio\",\n",
    "    \"aum_to_asset_ratio\",\n",
    "    \"policy_value_to_assets_ratio\",\n",
    "    \"age_assets\",\n",
    "    \"age_equity_ratio\",\n",
    "    \"log_total_assets\",\n",
    "    \"equity_to_bond_ratio\",\n",
    "    \"days_since_first_policy\"\n",
    "]\n",
    "available_cluster_features = [col for col in cluster_features if col in df.columns]\n",
    "print(f\"Features identified for clustering: {available_cluster_features}\")\n",
    "\n",
    "df = df.drop(columns=[\n",
    "    'log_wc_assetmix_stocks',\n",
    "    'log_wc_assetmix_bonds',\n",
    "    'log_wc_assetmix_mutual_funds',\n",
    "    'log_wc_assetmix_deposits',\n",
    "    'log_wc_assetmix_other_assets',\n",
    "    'log_acct_val_amt'\n",
    "])\n",
    "\n",
    "\n",
    "# Install required packages\n",
    "!pip install catboost scikit-learn imbalanced-learn\n",
    "# Prepare data for modeling - filter rows with second_product_category\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "df_trainable = df[df['second_product_category'].notna()].copy()\n",
    "print(f\"Rows with second_product_category: {len(df_trainable)}\")\n",
    "\n",
    "# Remove invalid target classes (None, nan, etc.)\n",
    "valid_targets = ['DISABILITY', 'HEALTH', 'INVESTMENT', 'LIFE_INSURANCE', 'NETWORK_PRODUCTS', 'RETIREMENT', 'OTHER']\n",
    "df_trainable = df_trainable[df_trainable['second_product_category'].isin(valid_targets)].copy()\n",
    "print(f\"Rows after filtering valid targets: {len(df_trainable)}\")\n",
    "\n",
    "# ===== CLASS MERGING: Merge rare classes to handle severe imbalance =====\n",
    "# Merge DISABILITY and HEALTH into OTHER_HEALTH (they're both very rare)\n",
    "print(\"\\n=== Merging Rare Classes ===\")\n",
    "print(f\"Before merging - DISABILITY: {(df_trainable['second_product_category'] == 'DISABILITY').sum()}\")\n",
    "print(f\"Before merging - HEALTH: {(df_trainable['second_product_category'] == 'HEALTH').sum()}\")\n",
    "\n",
    "df_trainable['second_product_category'] = df_trainable['second_product_category'].replace({\n",
    "    'DISABILITY': 'OTHER_HEALTH',\n",
    "    'HEALTH': 'OTHER_HEALTH'\n",
    "})\n",
    "\n",
    "print(f\"After merging - OTHER_HEALTH: {(df_trainable['second_product_category'] == 'OTHER_HEALTH').sum()}\")\n",
    "\n",
    "# Update valid targets after merging\n",
    "valid_targets = ['OTHER_HEALTH', 'INVESTMENT', 'LIFE_INSURANCE', 'NETWORK_PRODUCTS', 'RETIREMENT', 'OTHER']\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\n=== Class Distribution (After Merging) ===\")\n",
    "class_dist = df_trainable['second_product_category'].value_counts()\n",
    "print(class_dist)\n",
    "print(f\"\\nClass percentages:\\n{class_dist / len(df_trainable) * 100}\")\n",
    "\n",
    "# Drop rows with missing critical features\n",
    "critical_cols = ['age_at_first_policy', 'years_to_second', 'product_category']\n",
    "df_trainable = df_trainable.dropna(subset=critical_cols)\n",
    "print(f\"\\nRows after dropping missing critical features: {len(df_trainable)}\")\n",
    "\n",
    "# NOTE: DO NOT create features using second_product_category (the target variable)\n",
    "# This would cause data leakage. Features should only use information from the FIRST product.\n",
    "# Examples of what NOT to do:\n",
    "# - is_same_category = (product_category == second_product_category)  # LEAKAGE!\n",
    "# - product_transition = product_category + '_TO_' + second_product_category  # LEAKAGE!\n",
    "\n",
    "# ===== ADDITIONAL FEATURE ENGINEERING (NO LEAKAGE) =====\n",
    "# Extract temporal features from FIRST policy register_date only\n",
    "if 'register_date' in df_trainable.columns:\n",
    "    df_trainable['register_date'] = pd.to_datetime(df_trainable['register_date'], errors='coerce')\n",
    "    df_trainable['register_month'] = df_trainable['register_date'].dt.month\n",
    "    df_trainable['register_quarter'] = df_trainable['register_date'].dt.quarter\n",
    "    df_trainable['register_day_of_week'] = df_trainable['register_date'].dt.dayofweek\n",
    "    df_trainable['register_year'] = df_trainable['register_date'].dt.year\n",
    "    print(\"Temporal features added from register_date\")\n",
    "\n",
    "# Create age bands instead of raw age\n",
    "if 'age_at_first_policy' in df_trainable.columns:\n",
    "    df_trainable['age_band'] = pd.cut(\n",
    "        df_trainable['age_at_first_policy'], \n",
    "        bins=[0, 30, 40, 50, 60, 70, 100],\n",
    "        labels=['<30', '30-40', '40-50', '50-60', '60-70', '70+']\n",
    "    )\n",
    "    df_trainable['age_band'] = df_trainable['age_band'].astype(str)\n",
    "    print(\"Age bands created\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4990421a-7afb-4790-9402-d31a69bfd05b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train/Test split BEFORE any feature engineering to prevent data leakage\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check if we have enough samples per class for stratification\n",
    "class_counts = df_trainable['second_product_category'].value_counts()\n",
    "min_class_count = class_counts.min()\n",
    "print(f\"Minimum class count: {min_class_count}\")\n",
    "\n",
    "# Use stratification if all classes have at least 2 samples (for 20% test split)\n",
    "if min_class_count >= 2:\n",
    "    train_df, val_df = train_test_split(\n",
    "        df_trainable, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=df_trainable['second_product_category']\n",
    "    )\n",
    "else:\n",
    "    # If some classes are too small, don't stratify\n",
    "    train_df, val_df = train_test_split(\n",
    "        df_trainable, \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    print(\"Warning: Some classes too small for stratification, using random split\")\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}\")\n",
    "print(f\"\\nTrain class distribution:\\n{train_df['second_product_category'].value_counts()}\")\n",
    "print(f\"\\nVal class distribution:\\n{val_df['second_product_category'].value_counts()}\")\n",
    "\n",
    "# ===== FIX: Calculate days_since_first_policy from TRAIN data only =====\n",
    "if \"register_date\" in train_df.columns:\n",
    "    reference_date = train_df[\"register_date\"].max()  # Use train only!\n",
    "    train_df[\"days_since_first_policy\"] = (reference_date - train_df[\"register_date\"]).dt.days\n",
    "    train_df[\"days_since_first_policy\"] = train_df[\"days_since_first_policy\"].replace([np.inf, -np.inf], np.nan)\n",
    "    val_df[\"days_since_first_policy\"] = (reference_date - val_df[\"register_date\"]).dt.days\n",
    "    val_df[\"days_since_first_policy\"] = val_df[\"days_since_first_policy\"].replace([np.inf, -np.inf], np.nan)\n",
    "    print(\"days_since_first_policy calculated using train data reference date\")\n",
    "\n",
    "# Median imputation for allocation ratio columns (fit on train, apply to val)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "allocation_cols = [\n",
    "    'stock_allocation_ratio', 'bond_allocation_ratio', 'annuity_allocation_ratio',\n",
    "    'mutual_fund_allocation_ratio', 'aum_to_asset_ratio', 'policy_value_to_assets_ratio'\n",
    "]\n",
    "allocation_cols = [c for c in allocation_cols if c in train_df.columns]\n",
    "\n",
    "if allocation_cols:\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    train_df[allocation_cols] = imputer.fit_transform(train_df[allocation_cols])\n",
    "    val_df[allocation_cols] = imputer.transform(val_df[allocation_cols])\n",
    "    print(f\"Imputed {len(allocation_cols)} allocation ratio columns\")\n",
    "\n",
    "\n",
    "# Create propensity features from TRAIN data only (to prevent leakage)\n",
    "def create_propensity_features(train_data):\n",
    "    \"\"\"Create propensity features from training data\"\"\"\n",
    "    # Product-level cross-sell popularity\n",
    "    prod_counts = train_data.groupby('product_category').size().reset_index(name='p1_cross_sell_popularity')\n",
    "    \n",
    "    # Most common next product\n",
    "    most_common = train_data.groupby(['product_category', 'second_product_category']).size().reset_index(name='count')\n",
    "    most_common = most_common.sort_values('count', ascending=False).drop_duplicates('product_category')\n",
    "    most_common = most_common[['product_category', 'second_product_category']].rename(\n",
    "        columns={'second_product_category': 'p1_most_common_next_prod'}\n",
    "    )\n",
    "    \n",
    "    # Agent-level counts\n",
    "    agent_counts = train_data.groupby(['agt_no', 'product_category']).size().reset_index(name='agent_p1_cross_sell_count')\n",
    "    \n",
    "    # Branch-level counts\n",
    "    branch_counts = train_data.groupby(['branchoffice_code', 'product_category']).size().reset_index(name='branch_p1_cross_sell_count')\n",
    "    \n",
    "    return prod_counts, most_common, agent_counts, branch_counts\n",
    "\n",
    "def add_propensity_features(df, prod_counts, most_common, agent_counts, branch_counts):\n",
    "    \"\"\"Add propensity features to dataframe\"\"\"\n",
    "    df = df.merge(prod_counts, on='product_category', how='left')\n",
    "    df = df.merge(most_common, on='product_category', how='left')\n",
    "    df = df.merge(agent_counts, on=['agt_no', 'product_category'], how='left')\n",
    "    df = df.merge(branch_counts, on=['branchoffice_code', 'product_category'], how='left')\n",
    "    \n",
    "    # Fill missing values\n",
    "    df['p1_cross_sell_popularity'] = df['p1_cross_sell_popularity'].fillna(0)\n",
    "    df['agent_p1_cross_sell_count'] = df['agent_p1_cross_sell_count'].fillna(0)\n",
    "    df['branch_p1_cross_sell_count'] = df['branch_p1_cross_sell_count'].fillna(0)\n",
    "    df['p1_most_common_next_prod'] = df['p1_most_common_next_prod'].fillna('UNKNOWN')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create propensity features from TRAIN data only\n",
    "prod_counts, most_common, agent_counts, branch_counts = create_propensity_features(train_df)\n",
    "\n",
    "# Add propensity features to both train and val\n",
    "train_df = add_propensity_features(train_df, prod_counts, most_common, agent_counts, branch_counts)\n",
    "val_df = add_propensity_features(val_df, prod_counts, most_common, agent_counts, branch_counts)\n",
    "print(\"Propensity features added\")\n",
    "\n",
    "# ===== ADD: Product category interaction features (from train only) =====\n",
    "product_interactions = train_df.groupby('product_category').agg({\n",
    "    'wc_total_assets': 'mean',\n",
    "    'age_at_first_policy': 'mean',\n",
    "    'stock_allocation_ratio': 'mean'\n",
    "}).reset_index()\n",
    "product_interactions.columns = ['product_category', 'avg_assets_by_product', 'avg_age_by_product', 'avg_stock_ratio_by_product']\n",
    "\n",
    "train_df = train_df.merge(product_interactions, on='product_category', how='left')\n",
    "val_df = val_df.merge(product_interactions, on='product_category', how='left')\n",
    "print(\"Product category interaction features added\")\n",
    "\n",
    "# ===== ADD: Agent performance features (from train only) =====\n",
    "agent_stats = train_df.groupby('agt_no').agg({\n",
    "    'second_product_category': lambda x: x.value_counts().index[0] if len(x) > 0 else 'UNKNOWN',\n",
    "    'wc_total_assets': 'mean'\n",
    "}).reset_index()\n",
    "agent_stats.columns = ['agt_no', 'agent_most_common_cross_sell', 'agent_avg_assets']\n",
    "train_df = train_df.merge(agent_stats, on='agt_no', how='left')\n",
    "val_df = val_df.merge(agent_stats, on='agt_no', how='left')\n",
    "print(\"Agent performance features added\")\n",
    "\n",
    "# ===== ADD: Product × AUM interaction =====\n",
    "train_df['product_aum_interaction'] = train_df['product_category'].astype(str) + '_' + train_df['aum_band'].astype(str)\n",
    "val_df['product_aum_interaction'] = val_df['product_category'].astype(str) + '_' + val_df['aum_band'].astype(str)\n",
    "print(\"Product × AUM interaction feature added\")\n",
    "\n",
    "# Target encoding with 5-fold regularization (ONLY on train data)\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def target_encode_with_kfold(train_data, val_data, cols_to_encode, target_col, smoothing=20):\n",
    "    \"\"\"Target encode high-cardinality categoricals using K-fold on train, apply to val\"\"\"\n",
    "    train_encoded = train_data.copy()\n",
    "    val_encoded = val_data.copy()\n",
    "    \n",
    "    # Get unique classes for multi-class encoding\n",
    "    unique_classes = sorted(train_data[target_col].dropna().unique())\n",
    "    \n",
    "    for col in cols_to_encode:\n",
    "        if col not in train_data.columns:\n",
    "            continue\n",
    "            \n",
    "        # For each class, create a target encoding\n",
    "        for class_val in unique_classes:\n",
    "            te_col = f\"te_{col}_{class_val}\"\n",
    "            train_encoded[te_col] = np.nan\n",
    "            val_encoded[te_col] = np.nan\n",
    "            \n",
    "            # Create binary target for this class\n",
    "            train_data_binary = (train_data[target_col] == class_val).astype(int)\n",
    "            global_mean = train_data_binary.mean()\n",
    "            \n",
    "            # K-fold encoding on train\n",
    "            kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            valid_indices = train_data.index[train_data[target_col].notnull()].to_numpy()\n",
    "            \n",
    "            for train_idx, val_idx in kfold.split(valid_indices):\n",
    "                train_fold_idx = valid_indices[train_idx]\n",
    "                val_fold_idx = valid_indices[val_idx]\n",
    "                \n",
    "                fold_stats = train_data_binary.loc[train_fold_idx].groupby(train_data.loc[train_fold_idx, col]).agg(['mean', 'count'])\n",
    "                fold_stats['smoothed'] = (\n",
    "                    fold_stats['mean'] * fold_stats['count'] + global_mean * smoothing\n",
    "                ) / (fold_stats['count'] + smoothing)\n",
    "                mapping = fold_stats['smoothed']\n",
    "                \n",
    "                train_encoded.loc[val_fold_idx, te_col] = train_data.loc[val_fold_idx, col].map(mapping)\n",
    "            \n",
    "            # Fill remaining NaNs with global mean\n",
    "            train_encoded[te_col] = train_encoded[te_col].fillna(global_mean)\n",
    "            \n",
    "            # Apply encoding to validation set using full train stats\n",
    "            full_stats = train_data_binary.groupby(train_data[col]).agg(['mean', 'count'])\n",
    "            full_stats['smoothed'] = (\n",
    "                full_stats['mean'] * full_stats['count'] + global_mean * smoothing\n",
    "            ) / (full_stats['count'] + smoothing)\n",
    "            val_mapping = full_stats['smoothed']\n",
    "            val_encoded[te_col] = val_data[col].map(val_mapping).fillna(global_mean)\n",
    "    \n",
    "    return train_encoded, val_encoded\n",
    "\n",
    "# Apply target encoding with higher smoothing for imbalanced classes\n",
    "# Use higher smoothing parameter to prevent overfitting on minority classes\n",
    "high_cardinality_cols = [col for col in high_cardinality_cols if col in train_df.columns]\n",
    "if high_cardinality_cols:\n",
    "    # Increase smoothing from 20 to 50 for better handling of imbalanced data\n",
    "    train_df, val_df = target_encode_with_kfold(\n",
    "        train_df, val_df, high_cardinality_cols, 'second_product_category', smoothing=50\n",
    "    )\n",
    "    print(f\"Target encoding applied to {len(high_cardinality_cols)} columns with smoothing=50\")\n",
    "\n",
    "\n",
    "# Customer clustering (fit on train, apply to val)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "available_cluster_features = [col for col in available_cluster_features if col in train_df.columns]\n",
    "\n",
    "if available_cluster_features:\n",
    "    # Fill NaN with median for clustering\n",
    "    cluster_train = train_df[available_cluster_features].fillna(train_df[available_cluster_features].median())\n",
    "    cluster_val = val_df[available_cluster_features].fillna(train_df[available_cluster_features].median())\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    cluster_train_scaled = scaler.fit_transform(cluster_train)\n",
    "    cluster_val_scaled = scaler.transform(cluster_val)\n",
    "    \n",
    "    # Cluster\n",
    "    kmeans = KMeans(n_clusters=8, random_state=42, n_init=10)\n",
    "    train_df['client_cluster'] = kmeans.fit_predict(cluster_train_scaled).astype(str)\n",
    "    val_df['client_cluster'] = kmeans.predict(cluster_val_scaled).astype(str)\n",
    "    print(\"Clustering applied\")\n",
    "else:\n",
    "    train_df['client_cluster'] = 'UNKNOWN'\n",
    "    val_df['client_cluster'] = 'UNKNOWN'\n",
    "\n",
    "# Compute per-class weights using balanced approach\n",
    "# Use inverse frequency with stronger smoothing to prevent extreme weights\n",
    "class_counts = train_df['second_product_category'].value_counts()\n",
    "total_samples = len(train_df)\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "# Calculate balanced weights but cap them to prevent extreme values\n",
    "# Use square root scaling to reduce the impact of extreme imbalance\n",
    "max_weight_cap = 10.0  # Cap maximum weight to prevent over-weighting minority classes\n",
    "min_weight_floor = 0.5  # Floor minimum weight\n",
    "\n",
    "class_weights_dict = {}\n",
    "for class_name, count in class_counts.items():\n",
    "    # Standard balanced weight\n",
    "    balanced_weight = total_samples / (num_classes * count)\n",
    "    # Apply square root scaling to reduce extreme weights\n",
    "    sqrt_scaled_weight = np.sqrt(balanced_weight)\n",
    "    # Cap the weight\n",
    "    capped_weight = min(max(sqrt_scaled_weight, min_weight_floor), max_weight_cap)\n",
    "    class_weights_dict[class_name] = capped_weight\n",
    "\n",
    "print(\"Class weights (capped and sqrt-scaled):\")\n",
    "for k, v in sorted(class_weights_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {k}: {v:.4f} (count: {class_counts[k]}, ratio: {class_counts[k]/total_samples:.4f})\")\n",
    "\n",
    "# Store for model, but don't use in Pool (use class_weights parameter instead)\n",
    "print(\"\\nClass weights computed (will be used in model, not as sample weights)\")\n",
    "\n",
    "# # Filter to only features that exist in train_df\n",
    "feature_cols = [col for col in train_df.columns]\n",
    "\n",
    "# # Separate categorical and numeric\n",
    "categorical_feature_cols = [col for col in cat_cols if col in feature_cols] + ['p1_most_common_next_prod', 'client_cluster']\n",
    "\n",
    "\n",
    "# print(f\"Total features: {len(feature_cols)}\")\n",
    "# print(f\"  - Categorical: {len(categorical_feature_cols)}\")\n",
    "# print(f\"  - Numeric: {len(numeric_feature_cols)}\")\n",
    "# print(f\"  - Target encoded: {len(target_encoded_cols)}\")\n",
    "cols_to_be_removed = [\n",
    " 'branchoffice_code',\n",
    "'agt_no',\n",
    "'axa_party_id',\n",
    "'policy_no',\n",
    "'register_date',\n",
    "'trmn_eff_date',\n",
    "# 'isrd_brt_date',\n",
    "'acct_val_amt',\n",
    " 'face_amt',\n",
    " 'cash_val_amt',\n",
    " 'wc_total_assets',\n",
    " 'wc_assetmix_stocks',\n",
    " 'wc_assetmix_bonds',\n",
    " 'wc_assetmix_mutual_funds',\n",
    " 'wc_assetmix_annuity',\n",
    " 'wc_assetmix_deposits',\n",
    " 'wc_assetmix_other_assets',\n",
    "  'business_month',\n",
    "'Product',\n",
    "'second_policy_no',\n",
    " 'second_register_date',\n",
    " 'second_trmn_eff_date',\n",
    " 'second_wti_lob_txt',\n",
    " 'second_prod_lob',\n",
    " 'second_sub_product_level_1',\n",
    " 'second_sub_product_level_2',\n",
    " 'second_Product',\n",
    " 'age_at_second_policy',\n",
    " 'isrd_brth_date']\n",
    "\n",
    "target_encoded_cols = [col for col in train_df.columns if col.startswith(\"te_\")]\n",
    "for col in target_encoded_cols:\n",
    "  if col in train_df.columns:\n",
    "    cols_to_be_removed.append(col)\n",
    "# Only drop columns that exist in each DataFrame\n",
    "train_df = train_df.drop(\n",
    "    columns=[col for col in cols_to_be_removed if col in train_df.columns]\n",
    ")\n",
    "val_df = val_df.drop(\n",
    "    columns=[col for col in cols_to_be_removed if col in val_df.columns]\n",
    ")\n",
    "\n",
    "display(train_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "categorical_feature_cols = [\n",
    "    'wti_lob_txt', 'prod_lob', 'agt_class', 'client_seg', 'client_seg_1', 'aum_band',\n",
    "    'sub_product_level_1', 'sub_product_level_2', 'division_name', 'mkt_prod_hier',\n",
    "    'policy_status', 'channel', 'agent_segment', 'product_category',\n",
    "    'season_of_first_policy', 'p1_most_common_next_prod', 'client_cluster',\n",
    "    'age_band', 'register_month', 'register_quarter', 'register_day_of_week',\n",
    "    'cross_sell_timing_category', 'agent_most_common_cross_sell', 'product_aum_interaction'\n",
    "]\n",
    "# Filter to only columns that exist\n",
    "categorical_feature_cols = [col for col in categorical_feature_cols if col in train_df.columns]\n",
    "\n",
    "numerical_feature_cols = [\n",
    "    'psn_age', 'stock_allocation_ratio', 'bond_allocation_ratio', 'annuity_allocation_ratio',\n",
    "    'mutual_fund_allocation_ratio', 'aum_to_asset_ratio', 'policy_value_to_assets_ratio',\n",
    "    'age_at_first_policy', 'years_to_second', 'log_wc_total_assets', 'log_wc_assetmix_annuity',\n",
    "    'log_face_amt', 'log_cash_val_amt', 'age_assets', 'age_equity_ratio', 'days_since_first_policy',\n",
    "    'log_total_assets', 'equity_to_bond_ratio', 'p1_cross_sell_popularity',\n",
    "    'agent_p1_cross_sell_count', 'branch_p1_cross_sell_count',\n",
    "    'total_policies_per_client', 'days_between_policies',  # NEW: From SQL\n",
    "    'avg_assets_by_product', 'avg_age_by_product', 'avg_stock_ratio_by_product',  # NEW: Product interactions\n",
    "    'agent_avg_assets'  # NEW: Agent stats\n",
    "]\n",
    "\n",
    "print(\"Categorical columns:\", categorical_feature_cols)\n",
    "print(\"Numerical columns:\", numerical_feature_cols)\n",
    "\n",
    "# ===== IMPROVED MISSING VALUE HANDLING =====\n",
    "# Add missing value indicators before imputation\n",
    "missing_indicator_cols = []\n",
    "for col in numerical_feature_cols:\n",
    "    if col in train_df.columns:\n",
    "        # Create missing indicator\n",
    "        train_df[f'{col}_is_missing'] = train_df[col].isna().astype(int)\n",
    "        val_df[f'{col}_is_missing'] = val_df[col].isna().astype(int)\n",
    "        missing_indicator_cols.append(f'{col}_is_missing')\n",
    "        \n",
    "        # Then impute with median\n",
    "        median_val = train_df[col].median()\n",
    "        train_df[col] = train_df[col].fillna(median_val)\n",
    "        val_df[col] = val_df[col].fillna(median_val)\n",
    "\n",
    "# Add missing indicators to numerical features list\n",
    "numerical_feature_cols.extend(missing_indicator_cols)\n",
    "\n",
    "# Fill NaN in categorical features with mode\n",
    "for col in categorical_feature_cols:\n",
    "    if col in train_df.columns:\n",
    "        mode_val = train_df[col].mode()[0] if len(train_df[col].mode()) > 0 else 'UNKNOWN'\n",
    "        train_df[col] = train_df[col].fillna(mode_val)\n",
    "        val_df[col] = val_df[col].fillna(mode_val)\n",
    "\n",
    "print(f\"Missing values handled. Added {len(missing_indicator_cols)} missing indicators.\")\n",
    "# Exclude target and weight columns from features\n",
    "pool_columns = [col for col in train_df.columns if col not in ['second_product_category', 'class_weight']]\n",
    "\n",
    "# ===== ACTIVATE SMOTE for minority classes =====\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n=== Before SMOTE ===\")\n",
    "print(f\"Train class distribution:\\n{train_df['second_product_category'].value_counts()}\")\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "\n",
    "# Calculate target sample sizes for SMOTE\n",
    "# Upsample minority classes to at least 10% of majority class size\n",
    "majority_class_size = train_df['second_product_category'].value_counts().max()\n",
    "target_sizes = {}\n",
    "for class_name, count in train_df['second_product_category'].value_counts().items():\n",
    "    if count < majority_class_size * 0.1:  # If less than 10% of majority\n",
    "        target_sizes[class_name] = int(majority_class_size * 0.15)  # Upsample to 15% of majority\n",
    "        print(f\"  {class_name}: {count} -> {target_sizes[class_name]} (upsampling)\")\n",
    "\n",
    "if target_sizes:\n",
    "    # Store original train_df before SMOTE (for reference)\n",
    "    train_df_original = train_df.copy()\n",
    "    \n",
    "    # Prepare data for SMOTE\n",
    "    X_train_smote = train_df[pool_columns].copy()\n",
    "    y_train_smote = train_df['second_product_category'].copy()\n",
    "    \n",
    "    # Store original dtypes and categorical mappings\n",
    "    cat_encoders = {}\n",
    "    X_train_smote_encoded = X_train_smote.copy()\n",
    "    \n",
    "    # Separate numerical and categorical columns\n",
    "    numerical_cols_smote = [col for col in pool_columns if col in numerical_feature_cols]\n",
    "    categorical_cols_smote = [col for col in pool_columns if col in categorical_feature_cols]\n",
    "    \n",
    "    # Encode only categoricals for SMOTE (numericals stay as-is)\n",
    "    for col in categorical_cols_smote:\n",
    "        if col in X_train_smote_encoded.columns:\n",
    "            le = LabelEncoder()\n",
    "            # Handle NaN values\n",
    "            mask = X_train_smote_encoded[col].isna()\n",
    "            X_train_smote_encoded[col] = X_train_smote_encoded[col].astype(str)\n",
    "            X_train_smote_encoded.loc[mask, col] = 'UNKNOWN'\n",
    "            X_train_smote_encoded[col] = le.fit_transform(X_train_smote_encoded[col])\n",
    "            cat_encoders[col] = le\n",
    "    \n",
    "    # Apply SMOTE\n",
    "    # Calculate safe k_neighbors (must be less than smallest class size)\n",
    "    min_class_size = min(train_df['second_product_category'].value_counts().values)\n",
    "    safe_k_neighbors = min(3, min_class_size - 1) if min_class_size > 1 else 1\n",
    "    \n",
    "    smote = SMOTE(\n",
    "        sampling_strategy=target_sizes,\n",
    "        random_state=42,\n",
    "        k_neighbors=safe_k_neighbors\n",
    "    )\n",
    "    \n",
    "    print(\"\\nApplying SMOTE...\")\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_smote_encoded, y_train_smote)\n",
    "    \n",
    "    print(\"\\n=== After SMOTE ===\")\n",
    "    print(f\"Train class distribution:\\n{Counter(y_train_resampled)}\")\n",
    "    print(f\"Original train size: {len(train_df)}, After SMOTE: {len(X_train_resampled)}\")\n",
    "    \n",
    "    # Reconstruct train_df from resampled data\n",
    "    train_df_resampled = pd.DataFrame(X_train_resampled, columns=pool_columns)\n",
    "    \n",
    "    # Decode categoricals back to original values\n",
    "    for col in categorical_cols_smote:\n",
    "        if col in train_df_resampled.columns and col in cat_encoders:\n",
    "            # Round to nearest integer (SMOTE may produce floats)\n",
    "            train_df_resampled[col] = train_df_resampled[col].round().astype(int)\n",
    "            # Clip to valid range\n",
    "            valid_range = range(len(cat_encoders[col].classes_))\n",
    "            train_df_resampled[col] = train_df_resampled[col].clip(min(valid_range), max(valid_range))\n",
    "            train_df_resampled[col] = cat_encoders[col].inverse_transform(train_df_resampled[col])\n",
    "            # Keep UNKNOWN as is (it's a valid category)\n",
    "    \n",
    "    # Add target\n",
    "    train_df_resampled['second_product_category'] = y_train_resampled\n",
    "    \n",
    "    # Replace train_df with resampled version\n",
    "    train_df = train_df_resampled.copy()\n",
    "    print(\"SMOTE applied successfully - train_df updated with resampled data\")\n",
    "    \n",
    "    # Recalculate class weights after SMOTE\n",
    "    class_counts = train_df['second_product_category'].value_counts()\n",
    "    total_samples = len(train_df)\n",
    "    num_classes = len(class_counts)\n",
    "    \n",
    "    class_weights_dict = {}\n",
    "    for class_name, count in class_counts.items():\n",
    "        balanced_weight = total_samples / (num_classes * count)\n",
    "        sqrt_scaled_weight = np.sqrt(balanced_weight)\n",
    "        capped_weight = min(max(sqrt_scaled_weight, min_weight_floor), max_weight_cap)\n",
    "        class_weights_dict[class_name] = capped_weight\n",
    "    \n",
    "    print(\"\\nClass weights recalculated after SMOTE:\")\n",
    "    for k, v in sorted(class_weights_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {k}: {v:.4f} (count: {class_counts[k]})\")\n",
    "else:\n",
    "    print(\"No classes need SMOTE upsampling - all classes are sufficiently represented\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional recommendations for improving F1 score to >80%:\n",
    "# \n",
    "# 1. **SMOTE/ADASYN for minority classes**: Consider using SMOTE to oversample minority classes\n",
    "#    (DISABILITY, HEALTH) if they remain problematic after current improvements\n",
    "#\n",
    "# 2. **Ensemble methods**: Combine multiple models (CatBoost + XGBoost + LightGBM) for better performance\n",
    "#\n",
    "# 3. **Feature engineering (NO LEAKAGE)**: \n",
    "#    - Add temporal features from FIRST policy only: month, day of week, quarter\n",
    "#    - Add interaction features: product_category * age, product_category * total_assets\n",
    "#    - Add aggregated features from FIRST product: average assets by product_category (from train only)\n",
    "#    - Add client behavior features: time since first policy, age bands, asset allocation patterns\n",
    "#    - NEVER use second_product_category in feature creation (data leakage!)\n",
    "#\n",
    "# 4. **Hyperparameter tuning**: Use Optuna or GridSearchCV to find optimal hyperparameters\n",
    "#\n",
    "# 5. **Threshold tuning**: For each class, find optimal probability threshold instead of using default 0.5\n",
    "#\n",
    "# 6. **Cost-sensitive learning**: Adjust class weights based on business value of each prediction\n",
    "#\n",
    "# 7. **Remove or merge rare classes**: Consider merging DISABILITY and HEALTH into a single \"OTHER_HEALTH\" class\n",
    "#\n",
    "# 8. **More data**: If possible, collect more samples for minority classes\n",
    "#\n",
    "# Current improvements implemented:\n",
    "# - Removed \"None\" class\n",
    "# - CLASS MERGING: Merged DISABILITY + HEALTH → OTHER_HEALTH (active)\n",
    "# - SMOTE OVERSAMPLING: Activated for minority classes (active)\n",
    "# - Better class weight calculation with smoothing (sqrt-scaled, capped)\n",
    "# - Removed data leakage features (product_transition, is_same_category)\n",
    "# - Improved target encoding smoothing (smoothing=50)\n",
    "# - Optimized CatBoost hyperparameters for imbalanced data\n",
    "# - Added feature importance and confusion matrix analysis\n",
    "# - Added temporal features (month, quarter, day_of_week from register_date)\n",
    "# - Added age bands (categorical age groups)\n",
    "# - Added missing value indicators (before imputation)\n",
    "# - Removed max_leaves parameter (incompatible with SymmetricTree)\n",
    "# - Added ensemble model code (commented, ready to use)\n",
    "# - SQL Query improvements: pre-cleaned lookup, total policy count, time-based features\n",
    "# - Fixed data leakage: days_since_first_policy now uses train data only\n",
    "# - Added product category interaction features\n",
    "# - Added agent performance features\n",
    "# - Added product × AUM interaction features\n",
    "# - Added threshold tuning code (commented, ready to use)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ENSEMBLE MODELS (OPTIONAL - UNCOMMENT TO USE) =====\n",
    "# Ensemble can improve performance by 5-10% over single model\n",
    "# Uncomment below to train ensemble of CatBoost + XGBoost + LightGBM\n",
    "\n",
    "\"\"\"\n",
    "# Install additional libraries\n",
    "!pip install xgboost lightgbm\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare data for XGBoost and LightGBM (need label encoding for categoricals)\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(train_df['second_product_category'])\n",
    "y_val_encoded = le.transform(val_df['second_product_category'])\n",
    "\n",
    "# Get feature columns (exclude target)\n",
    "X_train = train_df[pool_columns].copy()\n",
    "X_val = val_df[pool_columns].copy()\n",
    "\n",
    "# Encode categorical features for XGBoost/LightGBM\n",
    "cat_indices = [i for i, col in enumerate(pool_columns) if col in categorical_feature_cols]\n",
    "\n",
    "# 1. XGBoost\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.05,\n",
    "    objective='multi:softprob',\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=100,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# 2. LightGBM\n",
    "lgb_model = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.05,\n",
    "    objective='multiclass',\n",
    "    random_state=42,\n",
    "    verbose=-1,\n",
    "    early_stopping_rounds=100\n",
    ")\n",
    "\n",
    "# Train individual models\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model.fit(\n",
    "    X_train, y_train_encoded,\n",
    "    eval_set=[(X_val, y_val_encoded)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Training LightGBM...\")\n",
    "lgb_model.fit(\n",
    "    X_train, y_train_encoded,\n",
    "    eval_set=[(X_val, y_val_encoded)],\n",
    "    categorical_feature=cat_indices\n",
    ")\n",
    "\n",
    "# Create ensemble\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('catboost', cat_model_regularized),\n",
    "        ('xgboost', xgb_model),\n",
    "        ('lightgbm', lgb_model)\n",
    "    ],\n",
    "    voting='soft',  # Use probability voting\n",
    "    weights=[2, 1, 1]  # Weight CatBoost more (handles categoricals better)\n",
    ")\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_pred = ensemble.predict(X_val)\n",
    "ensemble_pred_decoded = le.inverse_transform(ensemble_pred)\n",
    "\n",
    "f1_macro_ensemble = f1_score(val_df['second_product_category'], ensemble_pred_decoded, average='macro')\n",
    "print(f\"\\n=== Ensemble Performance ===\")\n",
    "print(f\"Ensemble Macro F1: {f1_macro_ensemble:.4f}\")\n",
    "print(f\"Single CatBoost Macro F1: {f1_macro:.4f}\")\n",
    "print(f\"Improvement: {f1_macro_ensemble - f1_macro:.4f}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== OPTIONAL: Per-class threshold tuning =====\n",
    "# Uncomment to find optimal probability thresholds for each class\n",
    "# This can improve Macro F1 by 5-15% for imbalanced datasets\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Find optimal thresholds for each class\n",
    "def find_optimal_thresholds(y_true, y_proba, classes):\n",
    "    optimal_thresholds = {}\n",
    "    for i, class_name in enumerate(classes):\n",
    "        y_binary = (y_true == class_name).astype(int)\n",
    "        proba_class = y_proba[:, i]\n",
    "        \n",
    "        best_threshold = 0.5\n",
    "        best_f1 = 0\n",
    "        \n",
    "        for threshold in np.arange(0.1, 0.9, 0.05):\n",
    "            y_pred_binary = (proba_class >= threshold).astype(int)\n",
    "            f1 = f1_score(y_binary, y_pred_binary, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        optimal_thresholds[class_name] = best_threshold\n",
    "        print(f\"{class_name}: optimal threshold = {best_threshold:.3f}, F1 = {best_f1:.3f}\")\n",
    "    \n",
    "    return optimal_thresholds\n",
    "\n",
    "# Get class order from model\n",
    "classes = cat_model_regularized.classes_\n",
    "optimal_thresholds = find_optimal_thresholds(\n",
    "    val_df['second_product_category'], \n",
    "    val_probabilities, \n",
    "    classes\n",
    ")\n",
    "\n",
    "# Apply optimal thresholds\n",
    "val_predictions_tuned = []\n",
    "for i in range(len(val_probabilities)):\n",
    "    probs = val_probabilities[i]\n",
    "    # Find class with highest probability above its threshold\n",
    "    best_class_idx = -1\n",
    "    best_score = -1\n",
    "    for j, class_name in enumerate(classes):\n",
    "        threshold = optimal_thresholds[class_name]\n",
    "        if probs[j] >= threshold and probs[j] > best_score:\n",
    "            best_score = probs[j]\n",
    "            best_class_idx = j\n",
    "    \n",
    "    if best_class_idx == -1:\n",
    "        # If no class meets threshold, use default prediction\n",
    "        best_class_idx = np.argmax(probs)\n",
    "    \n",
    "    val_predictions_tuned.append(classes[best_class_idx])\n",
    "\n",
    "val_predictions_tuned = np.array(val_predictions_tuned)\n",
    "f1_macro_tuned = f1_score(val_df['second_product_category'], val_predictions_tuned, average='macro')\n",
    "print(f\"\\n=== Threshold Tuning Results ===\")\n",
    "print(f\"Original Macro F1: {f1_macro:.4f}\")\n",
    "print(f\"Tuned Macro F1: {f1_macro_tuned:.4f}\")\n",
    "print(f\"Improvement: {f1_macro_tuned - f1_macro:.4f}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d21a0a84-121b-43b0-bcc1-a1d82a79352a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['psn_age', 'wc_total_assets', 'wc_assetmix_stocks', 'wc_assetmix_bonds',\n",
       "       'wc_assetmix_mutual_funds', 'wc_assetmix_annuity',\n",
       "       'wc_assetmix_deposits', 'wc_assetmix_other_assets',\n",
       "       'stock_allocation_ratio', 'bond_allocation_ratio',\n",
       "       'annuity_allocation_ratio', 'mutual_fund_allocation_ratio',\n",
       "       'aum_to_asset_ratio', 'policy_value_to_assets_ratio',\n",
       "       'age_at_first_policy', 'age_at_second_policy', 'years_to_second'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['axa_party_id', 'policy_no', 'trmn_eff_date', 'wti_lob_txt', 'prod_lob',\n",
       "       'agt_class', 'client_seg', 'client_seg_1', 'aum_band',\n",
       "       'sub_product_level_1', 'sub_product_level_2', 'Product',\n",
       "       'branchoffice_code', 'agt_no', 'division_name', 'mkt_prod_hier',\n",
       "       'policy_status', 'channel', 'agent_segment', 'second_policy_no',\n",
       "       'second_trmn_eff_date', 'second_wti_lob_txt', 'second_prod_lob',\n",
       "       'second_sub_product_level_1', 'second_sub_product_level_2',\n",
       "       'second_Product', 'product_category', 'second_product_category',\n",
       "       'season_of_first_policy'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install catboost\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost import Pool\n",
    "# Ensure categorical features don't include target or weight\n",
    "cat_feature_names = [col for col in categorical_feature_cols if col in pool_columns]\n",
    "\n",
    "\n",
    "# Create pools WITHOUT sample weights (using class_weights in model instead)\n",
    "# Using both weight in Pool AND class_weights in model causes double-weighting\n",
    "train_pool = Pool(\n",
    "    data=train_df[pool_columns],\n",
    "    label=train_df[\"second_product_category\"],\n",
    "    cat_features=cat_feature_names,\n",
    ")\n",
    "\n",
    "val_pool = Pool(\n",
    "    data=val_df[pool_columns],\n",
    "    label=val_df[\"second_product_category\"],\n",
    "    cat_features=cat_feature_names,\n",
    ")\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "# Optimized CatBoost for imbalanced multi-class classification\n",
    "# Using balanced class weights with capped values to prevent over-weighting\n",
    "cat_model_regularized = CatBoostClassifier(\n",
    "    iterations=2000,  # Increased iterations for better learning\n",
    "    depth=7,  # Moderate depth to prevent overfitting\n",
    "    learning_rate=0.05,  # Balanced learning rate\n",
    "    loss_function=\"MultiClass\",  # Multi-class classification\n",
    "    eval_metric=\"TotalF1\",  # Focus on F1 score\n",
    "    random_seed=42,\n",
    "    task_type=\"CPU\",\n",
    "    l2_leaf_reg=3.0,  # Moderate regularization\n",
    "    subsample=0.85,  # Use more data per tree\n",
    "    colsample_bylevel=0.85,  # Use more features per level\n",
    "    min_data_in_leaf=50,  # Higher minimum samples to prevent overfitting on minority classes\n",
    "    early_stopping_rounds=100,  # Stop if no improvement\n",
    "    # Note: max_leaves only works with Lossguide, so removed for SymmetricTree\n",
    "    verbose=100,\n",
    "    bootstrap_type=\"Bernoulli\",\n",
    "    class_weights=class_weights_dict,  # Capped class weights to prevent extreme weighting\n",
    "    grow_policy=\"SymmetricTree\",  # More stable than Lossguide for imbalanced data\n",
    "    boosting_type=\"Plain\",  # Standard boosting\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_model_regularized.fit(\n",
    "    train_pool, \n",
    "    eval_set=val_pool,\n",
    "    use_best_model=True  # Use best model based on validation\n",
    ")\n",
    "\n",
    "print(\"Regularized CatBoost model trained\")\n",
    "\n",
    "# Evaluate CatBoost model\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "val_predictions = cat_model_regularized.predict(val_pool)\n",
    "val_probabilities = cat_model_regularized.predict_proba(val_pool)\n",
    "\n",
    "f1_macro = f1_score(val_df['second_product_category'], val_predictions, average='macro')\n",
    "f1_weighted = f1_score(val_df['second_product_category'], val_predictions, average='weighted')\n",
    "accuracy = accuracy_score(val_df['second_product_category'], val_predictions)\n",
    "precision = precision_score(val_df['second_product_category'], val_predictions, average='macro')\n",
    "recall = recall_score(val_df['second_product_category'], val_predictions, average='macro')\n",
    "\n",
    "print(f\"\\n=== Model Performance ===\")\n",
    "print(f\"Macro F1: {f1_macro:.4f}\")\n",
    "print(f\"Weighted F1: {f1_weighted:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro Precision: {precision:.4f}\")\n",
    "print(f\"Macro Recall: {recall:.4f}\")\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(val_df['second_product_category'], val_predictions))\n",
    "\n",
    "# Feature importance analysis\n",
    "print(\"\\n=== Top 20 Feature Importances ===\")\n",
    "feature_importance = cat_model_regularized.get_feature_importance()\n",
    "feature_names = pool_columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(importance_df.head(20))\n",
    "\n",
    "# Per-class performance analysis\n",
    "print(\"\\n=== Per-Class Performance Analysis ===\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(val_df['second_product_category'], val_predictions)\n",
    "classes = sorted(val_df['second_product_category'].unique())\n",
    "cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_df)\n",
    "\n",
    "# Analyze prediction distribution\n",
    "print(\"\\n=== Prediction Distribution ===\")\n",
    "pred_dist = pd.Series(val_predictions.flatten()).value_counts().sort_index()\n",
    "true_dist = val_df['second_product_category'].value_counts().sort_index()\n",
    "comparison_df = pd.DataFrame({\n",
    "    'True Count': true_dist,\n",
    "    'Predicted Count': pred_dist,\n",
    "    'Difference': pred_dist - true_dist,\n",
    "    'Difference %': ((pred_dist - true_dist) / true_dist * 100).round(2)\n",
    "})\n",
    "print(comparison_df)\n",
    "\n",
    "# If model is still over-predicting minority classes, consider:\n",
    "# 1. Further reducing class weights (lower max_weight_cap)\n",
    "# 2. Using SMOTE for oversampling\n",
    "# 3. Merging very rare classes (DISABILITY + HEALTH)\n",
    "# 4. Using cost-sensitive threshold tuning per class\n",
    "# Display predictions with probabilities\n",
    "predictions_df = val_df[['product_category', 'second_product_category']].copy()\n",
    "predictions_df['predicted_second_product_category'] = val_predictions\n",
    "predictions_df['max_probability'] = val_probabilities.max(axis=1)\n",
    "predictions_df['prediction_confidence'] = predictions_df['max_probability'].apply(\n",
    "    lambda x: 'High' if x > 0.7 else 'Medium' if x > 0.5 else 'Low'\n",
    ")\n",
    "\n",
    "display(predictions_df.head(20))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "consolidated_ml_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
