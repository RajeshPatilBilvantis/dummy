{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "cbc2609d-8f3b-4e3c-b783-6c47aeb29b0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with base as (\n",
    "  select \n",
    "    r.axa_party_id,\n",
    "    r.policy_no,\n",
    "    r.register_date,\n",
    "    r.trmn_eff_date,\n",
    "    r.wti_lob_txt,\n",
    "    r.prod_lob,\n",
    "    r.agt_class,\n",
    "    r.isrd_brth_date,\n",
    "    r.psn_age,\n",
    "    r.acct_val_amt,\n",
    "    r.face_amt,\n",
    "    r.cash_val_amt,\n",
    "    r.wc_total_assets,\n",
    "    r.wc_assetmix_stocks,\n",
    "    r.wc_assetmix_bonds,\n",
    "    r.wc_assetmix_mutual_funds,\n",
    "    r.wc_assetmix_annuity,\n",
    "    r.wc_assetmix_deposits,\n",
    "    r.wc_assetmix_other_assets,\n",
    "    r.division_name,\n",
    "    r.mkt_prod_hier,\n",
    "    r.policy_status,\n",
    "    r.agent_segment,\n",
    "    r.channel,\n",
    "    r.client_seg,\n",
    "    r.client_seg_1,\n",
    "    r.aum_band,\n",
    "    r.business_month,\n",
    "    r.branchoffice_code,\n",
    "    r.agt_no,\n",
    "    h.sub_product_level_1,\n",
    "    h.sub_product_level_2,\n",
    "    h.Product,\n",
    "    row_number() over (partition by r.axa_party_id order by r.register_date asc) as rn,\n",
    "    row_number() over (partition by r.axa_party_id order by r.register_date asc) = 1 as is_first_policy\n",
    "  from dl_tenants_daas.us_wealth_management.wealth_management_client_metrics r\n",
    "  left join (\n",
    "    select distinct source_sys_id, idb_plan_cd, idb_sub_plan_cd, \n",
    "      trim(stmt_plan_typ_txt) as Product, sub_product_level_1, sub_product_level_2\n",
    "    from dl_tenants_daas.us_wealth_management.wealth_management_sub_product_group\n",
    "  ) h \n",
    "    on upper(r.source_sys_id) = upper(h.source_sys_id)\n",
    "    and trim(upper(REPLACE(LTRIM(REPLACE(r.plan_code,'0',' ')),' ','0'))) = trim(upper(h.idb_plan_cd))\n",
    "    and trim(upper(REPLACE(LTRIM(REPLACE(r.plan_subcd_code,'0',' ')),' ','0'))) = trim(upper(h.idb_sub_plan_cd))\n",
    "  where r.business_month = (select max(business_month) from dl_tenants_daas.us_wealth_management.wealth_management_client_metrics)\n",
    "    and r.axa_party_id is not null\n",
    "    and r.policy_no is not null\n",
    "),\n",
    "first_second as (\n",
    "  select\n",
    "    axa_party_id,\n",
    "    -- First policy fields\n",
    "    max(case when rn = 1 then policy_no end) as policy_no,\n",
    "    max(case when rn = 1 then register_date end) as register_date,\n",
    "    max(case when rn = 1 then trmn_eff_date end) as trmn_eff_date,\n",
    "    max(case when rn = 1 then wti_lob_txt end) as wti_lob_txt,\n",
    "    max(case when rn = 1 then prod_lob end) as prod_lob,\n",
    "    max(case when rn = 1 then agt_class end) as agt_class,\n",
    "    max(case when rn = 1 then isrd_brth_date end) as isrd_brth_date,\n",
    "    max(case when rn = 1 then psn_age end) as psn_age,\n",
    "    max(case when rn = 1 then acct_val_amt end) as acct_val_amt,\n",
    "    max(case when rn = 1 then face_amt end) as face_amt,\n",
    "    max(case when rn = 1 then cash_val_amt end) as cash_val_amt,\n",
    "    max(case when rn = 1 then wc_total_assets end) as wc_total_assets,\n",
    "    max(case when rn = 1 then wc_assetmix_stocks end) as wc_assetmix_stocks,\n",
    "    max(case when rn = 1 then wc_assetmix_bonds end) as wc_assetmix_bonds,\n",
    "    max(case when rn = 1 then wc_assetmix_mutual_funds end) as wc_assetmix_mutual_funds,\n",
    "    max(case when rn = 1 then wc_assetmix_annuity end) as wc_assetmix_annuity,\n",
    "    max(case when rn = 1 then wc_assetmix_deposits end) as wc_assetmix_deposits,\n",
    "    max(case when rn = 1 then wc_assetmix_other_assets end) as wc_assetmix_other_assets,\n",
    "    max(case when rn = 1 then client_seg end) as client_seg,\n",
    "    max(case when rn = 1 then client_seg_1 end) as client_seg_1,\n",
    "    max(case when rn = 1 then aum_band end) as aum_band,\n",
    "    max(case when rn = 1 then sub_product_level_1 end) as sub_product_level_1,\n",
    "    max(case when rn = 1 then sub_product_level_2 end) as sub_product_level_2,\n",
    "    max(case when rn = 1 then Product end) as Product,\n",
    "    max(case when rn = 1 then business_month end) as business_month,\n",
    "    max(case when rn = 1 then branchoffice_code end) as branchoffice_code,\n",
    "    max(case when rn = 1 then agt_no end) as agt_no,\n",
    "    max(case when rn = 1 then division_name end) as division_name,\n",
    "    max(case when rn = 1 then mkt_prod_hier end) as mkt_prod_hier,\n",
    "    max(case when rn = 1 then policy_status end) as policy_status ,\n",
    "    max(case when rn = 1 then channel end) as channel,\n",
    "    max(case when rn = 1 then agent_segment end) as agent_segment,\n",
    "    -- Second policy fields\n",
    "    max(case when rn = 2 then policy_no end) as second_policy_no,\n",
    "    max(case when rn = 2 then register_date end) as second_register_date,\n",
    "    max(case when rn = 2 then trmn_eff_date end) as second_trmn_eff_date,\n",
    "    max(case when rn = 2 then wti_lob_txt end) as second_wti_lob_txt,\n",
    "    max(case when rn = 2 then prod_lob end) as second_prod_lob,\n",
    "    max(case when rn = 2 then sub_product_level_1 end) as second_sub_product_level_1,\n",
    "    max(case when rn = 2 then sub_product_level_2 end) as second_sub_product_level_2,\n",
    "    max(case when rn = 2 then Product end) as second_Product\n",
    "  from base\n",
    "  where rn <= 2\n",
    "  group by axa_party_id\n",
    ")\n",
    "select *,\n",
    "  wc_assetmix_stocks / NULLIF(wc_total_assets, 0) AS stock_allocation_ratio,\n",
    "  wc_assetmix_bonds / NULLIF(wc_total_assets, 0) AS bond_allocation_ratio,\n",
    "  wc_assetmix_annuity / NULLIF(wc_total_assets, 0) AS annuity_allocation_ratio,\n",
    "  wc_assetmix_mutual_funds / NULLIF(wc_total_assets, 0) AS mutual_fund_allocation_ratio,\n",
    "  acct_val_amt / NULLIF(wc_total_assets, 0) AS aum_to_asset_ratio,\n",
    "  face_amt / NULLIF(wc_total_assets, 0) AS policy_value_to_assets_ratio,\n",
    "  \n",
    "  CASE \n",
    "    WHEN prod_lob = 'LIFE' THEN 'LIFE_INSURANCE'\n",
    "    WHEN sub_product_level_1 IN ('VLI', 'WL', 'UL/IUL', 'TERM', 'PROTECTIVE PRODUCT') THEN 'LIFE_INSURANCE'\n",
    "    WHEN sub_product_level_2 LIKE '%LIFE%' THEN 'LIFE_INSURANCE'\n",
    "    WHEN sub_product_level_2 IN ('VARIABLE UNIVERSAL LIFE', 'WHOLE LIFE', 'UNIVERSAL LIFE', \n",
    "                                'INDEX UNIVERSAL LIFE', 'TERM PRODUCT', 'VARIABLE LIFE', \n",
    "                                'SURVIVORSHIP WHOLE LIFE', 'MONY PROTECTIVE PRODUCT') THEN 'LIFE_INSURANCE'\n",
    "    WHEN prod_lob IN ('GROUP RETIREMENT', 'INDIVIDUAL RETIREMENT') THEN 'RETIREMENT'\n",
    "    WHEN sub_product_level_1 IN ('EQUIVEST', 'RETIREMENT 401K', 'ACCUMULATOR', \n",
    "                                'RETIREMENT CORNERSTONE', 'SCS', 'INVESTMENT EDGE') THEN 'RETIREMENT'\n",
    "    WHEN sub_product_level_2 LIKE '%403B%' OR sub_product_level_2 LIKE '%401%' \n",
    "         OR sub_product_level_2 LIKE '%IRA%' OR sub_product_level_2 LIKE '%SEP%' THEN 'RETIREMENT'\n",
    "    WHEN Product LIKE '%IRA%' OR Product LIKE '%401%' OR Product LIKE '%403%' \n",
    "         OR Product LIKE '%SEP%' OR Product LIKE '%Accumulator%' \n",
    "         OR Product LIKE '%Retirement%' THEN 'RETIREMENT'\n",
    "    WHEN prod_lob = 'BROKER DEALER' THEN 'INVESTMENT'\n",
    "    WHEN sub_product_level_1 IN ('INVESTMENT PRODUCT - DIRECT', 'INVESTMENT PRODUCT - BROKERAGE', \n",
    "                                'INVESTMENT PRODUCT - ADVISORY', 'DIRECT', 'BROKERAGE', \n",
    "                                'ADVISORY', 'CASH SOLICITOR') THEN 'INVESTMENT'\n",
    "    WHEN sub_product_level_2 LIKE '%Investment%' OR sub_product_level_2 LIKE '%Brokerage%' \n",
    "         OR sub_product_level_2 LIKE '%Advisory%' THEN 'INVESTMENT'\n",
    "    WHEN prod_lob = 'NETWORK' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN sub_product_level_1 = 'NETWORK PRODUCTS' OR sub_product_level_2 = 'NETWORK PRODUCTS' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN Product LIKE '%Network%' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN prod_lob = 'OTHERS' AND sub_product_level_1 = 'HAS' THEN 'DISABILITY'\n",
    "    WHEN sub_product_level_2 = 'HAS - DISABILITY' THEN 'DISABILITY'\n",
    "    WHEN Product LIKE '%Disability%' OR Product LIKE '%DI -%' THEN 'DISABILITY'\n",
    "    WHEN prod_lob = 'OTHERS' THEN 'HEALTH'\n",
    "    WHEN sub_product_level_2 = 'GROUP HEALTH PRODUCTS' THEN 'HEALTH'\n",
    "    WHEN Product LIKE '%Health%' OR Product LIKE '%Medical%' OR Product LIKE '%Hospital%' THEN 'HEALTH'\n",
    "    ELSE 'OTHER'\n",
    "  END AS product_category,\n",
    "  CASE \n",
    "    WHEN second_prod_lob IS NULL OR second_prod_lob = '' THEN NULL\n",
    "    WHEN second_prod_lob = 'LIFE' THEN 'LIFE_INSURANCE'\n",
    "    WHEN second_sub_product_level_1 IN ('VLI', 'WL', 'UL/IUL', 'TERM', 'PROTECTIVE PRODUCT') THEN 'LIFE_INSURANCE'\n",
    "    WHEN second_sub_product_level_2 LIKE '%LIFE%' THEN 'LIFE_INSURANCE'\n",
    "    WHEN second_sub_product_level_2 IN ('VARIABLE UNIVERSAL LIFE', 'WHOLE LIFE', 'UNIVERSAL LIFE', \n",
    "                                'INDEX UNIVERSAL LIFE', 'TERM PRODUCT', 'VARIABLE LIFE', \n",
    "                                'SURVIVORSHIP WHOLE LIFE', 'MONY PROTECTIVE PRODUCT') THEN 'LIFE_INSURANCE'\n",
    "    WHEN second_prod_lob IN ('GROUP RETIREMENT', 'INDIVIDUAL RETIREMENT') THEN 'RETIREMENT'\n",
    "    WHEN second_sub_product_level_1 IN ('EQUIVEST', 'RETIREMENT 401K', 'ACCUMULATOR', \n",
    "                                'RETIREMENT CORNERSTONE', 'SCS', 'INVESTMENT EDGE') THEN 'RETIREMENT'\n",
    "    WHEN second_sub_product_level_2 LIKE '%403B%' OR second_sub_product_level_2 LIKE '%401%' \n",
    "         OR second_sub_product_level_2 LIKE '%IRA%' OR second_sub_product_level_2 LIKE '%SEP%' THEN 'RETIREMENT'\n",
    "    WHEN second_Product LIKE '%IRA%' OR second_Product LIKE '%401%' OR second_Product LIKE '%403%' \n",
    "         OR second_Product LIKE '%SEP%' OR second_Product LIKE '%Accumulator%' \n",
    "         OR second_Product LIKE '%Retirement%' THEN 'RETIREMENT'\n",
    "    WHEN second_prod_lob = 'BROKER DEALER' THEN 'INVESTMENT'\n",
    "    WHEN second_sub_product_level_1 IN ('INVESTMENT PRODUCT - DIRECT', 'INVESTMENT PRODUCT - BROKERAGE', \n",
    "                                'INVESTMENT PRODUCT - ADVISORY', 'DIRECT', 'BROKERAGE', \n",
    "                                'ADVISORY', 'CASH SOLICITOR') THEN 'INVESTMENT'\n",
    "    WHEN second_sub_product_level_2 LIKE '%Investment%' OR second_sub_product_level_2 LIKE '%Brokerage%' \n",
    "         OR second_sub_product_level_2 LIKE '%Advisory%' THEN 'INVESTMENT'\n",
    "    WHEN second_prod_lob = 'NETWORK' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN second_sub_product_level_1 = 'NETWORK PRODUCTS' OR second_sub_product_level_2 = 'NETWORK PRODUCTS' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN second_Product LIKE '%Network%' THEN 'NETWORK_PRODUCTS'\n",
    "    WHEN second_prod_lob = 'OTHERS' AND second_sub_product_level_1 = 'HAS' THEN 'DISABILITY'\n",
    "    WHEN second_sub_product_level_2 = 'HAS - DISABILITY' THEN 'DISABILITY'\n",
    "    WHEN second_Product LIKE '%Disability%' OR second_Product LIKE '%DI -%' THEN 'DISABILITY'\n",
    "    WHEN second_prod_lob = 'OTHERS' THEN 'HEALTH'\n",
    "    WHEN second_sub_product_level_2 = 'GROUP HEALTH PRODUCTS' THEN 'HEALTH'\n",
    "    WHEN second_Product LIKE '%Health%' OR second_Product LIKE '%Medical%' OR second_Product LIKE '%Hospital%' THEN 'HEALTH'\n",
    "    ELSE 'OTHER'\n",
    "  END AS second_product_category,\n",
    "  CASE\n",
    "    WHEN MONTH(register_date) BETWEEN 1 AND 3 THEN 'Q1'\n",
    "    WHEN MONTH(register_date) BETWEEN 4 AND 6 THEN 'Q2'\n",
    "    WHEN MONTH(register_date) BETWEEN 7 AND 9 THEN 'Q3'\n",
    "    WHEN MONTH(register_date) BETWEEN 10 AND 12 THEN 'Q4'\n",
    "    ELSE 'Unknown'\n",
    "  END AS season_of_first_policy\n",
    "  \n",
    "from first_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88b97b50-dea8-4486-8bcf-2658c8ea4d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-1506772837420440>, line 5\u001b[0m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
       "\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m _sqldf\u001b[38;5;241m.\u001b[39mtoPandas()\n",
       "\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# df = pd.read_csv('/Users/rajesh/Desktop/improve_metrics_JOB.csv')\u001b[39;00m\n",
       "\u001b[1;32m      7\u001b[0m \n",
       "\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# age at first policy (calculated from dates)\u001b[39;00m\n",
       "\u001b[1;32m      9\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregister_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregister_date\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
       "\n",
       "\u001b[0;31mNameError\u001b[0m: name '_sqldf' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\nFile \u001b[0;32m<command-1506772837420440>, line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m _sqldf\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# df = pd.read_csv('/Users/rajesh/Desktop/improve_metrics_JOB.csv')\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# age at first policy (calculated from dates)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregister_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregister_date\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\n\u001b[0;31mNameError\u001b[0m: name '_sqldf' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name '_sqldf' is not defined",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = _sqldf.toPandas()\n",
    "# df = pd.read_csv('/Users/rajesh/Desktop/improve_metrics_JOB.csv')\n",
    "\n",
    "# age at first policy (calculated from dates)\n",
    "df['register_date'] = pd.to_datetime(df['register_date'], errors='coerce')\n",
    "df['isrd_brth_date'] = pd.to_datetime(df['isrd_brth_date'], errors='coerce')\n",
    "df['age_at_first_policy'] = (df['register_date'] - df['isrd_brth_date']).dt.days / 365.25\n",
    "\n",
    "# age at second policy\n",
    "df['second_register_date'] = pd.to_datetime(df['second_register_date'], errors='coerce')\n",
    "df['age_at_second_policy'] = (df['second_register_date'] - df['isrd_brth_date']).dt.days / 365.25\n",
    "\n",
    "# time gap between first and second policy\n",
    "df['years_to_second'] = (df['second_register_date'] - df['register_date']).dt.days / 365.25\n",
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "# Handle missing values\n",
    "\n",
    "# drop rows with missing target or critical features\n",
    "critical_cols = ['product_category']\n",
    "df = df.dropna(subset=critical_cols)\n",
    "\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "display(num_cols)\n",
    "\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "display(cat_cols)\n",
    "\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "# List of financial columns\n",
    "financial_cols = [col for col in df.columns if col.startswith('wc_')] + ['face_amt', 'cash_val_amt', 'acct_val_amt']\n",
    "financial_cols = [col for col in financial_cols if col in df.columns]\n",
    "\n",
    "# Compute skewness for each financial column\n",
    "skewness_dict = {col: stats.skew(df[col].dropna()) for col in financial_cols}\n",
    "skew_df = pd.DataFrame([skewness_dict])\n",
    "display(skew_df)\n",
    "\n",
    "# Apply log1p transformation to reduce skewness\n",
    "for col in financial_cols:\n",
    "    df[f'log_{col}'] = np.log1p(df[col])\n",
    "\n",
    "# Compute skewness for each log-transformed financial column\n",
    "log_skewness_dict = {f'log_{col}': stats.skew(df[f'log_{col}'].dropna()) for col in financial_cols}\n",
    "log_skew_df = pd.DataFrame([log_skewness_dict])\n",
    "display(log_skew_df)\n",
    "\n",
    "# Standardize date columns\n",
    "date_cols = ['register_date', 'second_register_date', 'isrd_brth_date', 'trmn_eff_date', 'second_trmn_eff_date']\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# Remove outliers in numerical features\n",
    "df = df[(df['age_at_first_policy'] >= 0) & (df['age_at_first_policy'] <= 100)]\n",
    "\n",
    "# Categorical encoding (LabelEncoder is correct for tree models, but for Spark MLlib, use StringIndexer)\n",
    "cat_cols = [\n",
    "    'product_category', 'prod_lob', 'client_seg', 'aum_band', 'agt_class', 'season_of_first_policy', 'client_seg_1', 'division_name','mkt_prod_hier', 'policy_status', 'channel', 'agent_segment']\n",
    "for col in cat_cols + ['second_product_category']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "        \n",
    "# NOTE: Target encoding will be done AFTER train/test split to avoid data leakage\n",
    "# This cell is just identifying which columns need target encoding\n",
    "high_cardinality_cols = ['client_seg', 'client_seg_1', 'division_name', 'mkt_prod_hier']\n",
    "\n",
    "high_cardinality_cols = [col for col in high_cardinality_cols if col in df.columns]\n",
    "print(f\"Columns identified for target encoding: {high_cardinality_cols}\")\n",
    "# Interaction features to capture non-linear relationships (with proper NaN/inf handling)\n",
    "if {\"age_at_first_policy\", \"wc_total_assets\"}.issubset(df.columns):\n",
    "    df[\"age_assets\"] = (df[\"age_at_first_policy\"] * df[\"wc_total_assets\"]).replace([np.inf, -np.inf], np.nan)\n",
    "else:\n",
    "    df[\"age_assets\"] = np.nan\n",
    "\n",
    "if {\"age_at_first_policy\", \"stock_allocation_ratio\"}.issubset(df.columns):\n",
    "    df[\"age_equity_ratio\"] = (df[\"age_at_first_policy\"] * df[\"stock_allocation_ratio\"]).replace([np.inf, -np.inf], np.nan)\n",
    "else:\n",
    "    df[\"age_equity_ratio\"] = np.nan\n",
    "\n",
    "if \"register_date\" in df.columns:\n",
    "    reference_date = df[\"register_date\"].max()\n",
    "    df[\"days_since_first_policy\"] = (reference_date - df[\"register_date\"]).dt.days\n",
    "    df[\"days_since_first_policy\"] = df[\"days_since_first_policy\"].replace([np.inf, -np.inf], np.nan)\n",
    "else:\n",
    "    df[\"days_since_first_policy\"] = np.nan\n",
    "\n",
    "if \"wc_total_assets\" in df.columns:\n",
    "    df[\"log_total_assets\"] = np.log1p(df[\"wc_total_assets\"].clip(lower=0))\n",
    "    df[\"log_total_assets\"] = df[\"log_total_assets\"].replace([np.inf, -np.inf], np.nan)\n",
    "else:\n",
    "    df[\"log_total_assets\"] = np.nan\n",
    "\n",
    "if {\"stock_allocation_ratio\", \"bond_allocation_ratio\"}.issubset(df.columns):\n",
    "    bond_ratio = df[\"bond_allocation_ratio\"].replace(0, np.nan)\n",
    "    df[\"equity_to_bond_ratio\"] = (df[\"stock_allocation_ratio\"] / bond_ratio).replace([np.inf, -np.inf], np.nan)\n",
    "else:\n",
    "    df[\"equity_to_bond_ratio\"] = np.nan\n",
    "\n",
    "# if {\"premium_amount\", \"income_estimate\"}.issubset(df.columns):\n",
    "#     income = df[\"income_estimate\"].replace(0, np.nan)\n",
    "#     df[\"premium_to_income\"] = (df[\"premium_amount\"] / income).replace([np.inf, -np.inf], np.nan)\n",
    "# else:\n",
    "#     df[\"premium_to_income\"] = np.nan\n",
    "\n",
    "\n",
    "# NOTE: Clustering will be done AFTER train/test split to avoid data leakage\n",
    "# This cell just identifies which features will be used for clustering\n",
    "cluster_features = [\n",
    "    \"age_at_first_policy\",\n",
    "    \"wc_total_assets\",\n",
    "    \"stock_allocation_ratio\",\n",
    "    \"bond_allocation_ratio\",\n",
    "    \"annuity_allocation_ratio\",\n",
    "    \"mutual_fund_allocation_ratio\",\n",
    "    \"aum_to_asset_ratio\",\n",
    "    \"policy_value_to_assets_ratio\",\n",
    "    \"age_assets\",\n",
    "    \"age_equity_ratio\",\n",
    "    \"log_total_assets\",\n",
    "    \"equity_to_bond_ratio\",\n",
    "    \"days_since_first_policy\"\n",
    "]\n",
    "available_cluster_features = [col for col in cluster_features if col in df.columns]\n",
    "print(f\"Features identified for clustering: {available_cluster_features}\")\n",
    "\n",
    "df = df.drop(columns=[\n",
    "    'log_wc_assetmix_stocks',\n",
    "    'log_wc_assetmix_bonds',\n",
    "    'log_wc_assetmix_mutual_funds',\n",
    "    'log_wc_assetmix_deposits',\n",
    "    'log_wc_assetmix_other_assets',\n",
    "    'log_acct_val_amt'\n",
    "])\n",
    "\n",
    "\n",
    "# Install required packages\n",
    "!pip install catboost scikit-learn\n",
    "# Prepare data for modeling - filter rows with second_product_category\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "df_trainable = df[df['second_product_category'].notna()].copy()\n",
    "print(f\"Rows with second_product_category: {len(df_trainable)}\")\n",
    "\n",
    "# Remove invalid target classes (None, nan, etc.)\n",
    "valid_targets = ['DISABILITY', 'HEALTH', 'INVESTMENT', 'LIFE_INSURANCE', 'NETWORK_PRODUCTS', 'RETIREMENT', 'OTHER']\n",
    "df_trainable = df_trainable[df_trainable['second_product_category'].isin(valid_targets)].copy()\n",
    "print(f\"Rows after filtering valid targets: {len(df_trainable)}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\n=== Class Distribution ===\")\n",
    "class_dist = df_trainable['second_product_category'].value_counts()\n",
    "print(class_dist)\n",
    "print(f\"\\nClass percentages:\\n{class_dist / len(df_trainable) * 100}\")\n",
    "\n",
    "# Drop rows with missing critical features\n",
    "critical_cols = ['age_at_first_policy', 'years_to_second', 'product_category']\n",
    "df_trainable = df_trainable.dropna(subset=critical_cols)\n",
    "print(f\"\\nRows after dropping missing critical features: {len(df_trainable)}\")\n",
    "\n",
    "# Add feature: same product category flag\n",
    "df_trainable['is_same_category'] = (df_trainable['product_category'] == df_trainable['second_product_category']).astype(int)\n",
    "\n",
    "# Add feature: product category transition (first -> second)\n",
    "df_trainable['product_transition'] = df_trainable['product_category'].astype(str) + '_TO_' + df_trainable['second_product_category'].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4990421a-7afb-4790-9402-d31a69bfd05b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train/Test split BEFORE any feature engineering to prevent data leakage\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check if we have enough samples per class for stratification\n",
    "class_counts = df_trainable['second_product_category'].value_counts()\n",
    "min_class_count = class_counts.min()\n",
    "print(f\"Minimum class count: {min_class_count}\")\n",
    "\n",
    "# Use stratification if all classes have at least 2 samples (for 20% test split)\n",
    "if min_class_count >= 2:\n",
    "    train_df, val_df = train_test_split(\n",
    "        df_trainable, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=df_trainable['second_product_category']\n",
    "    )\n",
    "else:\n",
    "    # If some classes are too small, don't stratify\n",
    "    train_df, val_df = train_test_split(\n",
    "        df_trainable, \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    print(\"Warning: Some classes too small for stratification, using random split\")\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}\")\n",
    "print(f\"\\nTrain class distribution:\\n{train_df['second_product_category'].value_counts()}\")\n",
    "print(f\"\\nVal class distribution:\\n{val_df['second_product_category'].value_counts()}\")\n",
    "\n",
    "# Median imputation for allocation ratio columns (fit on train, apply to val)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "allocation_cols = [\n",
    "    'stock_allocation_ratio', 'bond_allocation_ratio', 'annuity_allocation_ratio',\n",
    "    'mutual_fund_allocation_ratio', 'aum_to_asset_ratio', 'policy_value_to_assets_ratio'\n",
    "]\n",
    "allocation_cols = [c for c in allocation_cols if c in train_df.columns]\n",
    "\n",
    "if allocation_cols:\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    train_df[allocation_cols] = imputer.fit_transform(train_df[allocation_cols])\n",
    "    val_df[allocation_cols] = imputer.transform(val_df[allocation_cols])\n",
    "    print(f\"Imputed {len(allocation_cols)} allocation ratio columns\")\n",
    "\n",
    "\n",
    "# Create propensity features from TRAIN data only (to prevent leakage)\n",
    "def create_propensity_features(train_data):\n",
    "    \"\"\"Create propensity features from training data\"\"\"\n",
    "    # Product-level cross-sell popularity\n",
    "    prod_counts = train_data.groupby('product_category').size().reset_index(name='p1_cross_sell_popularity')\n",
    "    \n",
    "    # Most common next product\n",
    "    most_common = train_data.groupby(['product_category', 'second_product_category']).size().reset_index(name='count')\n",
    "    most_common = most_common.sort_values('count', ascending=False).drop_duplicates('product_category')\n",
    "    most_common = most_common[['product_category', 'second_product_category']].rename(\n",
    "        columns={'second_product_category': 'p1_most_common_next_prod'}\n",
    "    )\n",
    "    \n",
    "    # Agent-level counts\n",
    "    agent_counts = train_data.groupby(['agt_no', 'product_category']).size().reset_index(name='agent_p1_cross_sell_count')\n",
    "    \n",
    "    # Branch-level counts\n",
    "    branch_counts = train_data.groupby(['branchoffice_code', 'product_category']).size().reset_index(name='branch_p1_cross_sell_count')\n",
    "    \n",
    "    return prod_counts, most_common, agent_counts, branch_counts\n",
    "\n",
    "def add_propensity_features(df, prod_counts, most_common, agent_counts, branch_counts):\n",
    "    \"\"\"Add propensity features to dataframe\"\"\"\n",
    "    df = df.merge(prod_counts, on='product_category', how='left')\n",
    "    df = df.merge(most_common, on='product_category', how='left')\n",
    "    df = df.merge(agent_counts, on=['agt_no', 'product_category'], how='left')\n",
    "    df = df.merge(branch_counts, on=['branchoffice_code', 'product_category'], how='left')\n",
    "    \n",
    "    # Fill missing values\n",
    "    df['p1_cross_sell_popularity'] = df['p1_cross_sell_popularity'].fillna(0)\n",
    "    df['agent_p1_cross_sell_count'] = df['agent_p1_cross_sell_count'].fillna(0)\n",
    "    df['branch_p1_cross_sell_count'] = df['branch_p1_cross_sell_count'].fillna(0)\n",
    "    df['p1_most_common_next_prod'] = df['p1_most_common_next_prod'].fillna('UNKNOWN')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create propensity features from TRAIN data only\n",
    "prod_counts, most_common, agent_counts, branch_counts = create_propensity_features(train_df)\n",
    "\n",
    "# Add propensity features to both train and val\n",
    "train_df = add_propensity_features(train_df, prod_counts, most_common, agent_counts, branch_counts)\n",
    "val_df = add_propensity_features(val_df, prod_counts, most_common, agent_counts, branch_counts)\n",
    "print(\"Propensity features added\")\n",
    "\n",
    "# Target encoding with 5-fold regularization (ONLY on train data)\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def target_encode_with_kfold(train_data, val_data, cols_to_encode, target_col, smoothing=20):\n",
    "    \"\"\"Target encode high-cardinality categoricals using K-fold on train, apply to val\"\"\"\n",
    "    train_encoded = train_data.copy()\n",
    "    val_encoded = val_data.copy()\n",
    "    \n",
    "    # Get unique classes for multi-class encoding\n",
    "    unique_classes = sorted(train_data[target_col].dropna().unique())\n",
    "    \n",
    "    for col in cols_to_encode:\n",
    "        if col not in train_data.columns:\n",
    "            continue\n",
    "            \n",
    "        # For each class, create a target encoding\n",
    "        for class_val in unique_classes:\n",
    "            te_col = f\"te_{col}_{class_val}\"\n",
    "            train_encoded[te_col] = np.nan\n",
    "            val_encoded[te_col] = np.nan\n",
    "            \n",
    "            # Create binary target for this class\n",
    "            train_data_binary = (train_data[target_col] == class_val).astype(int)\n",
    "            global_mean = train_data_binary.mean()\n",
    "            \n",
    "            # K-fold encoding on train\n",
    "            kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            valid_indices = train_data.index[train_data[target_col].notnull()].to_numpy()\n",
    "            \n",
    "            for train_idx, val_idx in kfold.split(valid_indices):\n",
    "                train_fold_idx = valid_indices[train_idx]\n",
    "                val_fold_idx = valid_indices[val_idx]\n",
    "                \n",
    "                fold_stats = train_data_binary.loc[train_fold_idx].groupby(train_data.loc[train_fold_idx, col]).agg(['mean', 'count'])\n",
    "                fold_stats['smoothed'] = (\n",
    "                    fold_stats['mean'] * fold_stats['count'] + global_mean * smoothing\n",
    "                ) / (fold_stats['count'] + smoothing)\n",
    "                mapping = fold_stats['smoothed']\n",
    "                \n",
    "                train_encoded.loc[val_fold_idx, te_col] = train_data.loc[val_fold_idx, col].map(mapping)\n",
    "            \n",
    "            # Fill remaining NaNs with global mean\n",
    "            train_encoded[te_col] = train_encoded[te_col].fillna(global_mean)\n",
    "            \n",
    "            # Apply encoding to validation set using full train stats\n",
    "            full_stats = train_data_binary.groupby(train_data[col]).agg(['mean', 'count'])\n",
    "            full_stats['smoothed'] = (\n",
    "                full_stats['mean'] * full_stats['count'] + global_mean * smoothing\n",
    "            ) / (full_stats['count'] + smoothing)\n",
    "            val_mapping = full_stats['smoothed']\n",
    "            val_encoded[te_col] = val_data[col].map(val_mapping).fillna(global_mean)\n",
    "    \n",
    "    return train_encoded, val_encoded\n",
    "\n",
    "# Apply target encoding with higher smoothing for imbalanced classes\n",
    "# Use higher smoothing parameter to prevent overfitting on minority classes\n",
    "high_cardinality_cols = [col for col in high_cardinality_cols if col in train_df.columns]\n",
    "if high_cardinality_cols:\n",
    "    # Increase smoothing from 20 to 50 for better handling of imbalanced data\n",
    "    train_df, val_df = target_encode_with_kfold(\n",
    "        train_df, val_df, high_cardinality_cols, 'second_product_category', smoothing=50\n",
    "    )\n",
    "    print(f\"Target encoding applied to {len(high_cardinality_cols)} columns with smoothing=50\")\n",
    "\n",
    "\n",
    "# Customer clustering (fit on train, apply to val)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "available_cluster_features = [col for col in available_cluster_features if col in train_df.columns]\n",
    "\n",
    "if available_cluster_features:\n",
    "    # Fill NaN with median for clustering\n",
    "    cluster_train = train_df[available_cluster_features].fillna(train_df[available_cluster_features].median())\n",
    "    cluster_val = val_df[available_cluster_features].fillna(train_df[available_cluster_features].median())\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    cluster_train_scaled = scaler.fit_transform(cluster_train)\n",
    "    cluster_val_scaled = scaler.transform(cluster_val)\n",
    "    \n",
    "    # Cluster\n",
    "    kmeans = KMeans(n_clusters=8, random_state=42, n_init=10)\n",
    "    train_df['client_cluster'] = kmeans.fit_predict(cluster_train_scaled).astype(str)\n",
    "    val_df['client_cluster'] = kmeans.predict(cluster_val_scaled).astype(str)\n",
    "    print(\"Clustering applied\")\n",
    "else:\n",
    "    train_df['client_cluster'] = 'UNKNOWN'\n",
    "    val_df['client_cluster'] = 'UNKNOWN'\n",
    "\n",
    "# Compute per-class weights using balanced approach\n",
    "# Use inverse frequency with smoothing to handle extreme imbalance\n",
    "class_counts = train_df['second_product_category'].value_counts()\n",
    "total_samples = len(train_df)\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "# Balanced class weights: n_samples / (n_classes * class_count)\n",
    "# Add smoothing factor to prevent extreme weights\n",
    "smoothing_factor = 0.1\n",
    "class_weights_dict = {}\n",
    "for class_name, count in class_counts.items():\n",
    "    # Standard balanced weight\n",
    "    balanced_weight = total_samples / (num_classes * count)\n",
    "    # Apply smoothing: weight = (1 - smoothing) * balanced_weight + smoothing * 1.0\n",
    "    smoothed_weight = (1 - smoothing_factor) * balanced_weight + smoothing_factor * 1.0\n",
    "    class_weights_dict[class_name] = smoothed_weight\n",
    "\n",
    "print(\"Class weights:\")\n",
    "for k, v in sorted(class_weights_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {k}: {v:.4f} (count: {class_counts[k]})\")\n",
    "\n",
    "train_df['class_weight'] = train_df['second_product_category'].map(class_weights_dict)\n",
    "val_df['class_weight'] = val_df['second_product_category'].map(class_weights_dict).fillna(1.0)\n",
    "print(\"\\nClass weights computed\")\n",
    "\n",
    "# # Filter to only features that exist in train_df\n",
    "feature_cols = [col for col in train_df.columns]\n",
    "\n",
    "# # Separate categorical and numeric\n",
    "categorical_feature_cols = [col for col in cat_cols if col in feature_cols] + ['p1_most_common_next_prod', 'client_cluster']\n",
    "\n",
    "\n",
    "# print(f\"Total features: {len(feature_cols)}\")\n",
    "# print(f\"  - Categorical: {len(categorical_feature_cols)}\")\n",
    "# print(f\"  - Numeric: {len(numeric_feature_cols)}\")\n",
    "# print(f\"  - Target encoded: {len(target_encoded_cols)}\")\n",
    "cols_to_be_removed = [\n",
    " 'branchoffice_code',\n",
    "'agt_no',\n",
    "'axa_party_id',\n",
    "'policy_no',\n",
    "'register_date',\n",
    "'trmn_eff_date',\n",
    "# 'isrd_brt_date',\n",
    "'acct_val_amt',\n",
    " 'face_amt',\n",
    " 'cash_val_amt',\n",
    " 'wc_total_assets',\n",
    " 'wc_assetmix_stocks',\n",
    " 'wc_assetmix_bonds',\n",
    " 'wc_assetmix_mutual_funds',\n",
    " 'wc_assetmix_annuity',\n",
    " 'wc_assetmix_deposits',\n",
    " 'wc_assetmix_other_assets',\n",
    "  'business_month',\n",
    "'Product',\n",
    "'second_policy_no',\n",
    " 'second_register_date',\n",
    " 'second_trmn_eff_date',\n",
    " 'second_wti_lob_txt',\n",
    " 'second_prod_lob',\n",
    " 'second_sub_product_level_1',\n",
    " 'second_sub_product_level_2',\n",
    " 'second_Product',\n",
    " 'age_at_second_policy',\n",
    " 'isrd_brth_date']\n",
    "\n",
    "target_encoded_cols = [col for col in train_df.columns if col.startswith(\"te_\")]\n",
    "for col in target_encoded_cols:\n",
    "  if col in train_df.columns:\n",
    "    cols_to_be_removed.append(col)\n",
    "# Only drop columns that exist in each DataFrame\n",
    "train_df = train_df.drop(\n",
    "    columns=[col for col in cols_to_be_removed if col in train_df.columns]\n",
    ")\n",
    "val_df = val_df.drop(\n",
    "    columns=[col for col in cols_to_be_removed if col in val_df.columns]\n",
    ")\n",
    "\n",
    "display(train_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "categorical_feature_cols = [\n",
    "    'wti_lob_txt', 'prod_lob', 'agt_class', 'client_seg', 'client_seg_1', 'aum_band',\n",
    "    'sub_product_level_1', 'sub_product_level_2', 'division_name', 'mkt_prod_hier',\n",
    "    'policy_status', 'channel', 'agent_segment', 'product_category',\n",
    "    'season_of_first_policy', 'p1_most_common_next_prod', 'client_cluster', 'product_transition'\n",
    "]\n",
    "\n",
    "numerical_feature_cols = [\n",
    "    'psn_age', 'stock_allocation_ratio', 'bond_allocation_ratio', 'annuity_allocation_ratio',\n",
    "    'mutual_fund_allocation_ratio', 'aum_to_asset_ratio', 'policy_value_to_assets_ratio',\n",
    "    'age_at_first_policy', 'years_to_second', 'log_wc_total_assets', 'log_wc_assetmix_annuity',\n",
    "    'log_face_amt', 'log_cash_val_amt', 'age_assets', 'age_equity_ratio', 'days_since_first_policy',\n",
    "    'log_total_assets', 'equity_to_bond_ratio', 'p1_cross_sell_popularity',\n",
    "    'agent_p1_cross_sell_count', 'branch_p1_cross_sell_count', 'is_same_category'\n",
    "]\n",
    "\n",
    "print(\"Categorical columns:\", categorical_feature_cols)\n",
    "print(\"Numerical columns:\", numerical_feature_cols)\n",
    "# Handle missing values in features before training\n",
    "# Fill NaN in numeric features with median\n",
    "for col in numerical_feature_cols:\n",
    "    if col in train_df.columns:\n",
    "        # print(col)\n",
    "        median_val = train_df[col].median()\n",
    "        train_df[col] = train_df[col].fillna(median_val)\n",
    "        val_df[col] = val_df[col].fillna(median_val)\n",
    "\n",
    "# Fill NaN in categorical features with mode\n",
    "for col in categorical_feature_cols:\n",
    "    if col in train_df.columns:\n",
    "        mode_val = train_df[col].mode()[0] if len(train_df[col].mode()) > 0 else 'UNKNOWN'\n",
    "        train_df[col] = train_df[col].fillna(mode_val)\n",
    "        val_df[col] = val_df[col].fillna(mode_val)\n",
    "\n",
    "print(\"Missing values handled\")\n",
    "# Exclude target and weight columns from features\n",
    "pool_columns = [col for col in train_df.columns if col not in ['second_product_category', 'class_weight']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional recommendations for improving F1 score to >80%:\n",
    "# \n",
    "# 1. **SMOTE/ADASYN for minority classes**: Consider using SMOTE to oversample minority classes\n",
    "#    (DISABILITY, HEALTH) if they remain problematic after current improvements\n",
    "#\n",
    "# 2. **Ensemble methods**: Combine multiple models (CatBoost + XGBoost + LightGBM) for better performance\n",
    "#\n",
    "# 3. **Feature engineering**: \n",
    "#    - Add more temporal features (month, day of week, time since first policy)\n",
    "#    - Add interaction features between product_category and other key features\n",
    "#    - Add aggregated features (e.g., average assets by product category)\n",
    "#\n",
    "# 4. **Hyperparameter tuning**: Use Optuna or GridSearchCV to find optimal hyperparameters\n",
    "#\n",
    "# 5. **Threshold tuning**: For each class, find optimal probability threshold instead of using default 0.5\n",
    "#\n",
    "# 6. **Cost-sensitive learning**: Adjust class weights based on business value of each prediction\n",
    "#\n",
    "# 7. **Remove or merge rare classes**: Consider merging DISABILITY and HEALTH into a single \"OTHER_HEALTH\" class\n",
    "#\n",
    "# 8. **More data**: If possible, collect more samples for minority classes\n",
    "#\n",
    "# Current improvements implemented:\n",
    "# - Removed \"None\" class\n",
    "# - Better class weight calculation with smoothing\n",
    "# - Added product transition features\n",
    "# - Improved target encoding smoothing\n",
    "# - Optimized CatBoost hyperparameters for imbalanced data\n",
    "# - Added feature importance and confusion matrix analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d21a0a84-121b-43b0-bcc1-a1d82a79352a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['psn_age', 'wc_total_assets', 'wc_assetmix_stocks', 'wc_assetmix_bonds',\n",
       "       'wc_assetmix_mutual_funds', 'wc_assetmix_annuity',\n",
       "       'wc_assetmix_deposits', 'wc_assetmix_other_assets',\n",
       "       'stock_allocation_ratio', 'bond_allocation_ratio',\n",
       "       'annuity_allocation_ratio', 'mutual_fund_allocation_ratio',\n",
       "       'aum_to_asset_ratio', 'policy_value_to_assets_ratio',\n",
       "       'age_at_first_policy', 'age_at_second_policy', 'years_to_second'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['axa_party_id', 'policy_no', 'trmn_eff_date', 'wti_lob_txt', 'prod_lob',\n",
       "       'agt_class', 'client_seg', 'client_seg_1', 'aum_band',\n",
       "       'sub_product_level_1', 'sub_product_level_2', 'Product',\n",
       "       'branchoffice_code', 'agt_no', 'division_name', 'mkt_prod_hier',\n",
       "       'policy_status', 'channel', 'agent_segment', 'second_policy_no',\n",
       "       'second_trmn_eff_date', 'second_wti_lob_txt', 'second_prod_lob',\n",
       "       'second_sub_product_level_1', 'second_sub_product_level_2',\n",
       "       'second_Product', 'product_category', 'second_product_category',\n",
       "       'season_of_first_policy'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install catboost\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost import Pool\n",
    "# Ensure categorical features don't include target or weight\n",
    "cat_feature_names = [col for col in categorical_feature_cols if col in pool_columns]\n",
    "\n",
    "\n",
    "train_pool = Pool(\n",
    "    data=train_df[pool_columns],\n",
    "    label=train_df[\"second_product_category\"],\n",
    "    cat_features=cat_feature_names,\n",
    "    weight=train_df[\"class_weight\"],\n",
    ")\n",
    "\n",
    "val_pool = Pool(\n",
    "    data=val_df[pool_columns],\n",
    "    label=val_df[\"second_product_category\"],\n",
    "    cat_features=cat_feature_names,\n",
    "    weight=val_df[\"class_weight\"],\n",
    ")\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "# Optimized CatBoost for imbalanced multi-class classification\n",
    "# Key changes: balanced mode, better regularization, class weights\n",
    "cat_model_regularized = CatBoostClassifier(\n",
    "    iterations=2000,  # Increased iterations for better learning\n",
    "    depth=8,  # Slightly deeper for complex patterns\n",
    "    learning_rate=0.05,  # Balanced learning rate\n",
    "    loss_function=\"MultiClass\",  # Multi-class classification\n",
    "    eval_metric=\"TotalF1\",  # Focus on F1 score\n",
    "    random_seed=42,\n",
    "    task_type=\"CPU\",\n",
    "    l2_leaf_reg=5.0,  # Moderate regularization\n",
    "    subsample=0.8,  # Use more data per tree\n",
    "    colsample_bylevel=0.8,  # Use more features per level\n",
    "    min_data_in_leaf=20,  # Higher minimum samples for minority classes\n",
    "    max_leaves=64,  # Allow more leaves for complex patterns\n",
    "    early_stopping_rounds=150,  # More patience for improvement\n",
    "    verbose=100,\n",
    "    bootstrap_type=\"Bernoulli\",\n",
    "    class_weights=class_weights_dict,  # Explicit class weights (don't use auto_class_weights with this)\n",
    "    grow_policy=\"Lossguide\",  # Better for imbalanced data\n",
    "    boosting_type=\"Plain\",  # Standard boosting\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_model_regularized.fit(\n",
    "    train_pool, \n",
    "    eval_set=val_pool,\n",
    "    use_best_model=True  # Use best model based on validation\n",
    ")\n",
    "\n",
    "print(\"Regularized CatBoost model trained\")\n",
    "\n",
    "# Evaluate CatBoost model\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "val_predictions = cat_model_regularized.predict(val_pool)\n",
    "val_probabilities = cat_model_regularized.predict_proba(val_pool)\n",
    "\n",
    "f1_macro = f1_score(val_df['second_product_category'], val_predictions, average='macro')\n",
    "f1_weighted = f1_score(val_df['second_product_category'], val_predictions, average='weighted')\n",
    "accuracy = accuracy_score(val_df['second_product_category'], val_predictions)\n",
    "precision = precision_score(val_df['second_product_category'], val_predictions, average='macro')\n",
    "recall = recall_score(val_df['second_product_category'], val_predictions, average='macro')\n",
    "\n",
    "print(f\"\\n=== Model Performance ===\")\n",
    "print(f\"Macro F1: {f1_macro:.4f}\")\n",
    "print(f\"Weighted F1: {f1_weighted:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro Precision: {precision:.4f}\")\n",
    "print(f\"Macro Recall: {recall:.4f}\")\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(val_df['second_product_category'], val_predictions))\n",
    "\n",
    "# Feature importance analysis\n",
    "print(\"\\n=== Top 20 Feature Importances ===\")\n",
    "feature_importance = cat_model_regularized.get_feature_importance()\n",
    "feature_names = pool_columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(importance_df.head(20))\n",
    "\n",
    "# Per-class performance analysis\n",
    "print(\"\\n=== Per-Class Performance Analysis ===\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(val_df['second_product_category'], val_predictions)\n",
    "classes = sorted(val_df['second_product_category'].unique())\n",
    "cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_df)\n",
    "# Display predictions with probabilities\n",
    "predictions_df = val_df[['product_category', 'second_product_category']].copy()\n",
    "predictions_df['predicted_second_product_category'] = val_predictions\n",
    "predictions_df['max_probability'] = val_probabilities.max(axis=1)\n",
    "predictions_df['prediction_confidence'] = predictions_df['max_probability'].apply(\n",
    "    lambda x: 'High' if x > 0.7 else 'Medium' if x > 0.5 else 'Low'\n",
    ")\n",
    "\n",
    "display(predictions_df.head(20))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "consolidated_ml_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
