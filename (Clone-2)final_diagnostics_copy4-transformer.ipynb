{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3705869a-eabf-4fab-9ff2-7115bc1b1440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install synapseml\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5995642e-7c97-4210-962c-cf4daf7b36f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install CatBoost using pip\n",
    "# Best practice: Use %pip to install packages in Databricks notebooks to ensure the environment is updated.\n",
    "# %pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c49c87f-acbc-41c0-8bf1-7850413ae15c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use Spark DataFrame for large data exploration\n",
    "# This will load the complete table as a Spark DataFrame (distributed, scalable)\n",
    "df_raw = spark.table(\"dl_tenants_daas.us_wealth_management.wealth_management_client_metrics\")\n",
    "\n",
    "# Show schema and first few rows\n",
    "# df_raw.printSchema()\n",
    "# display(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "92442ea8-dbfc-4a79-a4aa-ebe847a53bb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Use Spark DataFrame for large data exploration\n",
    "# # This will load the filtered table as a Spark DataFrame (distributed, scalable)\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# # Find the most recent business_month\n",
    "# recent_month = spark.table(\"dl_tenants_daas.us_wealth_management.wealth_management_client_metrics\") \\\n",
    "#     .agg(F.max(\"business_month\").alias(\"recent_month\")).collect()[0][\"recent_month\"]\n",
    "\n",
    "# # display(recent_month)\n",
    "\n",
    "# df_raw = spark.table(\"dl_tenants_daas.us_wealth_management.wealth_management_client_metrics\") \\\n",
    "#     .filter((F.col(\"branchoffice_code\") == \"83\") & (F.col(\"business_month\") == recent_month))\n",
    "\n",
    "# # # Show schema and first few rows\n",
    "# # df_raw.printSchema()\n",
    "# # display(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afc69e2e-c7a4-4a27-b565-c721d9ff3d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling fraction: 0.2\n",
      "Event rows (approx): 48978063\n",
      "Vocabulary size (classes): 7\n",
      "Users with >= MIN_EVENTS (after dedupe): 388326\n",
      "Total training examples: 580620\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>cont_id</th><th>hist_seq</th><th>label</th></tr></thead><tbody><tr><td>105742452946624441</td><td>List(7)</td><td>3</td></tr><tr><td>113136767872267514</td><td>List(4)</td><td>7</td></tr><tr><td>116436779464619724</td><td>List(4)</td><td>3</td></tr><tr><td>119563957607010504</td><td>List(4)</td><td>5</td></tr><tr><td>142336771569134660</td><td>List(4)</td><td>7</td></tr><tr><td>147836766429708263</td><td>List(4)</td><td>3</td></tr><tr><td>148759886445756804</td><td>List(5)</td><td>4</td></tr><tr><td>150552342005316880</td><td>List(4)</td><td>6</td></tr><tr><td>161651064089692036</td><td>List(7)</td><td>4</td></tr><tr><td>163336760484201626</td><td>List(7)</td><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "105742452946624441",
         [
          7
         ],
         3
        ],
        [
         "113136767872267514",
         [
          4
         ],
         7
        ],
        [
         "116436779464619724",
         [
          4
         ],
         3
        ],
        [
         "119563957607010504",
         [
          4
         ],
         5
        ],
        [
         "142336771569134660",
         [
          4
         ],
         7
        ],
        [
         "147836766429708263",
         [
          4
         ],
         3
        ],
        [
         "148759886445756804",
         [
          5
         ],
         4
        ],
        [
         "150552342005316880",
         [
          4
         ],
         6
        ],
        [
         "161651064089692036",
         [
          7
         ],
         4
        ],
        [
         "163336760484201626",
         [
          7
         ],
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "cont_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "hist_seq",
         "type": "{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "label",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples with features: 580620\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>cont_id</th><th>hist_seq</th><th>label</th><th>seq_len</th><th>last_1</th><th>last_2</th><th>unique_prior</th><th>num_switches</th><th>freq_list</th></tr></thead><tbody><tr><td>105742452946624441</td><td>List(7)</td><td>3</td><td>1</td><td>7</td><td>0</td><td>1</td><td>0</td><td>List(0, 0, 0, 0, 0, 0, 1)</td></tr><tr><td>113136767872267514</td><td>List(4)</td><td>7</td><td>1</td><td>4</td><td>0</td><td>1</td><td>0</td><td>List(0, 0, 0, 1, 0, 0, 0)</td></tr><tr><td>116436779464619724</td><td>List(4)</td><td>3</td><td>1</td><td>4</td><td>0</td><td>1</td><td>0</td><td>List(0, 0, 0, 1, 0, 0, 0)</td></tr><tr><td>119563957607010504</td><td>List(4)</td><td>5</td><td>1</td><td>4</td><td>0</td><td>1</td><td>0</td><td>List(0, 0, 0, 1, 0, 0, 0)</td></tr><tr><td>142336771569134660</td><td>List(4)</td><td>7</td><td>1</td><td>4</td><td>0</td><td>1</td><td>0</td><td>List(0, 0, 0, 1, 0, 0, 0)</td></tr><tr><td>147836766429708263</td><td>List(4)</td><td>3</td><td>1</td><td>4</td><td>0</td><td>1</td><td>0</td><td>List(0, 0, 0, 1, 0, 0, 0)</td></tr><tr><td>148759886445756804</td><td>List(5)</td><td>4</td><td>1</td><td>5</td><td>0</td><td>1</td><td>0</td><td>List(0, 0, 0, 0, 1, 0, 0)</td></tr><tr><td>150552342005316880</td><td>List(4)</td><td>6</td><td>1</td><td>4</td><td>0</td><td>1</td><td>0</td><td>List(0, 0, 0, 1, 0, 0, 0)</td></tr><tr><td>161651064089692036</td><td>List(7)</td><td>4</td><td>1</td><td>7</td><td>0</td><td>1</td><td>0</td><td>List(0, 0, 0, 0, 0, 0, 1)</td></tr><tr><td>163336760484201626</td><td>List(7)</td><td>4</td><td>1</td><td>7</td><td>0</td><td>1</td><td>0</td><td>List(0, 0, 0, 0, 0, 0, 1)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "105742452946624441",
         [
          7
         ],
         3,
         1,
         7,
         0,
         1,
         0,
         [
          0,
          0,
          0,
          0,
          0,
          0,
          1
         ]
        ],
        [
         "113136767872267514",
         [
          4
         ],
         7,
         1,
         4,
         0,
         1,
         0,
         [
          0,
          0,
          0,
          1,
          0,
          0,
          0
         ]
        ],
        [
         "116436779464619724",
         [
          4
         ],
         3,
         1,
         4,
         0,
         1,
         0,
         [
          0,
          0,
          0,
          1,
          0,
          0,
          0
         ]
        ],
        [
         "119563957607010504",
         [
          4
         ],
         5,
         1,
         4,
         0,
         1,
         0,
         [
          0,
          0,
          0,
          1,
          0,
          0,
          0
         ]
        ],
        [
         "142336771569134660",
         [
          4
         ],
         7,
         1,
         4,
         0,
         1,
         0,
         [
          0,
          0,
          0,
          1,
          0,
          0,
          0
         ]
        ],
        [
         "147836766429708263",
         [
          4
         ],
         3,
         1,
         4,
         0,
         1,
         0,
         [
          0,
          0,
          0,
          1,
          0,
          0,
          0
         ]
        ],
        [
         "148759886445756804",
         [
          5
         ],
         4,
         1,
         5,
         0,
         1,
         0,
         [
          0,
          0,
          0,
          0,
          1,
          0,
          0
         ]
        ],
        [
         "150552342005316880",
         [
          4
         ],
         6,
         1,
         4,
         0,
         1,
         0,
         [
          0,
          0,
          0,
          1,
          0,
          0,
          0
         ]
        ],
        [
         "161651064089692036",
         [
          7
         ],
         4,
         1,
         7,
         0,
         1,
         0,
         [
          0,
          0,
          0,
          0,
          0,
          0,
          1
         ]
        ],
        [
         "163336760484201626",
         [
          7
         ],
         4,
         1,
         7,
         0,
         1,
         0,
         [
          0,
          0,
          0,
          0,
          0,
          0,
          1
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "cont_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "hist_seq",
         "type": "{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "label",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "seq_len",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "last_1",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "last_2",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "unique_prior",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "num_switches",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "freq_list",
         "type": "{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples after join: 580620\n",
      "Train / Val / Test counts: 464400 57875 58345\n",
      "Num classes: 7\n",
      "Train shape: (464597, 42) Val shape: (57812, 42) Test shape: (58211, 42)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_raw = df_raw.withColumn(\n",
    "    \"product_category\",\n",
    "    F.when(F.col(\"prod_lob\") == \"LIFE\", \"LIFE_INSURANCE\")\n",
    "    .when(F.col(\"sub_product_level_1\").isin(\"VLI\", \"WL\", \"UL/IUL\", \"TERM\", \"PROTECTIVE PRODUCT\"), \"LIFE_INSURANCE\")\n",
    "    .when(F.col(\"sub_product_level_2\").like(\"%LIFE%\"), \"LIFE_INSURANCE\")\n",
    "    .when(F.col(\"sub_product_level_2\").isin(\n",
    "        \"VARIABLE UNIVERSAL LIFE\", \"WHOLE LIFE\", \"UNIVERSAL LIFE\",\n",
    "        \"INDEX UNIVERSAL LIFE\", \"TERM PRODUCT\", \"VARIABLE LIFE\",\n",
    "        \"SURVIVORSHIP WHOLE LIFE\", \"MONY PROTECTIVE PRODUCT\"\n",
    "    ), \"LIFE_INSURANCE\")\n",
    "    .when(F.col(\"prod_lob\").isin(\"GROUP RETIREMENT\", \"INDIVIDUAL RETIREMENT\"), \"RETIREMENT\")\n",
    "    .when(F.col(\"sub_product_level_1\").isin(\n",
    "        \"EQUIVEST\", \"RETIREMENT 401K\", \"ACCUMULATOR\",\n",
    "        \"RETIREMENT CORNERSTONE\", \"SCS\", \"INVESTMENT EDGE\"\n",
    "    ), \"RETIREMENT\")\n",
    "    .when(\n",
    "        (F.col(\"sub_product_level_2\").like(\"%403B%\")) |\n",
    "        (F.col(\"sub_product_level_2\").like(\"%401%\")) |\n",
    "        (F.col(\"sub_product_level_2\").like(\"%IRA%\")) |\n",
    "        (F.col(\"sub_product_level_2\").like(\"%SEP%\")),\n",
    "        \"RETIREMENT\"\n",
    "    )\n",
    "    .when(F.col(\"prod_lob\") == \"BROKER DEALER\", \"INVESTMENT\")\n",
    "    .when(F.col(\"sub_product_level_1\").isin(\n",
    "        \"INVESTMENT PRODUCT - DIRECT\", \"INVESTMENT PRODUCT - BROKERAGE\",\n",
    "        \"INVESTMENT PRODUCT - ADVISORY\", \"DIRECT\", \"BROKERAGE\",\n",
    "        \"ADVISORY\", \"CASH SOLICITOR\"\n",
    "    ), \"INVESTMENT\")\n",
    "    .when(\n",
    "        (F.col(\"sub_product_level_2\").like(\"%Investment%\")) |\n",
    "        (F.col(\"sub_product_level_2\").like(\"%Brokerage%\")) |\n",
    "        (F.col(\"sub_product_level_2\").like(\"%Advisory%\")),\n",
    "        \"INVESTMENT\"\n",
    "    )\n",
    "    .when(F.col(\"prod_lob\") == \"NETWORK\", \"NETWORK_PRODUCTS\")\n",
    "    .when(\n",
    "        (F.col(\"sub_product_level_1\") == \"NETWORK PRODUCTS\") |\n",
    "        (F.col(\"sub_product_level_2\") == \"NETWORK PRODUCTS\"),\n",
    "        \"NETWORK_PRODUCTS\"\n",
    "    )\n",
    "    .when(\n",
    "        (F.col(\"prod_lob\") == \"OTHERS\") & (F.col(\"sub_product_level_1\") == \"HAS\"),\n",
    "        \"DISABILITY\"\n",
    "    )\n",
    "    .when(F.col(\"sub_product_level_2\") == \"HAS - DISABILITY\", \"DISABILITY\"\n",
    "    )\n",
    "    .when(F.col(\"prod_lob\") == \"OTHERS\", \"HEALTH\")\n",
    "    .when(F.col(\"sub_product_level_2\") == \"GROUP HEALTH PRODUCTS\", \"HEALTH\")\n",
    "    .otherwise(\"OTHER\")\n",
    ")\n",
    "# ------------- PARAMETERS -------------\n",
    "SAMPLE_FRACTION = 0.2   # set None to use full data (be careful with memory)\n",
    "MIN_EVENTS = 2          # minimum number of prior events to produce an example (2 => at least 1 history item + label)\n",
    "MAX_SEQ_LEN = 10        # history length used for padded features\n",
    "TRAIN_FRAC = 0.8\n",
    "VAL_FRAC = 0.1\n",
    "TEST_FRAC = 0.1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# LightGBM training params - OPTIMIZED FOR BETTER F1 SCORES (REVISED)\n",
    "LGB_PARAMS = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 6,               # we'll set dynamically later\n",
    "    \"metric\": \"multi_logloss\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.05,        # Increased back to 0.05 for faster learning\n",
    "    \"num_leaves\": 96,             # Balanced between 64 and 128\n",
    "    \"min_data_in_leaf\": 40,       # Increased from 30 to reduce overfitting\n",
    "    \"feature_fraction\": 0.8,      # Back to 0.8 for better generalization\n",
    "    \"subsample\": 0.8,             # Back to 0.8\n",
    "    \"subsample_freq\": 1,\n",
    "    \"lambda_l1\": 0.05,            # Reduced L1 regularization\n",
    "    \"lambda_l2\": 2.0,             # Back to original L2\n",
    "    \"max_depth\": 10,              # Reduced depth to prevent overfitting\n",
    "    \"min_gain_to_split\": 0.05,    # Reduced threshold to allow more splits\n",
    "    \"verbosity\": -1,\n",
    "    \"force_col_wise\": True,       # Better for wide datasets\n",
    "    \"min_sum_hessian_in_leaf\": 1e-3,  # Additional regularization\n",
    "    \"bagging_freq\": 5,            # Bagging every 5 iterations\n",
    "    \"bagging_fraction\": 0.8       # Bagging fraction\n",
    "}\n",
    "NUM_BOOST_ROUND = 2500            # Balanced number of rounds\n",
    "EARLY_STOP = 75                  # Moderate patience\n",
    "\n",
    "# ------------- IMPORTS -------------\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ------------- 1) Load and optional sample -------------\n",
    "# Assumes df_raw exists (you had populated df_raw earlier). If not, re-load:\n",
    "# df_raw = spark.table(\"dl_tenants_daas.us_wealth_management.wealth_management_client_metrics\")\n",
    "\n",
    "# Keep only rows with product_category already populated\n",
    "df_events = df_raw.select(\"cont_id\", \"product_category\", \"register_date\",\n",
    "                          \"acct_val_amt\",\"face_amt\",\"cash_val_amt\",\"wc_total_assets\",\n",
    "                          \"wc_assetmix_stocks\",\"wc_assetmix_bonds\",\"wc_assetmix_mutual_funds\",\n",
    "                          \"wc_assetmix_annuity\",\"wc_assetmix_deposits\",\"wc_assetmix_other_assets\",\n",
    "                          \"psn_age\",\"client_seg\",\"client_seg_1\",\"aum_band\",\"channel\",\"agent_segment\",\n",
    "                          \"branchoffice_code\",\"policy_status\"\n",
    "                         ).filter(\n",
    "    (F.col(\"cont_id\").isNotNull()) &\n",
    "    (F.col(\"register_date\").isNotNull()) &\n",
    "    (F.col(\"product_category\").isNotNull())\n",
    ")\n",
    "\n",
    "if SAMPLE_FRACTION is not None:\n",
    "    print(\"Sampling fraction:\", SAMPLE_FRACTION)\n",
    "    df_events = df_events.sample(withReplacement=False, fraction=float(SAMPLE_FRACTION), seed=RANDOM_SEED)\n",
    "\n",
    "print(\"Event rows (approx):\", df_events.count())\n",
    "\n",
    "# ------------- 2) Keep only Active policies (optional but recommended) -------------\n",
    "df_events = df_events.filter(F.col(\"policy_status\") == \"Active\")\n",
    "\n",
    "# ------------- 3) convert and order events per user -------------\n",
    "df_events = df_events.withColumn(\"register_ts\", F.to_timestamp(\"register_date\"))\n",
    "w = Window.partitionBy(\"cont_id\").orderBy(\"register_ts\")\n",
    "df_events = df_events.withColumn(\"event_idx\", F.row_number().over(w))\n",
    "\n",
    "# ------------- 4) Build full product vocabulary from the ENTIRE (sampled) df_events -------------\n",
    "prod_list = df_events.select(\"product_category\").distinct().rdd.map(lambda r: r[0]).collect()\n",
    "prod_list = sorted([p for p in prod_list if p is not None])\n",
    "prod2id = {p: i+1 for i, p in enumerate(prod_list)}   # start ids at 1; reserve 0 for padding\n",
    "id2prod = {v:k for k,v in prod2id.items()}\n",
    "NUM_CLASSES = len(prod2id)\n",
    "print(\"Vocabulary size (classes):\", NUM_CLASSES)\n",
    "LGB_PARAMS[\"num_class\"] = NUM_CLASSES + 1   # +1 if you want to reserve 0? We'll keep labels in 1..N\n",
    "\n",
    "# ------------- 5) Build grouped sequences RDD and dedupe consecutive -------------\n",
    "# Create RDD of (cont_id, (event_idx, product_category))\n",
    "rdd = df_events.select(\"cont_id\",\"event_idx\",\"product_category\").rdd.map(lambda r: (r[\"cont_id\"], (int(r[\"event_idx\"]), r[\"product_category\"])))\n",
    "\n",
    "# group by cont_id\n",
    "grouped = rdd.groupByKey().mapValues(lambda evs: [p for _, p in sorted(list(evs), key=lambda x: x[0])])\n",
    "\n",
    "# remove consecutive duplicates (keeps only transitions)\n",
    "def dedupe_consecutive(seq):\n",
    "    if not seq:\n",
    "        return []\n",
    "    out = [seq[0]]\n",
    "    for x in seq[1:]:\n",
    "        if x != out[-1]:\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "grouped = grouped.mapValues(dedupe_consecutive).filter(lambda kv: len(kv[1]) >= MIN_EVENTS)\n",
    "\n",
    "print(\"Users with >= MIN_EVENTS (after dedupe):\", grouped.count())\n",
    "\n",
    "# ------------- 6) Sliding-window training example generation -------------\n",
    "def make_examples(kv):\n",
    "    cont_id, seq = kv\n",
    "    # map to ids, unknown -> 0 (shouldn't happen because vocab built from df_events)\n",
    "    seq_ids = [prod2id.get(x, 0) for x in seq]\n",
    "    n = len(seq_ids)\n",
    "    samples = []\n",
    "    for i in range(1, n):   # i is index of label in seq_ids\n",
    "        history = seq_ids[max(0, i - MAX_SEQ_LEN): i]   # history (most recent up to MAX_SEQ_LEN)\n",
    "        label = seq_ids[i]\n",
    "        if len(history) >= 1:   # history length >= 1\n",
    "            samples.append((cont_id, history, label))\n",
    "    return samples\n",
    "\n",
    "examples_rdd = grouped.flatMap(make_examples)\n",
    "\n",
    "# OPTIONAL: reduce volume by sampling examples (if still huge); comment out if you want full\n",
    "# examples_rdd = examples_rdd.sample(False, 1.0, seed=RANDOM_SEED)\n",
    "\n",
    "# Convert to DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"cont_id\", StringType(), True),\n",
    "    StructField(\"hist_seq\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"label\", IntegerType(), True),\n",
    "])\n",
    "examples_df = spark.createDataFrame(examples_rdd, schema).cache()\n",
    "\n",
    "print(\"Total training examples:\", examples_df.count())\n",
    "display(examples_df.limit(10))\n",
    "\n",
    "# ------------- 7) Create tabular history-derived features (in Spark) -------------\n",
    "# We'll compute: seq_len, last_1, last_2, num_prior, unique_prior, num_switches, freq for each product id (sparse)\n",
    "def history_features_udf(hist):\n",
    "    # build summary stats as dict: we will compute in Python side for quicker dev, but create columns in Spark later\n",
    "    return None\n",
    "\n",
    "# easier: convert RDD -> DataFrame by mapping to tuples of features (done in Python for flexibility)\n",
    "def hist_to_features_row(x):\n",
    "    cont_id, hist, label = x\n",
    "    seq_len = len(hist)\n",
    "    last_1 = hist[-1] if seq_len >= 1 else 0\n",
    "    last_2 = hist[-2] if seq_len >= 2 else 0\n",
    "    last_3 = hist[-3] if seq_len >= 3 else 0  # Added third last\n",
    "    unique_prior = len(set(hist))\n",
    "    num_switches = sum(1 for i in range(1, seq_len) if hist[i] != hist[i-1])\n",
    "    \n",
    "    # Enhanced frequency features\n",
    "    freq = Counter(hist)\n",
    "    freq_features = [freq.get(i, 0) for i in range(1, NUM_CLASSES+1)]  # product ids are 1..NUM_CLASSES\n",
    "    \n",
    "    # Additional engineered features\n",
    "    # 1. Diversity ratio (unique products / total products)\n",
    "    diversity_ratio = unique_prior / seq_len if seq_len > 0 else 0.0\n",
    "    \n",
    "    # 2. Switch ratio (switches / sequence length)\n",
    "    switch_ratio = num_switches / seq_len if seq_len > 0 else 0.0\n",
    "    \n",
    "    # 3. Most frequent product ID\n",
    "    most_freq_product = max(freq.items(), key=lambda x: x[1])[0] if freq else 0\n",
    "    \n",
    "    # 4. Recency-weighted features (recent products weighted more)\n",
    "    recency_weight = float(sum((i+1) * hist[i] for i in range(seq_len))) if seq_len > 0 else 0.0\n",
    "    \n",
    "    # 5. Product transition patterns (last_1 to last_2 transition)\n",
    "    transition_pattern = (last_1 * 100 + last_2) if seq_len >= 2 else 0\n",
    "    \n",
    "    return (str(cont_id), hist, label, seq_len, last_1, last_2, last_3, unique_prior, \n",
    "            num_switches, diversity_ratio, switch_ratio, most_freq_product, recency_weight,\n",
    "            transition_pattern, freq_features)\n",
    "\n",
    "# Because we will convert to Pandas, do a mapPartitions to Python to build features and then to Spark DF\n",
    "sampled = examples_rdd  # rename\n",
    "# WARNING: converting huge RDD to driver is expensive. We'll convert partitions to rows and then to Spark DF\n",
    "rows_rdd = sampled.map(hist_to_features_row)\n",
    "\n",
    "# define schema for features DF\n",
    "from pyspark.sql.types import ArrayType, LongType, DoubleType\n",
    "feat_schema = StructType([\n",
    "    StructField(\"cont_id\", StringType(), True),\n",
    "    StructField(\"hist_seq\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"label\", IntegerType(), True),\n",
    "    StructField(\"seq_len\", IntegerType(), True),\n",
    "    StructField(\"last_1\", IntegerType(), True),\n",
    "    StructField(\"last_2\", IntegerType(), True),\n",
    "    StructField(\"last_3\", IntegerType(), True),  # Added\n",
    "    StructField(\"unique_prior\", IntegerType(), True),\n",
    "    StructField(\"num_switches\", IntegerType(), True),\n",
    "    StructField(\"diversity_ratio\", DoubleType(), True),  # Added\n",
    "    StructField(\"switch_ratio\", DoubleType(), True),    # Added\n",
    "    StructField(\"most_freq_product\", IntegerType(), True),  # Added\n",
    "    StructField(\"recency_weight\", DoubleType(), True),  # Added\n",
    "    StructField(\"transition_pattern\", IntegerType(), True),  # Added\n",
    "    StructField(\"freq_list\", ArrayType(IntegerType()), True),\n",
    "])\n",
    "\n",
    "examples_feats_df = spark.createDataFrame(rows_rdd, feat_schema).cache()\n",
    "print(\"Examples with features:\", examples_feats_df.count())\n",
    "display(examples_feats_df.limit(10))\n",
    "\n",
    "# ------------- 8) Expand freq_list into separate columns (Spark) -------------\n",
    "# create columns freq_1 ... freq_N\n",
    "for i in range(1, NUM_CLASSES+1):\n",
    "    examples_feats_df = examples_feats_df.withColumn(f\"freq_{i}\", F.col(\"freq_list\")[i-1])\n",
    "# drop freq_list\n",
    "examples_feats_df = examples_feats_df.drop(\"freq_list\")\n",
    "\n",
    "# ------------- 9) Build last snapshot static features per user and join -------------\n",
    "# last known static snapshot from df_events (we already had these cols in df_events)\n",
    "w2 = Window.partitionBy(\"cont_id\").orderBy(F.col(\"register_ts\").desc())\n",
    "client_snapshot = (df_events\n",
    "                   .withColumn(\"rn\", F.row_number().over(w2))\n",
    "                   .filter(F.col(\"rn\") == 1)\n",
    "                   .select(\"cont_id\",\n",
    "                           \"acct_val_amt\",\"face_amt\",\"cash_val_amt\",\"wc_total_assets\",\n",
    "                           \"wc_assetmix_stocks\",\"wc_assetmix_bonds\",\"wc_assetmix_mutual_funds\",\n",
    "                           \"wc_assetmix_annuity\",\"wc_assetmix_deposits\",\"wc_assetmix_other_assets\",\n",
    "                           \"psn_age\",\"client_seg\",\"client_seg_1\",\"aum_band\",\"channel\",\"agent_segment\",\"branchoffice_code\")\n",
    "                   # Add engineered features\n",
    "                   .withColumn(\"total_assetmix\", \n",
    "                              F.coalesce(F.col(\"wc_assetmix_stocks\"), F.lit(0)) +\n",
    "                              F.coalesce(F.col(\"wc_assetmix_bonds\"), F.lit(0)) +\n",
    "                              F.coalesce(F.col(\"wc_assetmix_mutual_funds\"), F.lit(0)) +\n",
    "                              F.coalesce(F.col(\"wc_assetmix_annuity\"), F.lit(0)) +\n",
    "                              F.coalesce(F.col(\"wc_assetmix_deposits\"), F.lit(0)) +\n",
    "                              F.coalesce(F.col(\"wc_assetmix_other_assets\"), F.lit(0)))\n",
    "                   .withColumn(\"stocks_ratio\", \n",
    "                              F.when(F.col(\"total_assetmix\") > 0,\n",
    "                                    F.coalesce(F.col(\"wc_assetmix_stocks\"), F.lit(0)) / F.col(\"total_assetmix\"))\n",
    "                              .otherwise(F.lit(0)))\n",
    "                   .withColumn(\"bonds_ratio\",\n",
    "                              F.when(F.col(\"total_assetmix\") > 0,\n",
    "                                    F.coalesce(F.col(\"wc_assetmix_bonds\"), F.lit(0)) / F.col(\"total_assetmix\"))\n",
    "                              .otherwise(F.lit(0)))\n",
    "                   .withColumn(\"annuity_ratio\",\n",
    "                              F.when(F.col(\"total_assetmix\") > 0,\n",
    "                                    F.coalesce(F.col(\"wc_assetmix_annuity\"), F.lit(0)) / F.col(\"total_assetmix\"))\n",
    "                              .otherwise(F.lit(0)))\n",
    "                   .withColumn(\"account_to_face_ratio\",\n",
    "                              F.when(F.col(\"face_amt\") > 0,\n",
    "                                    F.coalesce(F.col(\"acct_val_amt\"), F.lit(0)) / F.col(\"face_amt\"))\n",
    "                              .otherwise(F.lit(0)))\n",
    "                   .withColumn(\"cash_to_account_ratio\",\n",
    "                              F.when(F.col(\"acct_val_amt\") > 0,\n",
    "                                    F.coalesce(F.col(\"cash_val_amt\"), F.lit(0)) / F.col(\"acct_val_amt\"))\n",
    "                              .otherwise(F.lit(0))))\n",
    "\n",
    "# Left join - preserve all examples\n",
    "examples_full = examples_feats_df.join(client_snapshot, on=\"cont_id\", how=\"left\")\n",
    "\n",
    "print(\"Examples after join:\", examples_full.count())\n",
    "\n",
    "# ------------- 10) Fill missing values sensibly -------------\n",
    "# numeric cols to fill 0 (excluding engineered ratio features which should use median)\n",
    "numeric_cols = [c for c, t in examples_full.dtypes if t in (\"int\", \"double\", \"bigint\", \"float\") \n",
    "                and c not in (\"label\",\"seq_len\",\"last_1\",\"last_2\",\"last_3\",\"unique_prior\",\"num_switches\",\n",
    "                             \"diversity_ratio\",\"switch_ratio\",\"stocks_ratio\",\"bonds_ratio\",\"annuity_ratio\",\n",
    "                             \"account_to_face_ratio\",\"cash_to_account_ratio\")]\n",
    "# we will fill numeric nulls with 0\n",
    "fill_dict = {c: 0 for c in numeric_cols}\n",
    "\n",
    "# Fill ratio features with median (more robust than 0)\n",
    "ratio_features = [\"diversity_ratio\",\"switch_ratio\",\"stocks_ratio\",\"bonds_ratio\",\"annuity_ratio\",\n",
    "                  \"account_to_face_ratio\",\"cash_to_account_ratio\"]\n",
    "for rf in ratio_features:\n",
    "    if rf in examples_full.columns:\n",
    "        median_val = examples_full.select(F.percentile_approx(F.col(rf), 0.5).alias(\"median\")).collect()[0][\"median\"]\n",
    "        if median_val is None:\n",
    "            median_val = 0.0\n",
    "        fill_dict[rf] = median_val\n",
    "\n",
    "# categorical modes\n",
    "categorical_cols = [\"client_seg\",\"client_seg_1\",\"aum_band\",\"channel\",\"agent_segment\",\"branchoffice_code\"]\n",
    "modes = {}\n",
    "for c in categorical_cols:\n",
    "    try:\n",
    "        m = examples_full.groupBy(c).count().orderBy(F.desc(\"count\")).first()[0]\n",
    "        modes[c] = m if m is not None else \"UNKNOWN\"\n",
    "    except:\n",
    "        modes[c] = \"UNKNOWN\"\n",
    "\n",
    "examples_full = examples_full.fillna(fill_dict)\n",
    "for c in categorical_cols:\n",
    "    examples_full = examples_full.withColumn(c, F.when(F.col(c).isNull(), F.lit(modes[c])).otherwise(F.col(c)))\n",
    "\n",
    "# ------------- 11) Convert hist_seq to fixed-length padded columns (for LightGBM tabular) -------------\n",
    "def pad_history(hist):\n",
    "    arr = hist[-MAX_SEQ_LEN:] if hist is not None else []\n",
    "    pad_len = MAX_SEQ_LEN - len(arr)\n",
    "    return [0]*pad_len + arr\n",
    "\n",
    "pad_udf = F.udf(lambda x: pad_history(x), ArrayType(IntegerType()))\n",
    "examples_full = examples_full.withColumn(\"hist_padded\", pad_udf(F.col(\"hist_seq\")))\n",
    "\n",
    "# expand hist_padded into hist_0 ... hist_{MAX_SEQ_LEN-1}\n",
    "for i in range(MAX_SEQ_LEN):\n",
    "    examples_full = examples_full.withColumn(f\"hist_{i}\", F.col(\"hist_padded\")[i])\n",
    "\n",
    "examples_full = examples_full.drop(\"hist_seq\", \"hist_padded\")\n",
    "\n",
    "# ------------- 12) Final column list for modeling -------------\n",
    "# label = 'label' (1..N)\n",
    "model_feature_cols = [f\"hist_{i}\" for i in range(MAX_SEQ_LEN)] + \\\n",
    "                     [\"seq_len\",\"last_1\",\"last_2\",\"last_3\",\"unique_prior\",\"num_switches\",\n",
    "                      \"diversity_ratio\",\"switch_ratio\",\"most_freq_product\",\"recency_weight\",\n",
    "                      \"transition_pattern\"] + \\\n",
    "                     [f\"freq_{i}\" for i in range(1, NUM_CLASSES+1)] + \\\n",
    "                     [\"acct_val_amt\",\"face_amt\",\"cash_val_amt\",\"wc_total_assets\",\n",
    "                      \"wc_assetmix_stocks\",\"wc_assetmix_bonds\",\"wc_assetmix_mutual_funds\",\n",
    "                      \"wc_assetmix_annuity\",\"wc_assetmix_deposits\",\"wc_assetmix_other_assets\",\n",
    "                      \"total_assetmix\",\"stocks_ratio\",\"bonds_ratio\",\"annuity_ratio\",\n",
    "                      \"account_to_face_ratio\",\"cash_to_account_ratio\",\n",
    "                      \"psn_age\"]\n",
    "# convert categorical strings to index using simple string->index map (lightgbm accepts categorical as int)\n",
    "# create mapping for categorical_cols\n",
    "for c in categorical_cols:\n",
    "    vals = [r[0] for r in examples_full.select(c).distinct().collect()]\n",
    "    m = {v:i for i,v in enumerate(sorted([str(x) for x in vals]))}\n",
    "    b = spark.sparkContext.broadcast(m)\n",
    "    examples_full = examples_full.withColumn(c + \"_idx\", F.coalesce(F.col(c).cast(\"string\"), F.lit(\"UNKNOWN\")))\n",
    "    # NOTE: converting to integer index using udf\n",
    "    examples_full = examples_full.withColumn(c + \"_idx\", F.udf(lambda s: int(b.value.get(str(s), 0)), IntegerType())(F.col(c + \"_idx\")))\n",
    "    model_feature_cols.append(c + \"_idx\")\n",
    "\n",
    "# Ensure label values are in 1..NUM_CLASSES\n",
    "# If needed remap label ranges (they already are product ids 1..NUM_CLASSES)\n",
    "examples_full = examples_full.withColumn(\"label\", F.col(\"label\").cast(IntegerType()))\n",
    "\n",
    "# ------------- 13) Split train/val/test (random) and convert to Pandas -------------\n",
    "train_spark, val_spark, test_spark = examples_full.randomSplit([TRAIN_FRAC, VAL_FRAC, TEST_FRAC], seed=RANDOM_SEED)\n",
    "\n",
    "print(\"Train / Val / Test counts:\", train_spark.count(), val_spark.count(), test_spark.count())\n",
    "\n",
    "# persist to speed up conversion\n",
    "train_spark = train_spark.cache()\n",
    "val_spark = val_spark.cache()\n",
    "test_spark = test_spark.cache()\n",
    "\n",
    "train_pd = train_spark.select([\"cont_id\",\"label\"] + model_feature_cols).toPandas()\n",
    "val_pd   = val_spark.select([\"cont_id\",\"label\"] + model_feature_cols).toPandas()\n",
    "test_pd  = test_spark.select([\"cont_id\",\"label\"] + model_feature_cols).toPandas()\n",
    "\n",
    "# ------------- 14) Final data sanity & fillna in pandas -------------\n",
    "train_pd.fillna(0, inplace=True)\n",
    "val_pd.fillna(0, inplace=True)\n",
    "test_pd.fillna(0, inplace=True)\n",
    "\n",
    "# Ensure label is zero-based for LightGBM (optional) -- we'll make labels 0..K-1\n",
    "label_map = {lab: i for i, lab in enumerate(sorted(train_pd[\"label\"].unique()))}\n",
    "train_pd[\"label0\"] = train_pd[\"label\"].map(label_map)\n",
    "val_pd[\"label0\"] = val_pd[\"label\"].map(lambda x: label_map.get(x, 0))\n",
    "test_pd[\"label0\"] = test_pd[\"label\"].map(lambda x: label_map.get(x, 0))\n",
    "\n",
    "# Update params num_class to exact count\n",
    "num_classes = len(label_map)\n",
    "LGB_PARAMS[\"num_class\"] = num_classes\n",
    "\n",
    "print(\"Num classes:\", num_classes)\n",
    "print(\"Train shape:\", train_pd.shape, \"Val shape:\", val_pd.shape, \"Test shape:\", test_pd.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4d75e26-6556-46be-8c14-318dcbed4e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[530]\ttrain's multi_logloss: 0.592556\tval's multi_logloss: 0.677966\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fa024c756f4355ad6f62b45b501359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7451684389548368\n",
      "Test F1 weighted: 0.7422177339577981\n",
      "Test F1 macro: 0.5933523901633214\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.57      0.64       360\n",
      "           1       0.50      0.08      0.14        24\n",
      "           2       0.73      0.73      0.73     15453\n",
      "           3       0.77      0.77      0.77     14093\n",
      "           4       0.73      0.68      0.71     11021\n",
      "           5       0.57      0.28      0.38      1313\n",
      "           6       0.75      0.82      0.78     15947\n",
      "\n",
      "    accuracy                           0.75     58211\n",
      "   macro avg       0.68      0.56      0.59     58211\n",
      "weighted avg       0.74      0.75      0.74     58211\n",
      "\n",
      "Confusion matrix shape: (7, 7)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------- 15) Calculate class weights for imbalanced data -------------\n",
    "print(\"=\" * 80)\n",
    "print(\"CALCULATING CLASS WEIGHTS FOR IMBALANCED DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate class weights to handle imbalance - USE MODERATE WEIGHTING\n",
    "# 'balanced' can be too aggressive. We'll use a custom approach that's less extreme\n",
    "unique_labels = sorted(train_pd[\"label0\"].unique())\n",
    "# Convert to numpy array as required by compute_class_weight\n",
    "unique_labels_array = np.array(unique_labels)\n",
    "\n",
    "# Get class frequencies\n",
    "class_counts = train_pd[\"label0\"].value_counts().sort_index()\n",
    "total_samples = len(train_pd)\n",
    "n_classes = len(unique_labels)\n",
    "\n",
    "# Method 1: Use balanced weights but scale them down (less aggressive)\n",
    "balanced_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=unique_labels_array,\n",
    "    y=train_pd[\"label0\"]\n",
    ")\n",
    "\n",
    "# Method 2: Custom weighting - moderate boost for minority classes\n",
    "# Weight = sqrt(balanced_weight) to make it less extreme\n",
    "class_weights = np.sqrt(balanced_weights) * (balanced_weights.mean() / np.sqrt(balanced_weights).mean())\n",
    "\n",
    "# Alternative: Use inverse frequency with smoothing (less extreme than balanced)\n",
    "# class_weights = []\n",
    "# for label in unique_labels:\n",
    "#     freq = class_counts[label] / total_samples\n",
    "#     # Inverse frequency with smoothing factor of 0.1\n",
    "#     weight = 1.0 / (freq + 0.1)\n",
    "#     class_weights.append(weight)\n",
    "# class_weights = np.array(class_weights)\n",
    "# # Normalize so mean weight is 1.0\n",
    "# class_weights = class_weights / class_weights.mean()\n",
    "\n",
    "class_weight_dict = dict(zip(unique_labels, class_weights))\n",
    "\n",
    "print(\"Class distribution in training set:\")\n",
    "class_counts = train_pd[\"label0\"].value_counts().sort_index()\n",
    "for label, count in class_counts.items():\n",
    "    weight = class_weight_dict[label]\n",
    "    freq = count / len(train_pd)\n",
    "    print(f\"  Class {label}: {count:6d} samples ({freq*100:5.2f}%) - weight: {weight:.3f}\")\n",
    "\n",
    "print(f\"\\nWeight statistics:\")\n",
    "print(f\"  Min weight: {min(class_weight_dict.values()):.3f}\")\n",
    "print(f\"  Max weight: {max(class_weight_dict.values()):.3f}\")\n",
    "print(f\"  Mean weight: {np.mean(list(class_weight_dict.values())):.3f}\")\n",
    "print(f\"  Weight range ratio: {max(class_weight_dict.values())/min(class_weight_dict.values()):.2f}x\")\n",
    "\n",
    "# Create sample weights for training\n",
    "train_sample_weights = train_pd[\"label0\"].map(class_weight_dict).values\n",
    "val_sample_weights = val_pd[\"label0\"].map(class_weight_dict).values\n",
    "\n",
    "# ------------- 16) LightGBM training with class weights -------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING LIGHTGBM MODEL WITH CLASS WEIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Optional: Feature selection based on variance and correlation\n",
    "# Remove features with very low variance (likely not informative)\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE SELECTION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check feature variance\n",
    "feature_variance = train_pd[model_feature_cols].var()\n",
    "low_variance_features = feature_variance[feature_variance < 1e-6].index.tolist()\n",
    "\n",
    "if low_variance_features:\n",
    "    print(f\"\\nâš ï¸  Found {len(low_variance_features)} features with very low variance (< 1e-6):\")\n",
    "    for feat in low_variance_features[:10]:  # Show first 10\n",
    "        print(f\"  - {feat}: variance = {feature_variance[feat]:.2e}\")\n",
    "    print(\"\\nðŸ’¡ Consider removing these features as they provide little information\")\n",
    "else:\n",
    "    print(\"\\nâœ… No features with extremely low variance found\")\n",
    "\n",
    "# Use all features for now (can be optimized later)\n",
    "feature_cols_final = model_feature_cols  # order as defined\n",
    "print(f\"\\nðŸ“Š Using {len(feature_cols_final)} features for training\")\n",
    "print(\"=\" * 80)\n",
    "train_ds = lgb.Dataset(\n",
    "    train_pd[feature_cols_final], \n",
    "    label=train_pd[\"label0\"],\n",
    "    weight=train_sample_weights  # Add sample weights\n",
    ")\n",
    "val_ds = lgb.Dataset(\n",
    "    val_pd[feature_cols_final], \n",
    "    label=val_pd[\"label0\"], \n",
    "    reference=train_ds,\n",
    "    weight=val_sample_weights\n",
    ")\n",
    "\n",
    "# Add custom evaluation metrics for F1 score\n",
    "def f1_eval(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Custom F1 evaluation function for LightGBM\n",
    "    \"\"\"\n",
    "    y_true = y_true.get_label()\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    y_pred = y_pred.reshape(num_classes, -1).argmax(axis=0)\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    return 'f1_weighted', f1_weighted, True\n",
    "\n",
    "# Also add macro F1 for monitoring\n",
    "def f1_macro_eval(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Custom F1 macro evaluation function for LightGBM\n",
    "    \"\"\"\n",
    "    y_true = y_true.get_label()\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    y_pred = y_pred.reshape(num_classes, -1).argmax(axis=0)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    return 'f1_macro', f1_macro, True\n",
    "\n",
    "# Train with multiple evaluation metrics\n",
    "model = lgb.train(\n",
    "    LGB_PARAMS,\n",
    "    train_ds,\n",
    "    valid_sets=[train_ds, val_ds],\n",
    "    valid_names=[\"train\",\"val\"],\n",
    "    num_boost_round=NUM_BOOST_ROUND,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(EARLY_STOP, first_metric_only=False),\n",
    "        lgb.log_evaluation(period=50)  # Log every 50 iterations for better monitoring\n",
    "    ],\n",
    "    feval=[f1_eval, f1_macro_eval]  # Monitor both weighted and macro F1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL TRAINING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ------------- 17) Enhanced Evaluation -------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_pred_prob = model.predict(test_pd[feature_cols_final])\n",
    "test_pred = np.argmax(test_pred_prob, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "acc = accuracy_score(test_pd[\"label0\"], test_pred)\n",
    "f1_weighted = f1_score(test_pd[\"label0\"], test_pred, average=\"weighted\")\n",
    "f1_macro = f1_score(test_pd[\"label0\"], test_pred, average=\"macro\")\n",
    "f1_micro = f1_score(test_pd[\"label0\"], test_pred, average=\"micro\")\n",
    "\n",
    "# Per-class F1 scores\n",
    "f1_per_class = f1_score(test_pd[\"label0\"], test_pred, average=None)\n",
    "\n",
    "print(f\"\\nðŸ“Š Overall Metrics:\")\n",
    "print(f\"  Accuracy: {acc:.4f}\")\n",
    "print(f\"  F1 Weighted: {f1_weighted:.4f}\")\n",
    "print(f\"  F1 Macro: {f1_macro:.4f}\")\n",
    "print(f\"  F1 Micro: {f1_micro:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Per-Class F1 Scores:\")\n",
    "for i, f1_val in enumerate(f1_per_class):\n",
    "    class_name = id2prod.get(label_map.get(i, i), f\"Class {i}\")\n",
    "    print(f\"  {class_name}: {f1_val:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Detailed Classification Report:\")\n",
    "print(classification_report(test_pd[\"label0\"], test_pred, \n",
    "                          target_names=[id2prod.get(label_map.get(i, i), f\"Class {i}\") \n",
    "                                       for i in range(num_classes)]))\n",
    "\n",
    "cm = confusion_matrix(test_pd[\"label0\"], test_pred)\n",
    "print(f\"\\nðŸ“Š Confusion Matrix Shape: {cm.shape}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate additional metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(\n",
    "    test_pd[\"label0\"], test_pred, average=None\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-CLASS METRICS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': [id2prod.get(label_map.get(i, i), f\"Class {i}\") for i in range(num_classes)],\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': fscore,\n",
    "    'Support': support\n",
    "})\n",
    "display(metrics_df)\n",
    "\n",
    "print(\"\\nâœ… Evaluation Complete!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# HYPERPARAMETER TUNING SUGGESTIONS (OPTIONAL)\n",
    "# ============================================\n",
    "# If F1 is still not improving, try these approaches:\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"HYPERPARAMETER TUNING RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\"\"\n",
    "If F1 scores are still not optimal, consider:\n",
    "\n",
    "1. **Grid Search for Learning Rate and Num Leaves:**\n",
    "   - Try learning_rate: [0.03, 0.05, 0.07]\n",
    "   - Try num_leaves: [64, 96, 128]\n",
    "   - Use cross-validation to find best combination\n",
    "\n",
    "2. **Feature Selection:**\n",
    "   - Remove low-importance features (bottom 10-20%)\n",
    "   - Focus on top features that contribute most\n",
    "\n",
    "3. **Ensemble Methods:**\n",
    "   - Train multiple models with different random seeds\n",
    "   - Average predictions (soft voting)\n",
    "\n",
    "4. **Alternative Class Weight Strategies:**\n",
    "   - Try 'balanced_subsample' instead of 'balanced'\n",
    "   - Use custom weights based on business importance\n",
    "   - Consider focal loss approach\n",
    "\n",
    "5. **Data Augmentation:**\n",
    "   - SMOTE for minority classes (if applicable)\n",
    "   - Synthetic data generation for rare classes\n",
    "\n",
    "6. **Model Architecture:**\n",
    "   - Try CatBoost as alternative (often better for categorical features)\n",
    "   - Consider XGBoost with different parameters\n",
    "   - Try neural networks if you have enough data\n",
    "\n",
    "7. **Feature Engineering:**\n",
    "   - Remove noisy features (diversity_ratio, switch_ratio if not helping)\n",
    "   - Add interaction features\n",
    "   - Try polynomial features for important variables\n",
    "\n",
    "Current model should perform better with the revised hyperparameters.\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FEATURE IMPORTANCE ANALYSIS (AFTER TRAINING)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get feature importance from the model\n",
    "feature_importance = model.feature_importance(importance_type='gain')\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols_final,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nðŸ“Š Top 20 Most Important Features:\")\n",
    "display(feature_importance_df.head(20))\n",
    "\n",
    "# Plot feature importance\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance_df.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance'].values)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
    "plt.xlabel('Importance (Gain)')\n",
    "plt.title('Top 20 Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Feature importance analysis complete\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d08ef2-08b2-46e2-888d-34fac23a7ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d0e4f2d73347b9ab2416f50f6f32b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'eda_smartlist.models.final_lgbm_multiclass_model'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8216d378c84d66ae1f84ccdb30b799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'eda_smartlist.models.final_lgbm_multiclass_model'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Use train_pd and model from previous cells\n",
    "input_example = train_pd[feature_cols_final].iloc[:5]\n",
    "output_example = model.predict(input_example)\n",
    "signature = infer_signature(input_example, output_example)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.lightgbm.log_model(\n",
    "        model,\n",
    "        artifact_path=\"final_lgbm_multiclass_model\",\n",
    "        registered_model_name=\"eda_smartlist.models.final_lgbm_multiclass_model\",\n",
    "        signature=signature,\n",
    "        input_example=input_example\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "60f9df38-934c-4c2a-98bf-fc2af0a0d9b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# register and create an alias\n",
    "\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "model_name = \"eda_smartlist.models.final_lgbm_multiclass_model\"\n",
    "model_version = \"1\"  \n",
    "alias = \"lgbm_74\"\n",
    "\n",
    "client.set_registered_model_alias(\n",
    "    name=model_name,\n",
    "    alias=alias,\n",
    "    version=model_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1827720-3efc-4e58-81a2-7e3a8311722c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most recent business_month: 202511\n",
      "Rows for prediction (branchoffice_code='83', business_month=202511): 300955\n",
      "Historical events for prediction clients: 15058055\n",
      "Users with sequences for prediction: 245728\n",
      "Prediction examples: 245728\n",
      "Prediction examples after join: 245728\n",
      "Prediction data shape: (245728, 41)\n",
      "\n",
      "Final predictions shape: (245728, 23)\n",
      "\n",
      "Final predictions sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>cont_id</th><th>axa_party_id</th><th>product_category</th><th>psn_age</th><th>client_seg</th><th>client_seg_1</th><th>aum_band</th><th>channel</th><th>division_name</th><th>branch_name</th><th>business_city</th><th>business_state_cod</th><th>client_tenure</th><th>pred_class_id</th><th>pred_product</th><th>pred_prob</th><th>prob_0</th><th>prob_1</th><th>prob_2</th><th>prob_3</th><th>prob_4</th><th>prob_5</th><th>prob_6</th></tr></thead><tbody><tr><td>100236768405239730</td><td>00BK05RY3MBBYZETXXXX</td><td>RETIREMENT</td><td>67.0</td><td>1-2.99m</td><td>Next Chapters</td><td><$25K</td><td>Branch Assist</td><td>Division 8</td><td>PCPG</td><td>SYRACUSE</td><td>NY</td><td>35.47</td><td>3</td><td>LIFE_INSURANCE</td><td>0.5938047262505148</td><td>0.0012894623356847466</td><td>1.6900128581065633E-6</td><td>0.304129020734926</td><td>0.5938047262505148</td><td>0.05471542317824895</td><td>0.046018349081623194</td><td>4.132840614402719E-5</td></tr><tr><td>100236775740980032</td><td>94YK0DQHXLXIDUTAXXXX</td><td>LIFE_INSURANCE</td><td>60.0</td><td><200k</td><td>Strategic Spenders</td><td>$50-$100K</td><td>Branch Assist</td><td>Division 8</td><td>PCPG</td><td>SYRACUSE</td><td>NY</td><td>10.58</td><td>5</td><td>OTHER</td><td>0.3959357872039781</td><td>0.017404929202043926</td><td>3.096362893112268E-5</td><td>0.212794693854749</td><td>0.004068445586196477</td><td>0.11514574458162888</td><td>0.3959357872039781</td><td>0.25461943594247255</td></tr><tr><td>100336777803286137</td><td>03BK05RY51WW64PHXXXX</td><td>LIFE_INSURANCE</td><td>73.0</td><td><200k</td><td>Strategic Spenders</td><td><$25K</td><td>Branch Assist</td><td>Division 8</td><td>PCPG</td><td>SYRACUSE</td><td>NY</td><td>24.43</td><td>4</td><td>NETWORK_PRODUCTS</td><td>0.898848040500579</td><td>0.0017949733570290158</td><td>1.816864919724236E-6</td><td>0.04189879744226347</td><td>0.0020453718716081848</td><td>0.898848040500579</td><td>0.004626477713615976</td><td>0.05078452224998479</td></tr><tr><td>100367602181183095</td><td>96YK0DQHXI9402H9XXXX</td><td>LIFE_INSURANCE</td><td>71.0</td><td>1-2.99m</td><td>Next Chapters</td><td><$25K</td><td>Branch Assist</td><td>Division 8</td><td>PCPG</td><td>SYRACUSE</td><td>NY</td><td>22.35</td><td>6</td><td>RETIREMENT</td><td>0.46700793050411765</td><td>0.0037475398256119606</td><td>6.702267388993642E-6</td><td>0.16738946721691433</td><td>0.0055117268913735095</td><td>0.292494741219604</td><td>0.06384189207498951</td><td>0.46700793050411765</td></tr><tr><td>100367742412104944</td><td>31YK0DQHXNK0ROV0XXXX</td><td>LIFE_INSURANCE</td><td>74.0</td><td>1-2.99m</td><td>Next Chapters</td><td><$25K</td><td>Branch Assist</td><td>Division 8</td><td>PCPG</td><td>SYRACUSE</td><td>NY</td><td>64.22</td><td>6</td><td>RETIREMENT</td><td>0.8291345205905127</td><td>9.547981798185405E-4</td><td>4.264798620555953E-6</td><td>0.08004212092208692</td><td>0.002249119795801198</td><td>0.06982843240524862</td><td>0.017786743307911478</td><td>0.8291345205905127</td></tr><tr><td>100436767891559904</td><td>55BK0RJOHQPUQDW8XXXX</td><td>LIFE_INSURANCE</td><td>46.0</td><td>1-2.99m</td><td>Wealth Builders</td><td><$25K</td><td>Branch Assist</td><td>Division 8</td><td>PCPG</td><td>SYRACUSE</td><td>NY</td><td>15.41</td><td>5</td><td>OTHER</td><td>0.44189686614316537</td><td>4.923944050793394E-4</td><td>1.3373821540548045E-4</td><td>0.19472759071849458</td><td>0.0029328647034959922</td><td>0.0976988924076201</td><td>0.44189686614316537</td><td>0.2621176534067392</td></tr><tr><td>100436778268667989</td><td>66BK05RY2G9MGHQYXXXX</td><td>LIFE_INSURANCE</td><td>88.0</td><td>3-4.99m</td><td>Life Legacies</td><td><$25K</td><td>Branch Assist</td><td>Division 8</td><td>PCPG</td><td>SYRACUSE</td><td>NY</td><td>67.45</td><td>6</td><td>RETIREMENT</td><td>0.8748926140368007</td><td>6.886450599959803E-4</td><td>4.457037343788962E-6</td><td>0.05420707579492654</td><td>0.0013237483779768844</td><td>0.06766500893741763</td><td>0.0012184507555384047</td><td>0.8748926140368007</td></tr><tr><td>100536768419521458</td><td>64BK05RY3MX4DAPEXXXX</td><td>RETIREMENT</td><td>59.0</td><td><200k</td><td>Strategic Spenders</td><td><$25K</td><td>Branch Assist</td><td>Division 8</td><td>PCPG</td><td>SYRACUSE</td><td>NY</td><td>32.93</td><td>3</td><td>LIFE_INSURANCE</td><td>0.37199248064986423</td><td>6.067432735407653E-4</td><td>1.2397286382229538E-6</td><td>0.31438021989287906</td><td>0.37199248064986423</td><td>0.04018421636458055</td><td>0.2727885497353888</td><td>4.6550355108367394E-5</td></tr><tr><td>100636768966578192</td><td>83BK05RY3MX5OGVGXXXX</td><td>RETIREMENT</td><td>73.0</td><td>200-399k</td><td>Strategic Spenders</td><td>$50-$100K</td><td>Branch Assist</td><td>Division 8</td><td>PCPG</td><td>SYRACUSE</td><td>NY</td><td>10.56</td><td>3</td><td>LIFE_INSURANCE</td><td>0.7336613204298862</td><td>8.442334025892909E-5</td><td>1.0360020716577716E-6</td><td>0.11884209566982998</td><td>0.7336613204298862</td><td>0.09296796855916155</td><td>0.054419793482903205</td><td>2.3362515888471893E-5</td></tr><tr><td>100658269393234144</td><td>20208ac72304946245c7</td><td>LIFE_INSURANCE</td><td>75.0</td><td>1-2.99m</td><td>Next Chapters</td><td><$25K</td><td>Branch Assist</td><td>Division 8</td><td>PCPG</td><td>SYRACUSE</td><td>NY</td><td>51.71</td><td>6</td><td>RETIREMENT</td><td>0.6369366973193312</td><td>0.0036561512962371523</td><td>2.4741755891599476E-6</td><td>0.1308979938624453</td><td>0.0028386100434686272</td><td>0.1424697282705258</td><td>0.08319834503240277</td><td>0.6369366973193312</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "100236768405239730",
         "00BK05RY3MBBYZETXXXX",
         "RETIREMENT",
         67,
         "1-2.99m",
         "Next Chapters",
         "<$25K",
         "Branch Assist",
         "Division 8",
         "PCPG",
         "SYRACUSE",
         "NY",
         35.47,
         3,
         "LIFE_INSURANCE",
         0.5938047262505148,
         0.0012894623356847466,
         0.0000016900128581065633,
         0.304129020734926,
         0.5938047262505148,
         0.05471542317824895,
         0.046018349081623194,
         0.00004132840614402719
        ],
        [
         "100236775740980032",
         "94YK0DQHXLXIDUTAXXXX",
         "LIFE_INSURANCE",
         60,
         "<200k",
         "Strategic Spenders",
         "$50-$100K",
         "Branch Assist",
         "Division 8",
         "PCPG",
         "SYRACUSE",
         "NY",
         10.58,
         5,
         "OTHER",
         0.3959357872039781,
         0.017404929202043926,
         0.00003096362893112268,
         0.212794693854749,
         0.004068445586196477,
         0.11514574458162888,
         0.3959357872039781,
         0.25461943594247255
        ],
        [
         "100336777803286137",
         "03BK05RY51WW64PHXXXX",
         "LIFE_INSURANCE",
         73,
         "<200k",
         "Strategic Spenders",
         "<$25K",
         "Branch Assist",
         "Division 8",
         "PCPG",
         "SYRACUSE",
         "NY",
         24.43,
         4,
         "NETWORK_PRODUCTS",
         0.898848040500579,
         0.0017949733570290158,
         0.000001816864919724236,
         0.04189879744226347,
         0.0020453718716081848,
         0.898848040500579,
         0.004626477713615976,
         0.05078452224998479
        ],
        [
         "100367602181183095",
         "96YK0DQHXI9402H9XXXX",
         "LIFE_INSURANCE",
         71,
         "1-2.99m",
         "Next Chapters",
         "<$25K",
         "Branch Assist",
         "Division 8",
         "PCPG",
         "SYRACUSE",
         "NY",
         22.35,
         6,
         "RETIREMENT",
         0.46700793050411765,
         0.0037475398256119606,
         0.000006702267388993642,
         0.16738946721691433,
         0.0055117268913735095,
         0.292494741219604,
         0.06384189207498951,
         0.46700793050411765
        ],
        [
         "100367742412104944",
         "31YK0DQHXNK0ROV0XXXX",
         "LIFE_INSURANCE",
         74,
         "1-2.99m",
         "Next Chapters",
         "<$25K",
         "Branch Assist",
         "Division 8",
         "PCPG",
         "SYRACUSE",
         "NY",
         64.22,
         6,
         "RETIREMENT",
         0.8291345205905127,
         0.0009547981798185405,
         0.000004264798620555953,
         0.08004212092208692,
         0.002249119795801198,
         0.06982843240524862,
         0.017786743307911478,
         0.8291345205905127
        ],
        [
         "100436767891559904",
         "55BK0RJOHQPUQDW8XXXX",
         "LIFE_INSURANCE",
         46,
         "1-2.99m",
         "Wealth Builders",
         "<$25K",
         "Branch Assist",
         "Division 8",
         "PCPG",
         "SYRACUSE",
         "NY",
         15.41,
         5,
         "OTHER",
         0.44189686614316537,
         0.0004923944050793394,
         0.00013373821540548045,
         0.19472759071849458,
         0.0029328647034959922,
         0.0976988924076201,
         0.44189686614316537,
         0.2621176534067392
        ],
        [
         "100436778268667989",
         "66BK05RY2G9MGHQYXXXX",
         "LIFE_INSURANCE",
         88,
         "3-4.99m",
         "Life Legacies",
         "<$25K",
         "Branch Assist",
         "Division 8",
         "PCPG",
         "SYRACUSE",
         "NY",
         67.45,
         6,
         "RETIREMENT",
         0.8748926140368007,
         0.0006886450599959803,
         0.000004457037343788962,
         0.05420707579492654,
         0.0013237483779768844,
         0.06766500893741763,
         0.0012184507555384047,
         0.8748926140368007
        ],
        [
         "100536768419521458",
         "64BK05RY3MX4DAPEXXXX",
         "RETIREMENT",
         59,
         "<200k",
         "Strategic Spenders",
         "<$25K",
         "Branch Assist",
         "Division 8",
         "PCPG",
         "SYRACUSE",
         "NY",
         32.93,
         3,
         "LIFE_INSURANCE",
         0.37199248064986423,
         0.0006067432735407653,
         0.0000012397286382229538,
         0.31438021989287906,
         0.37199248064986423,
         0.04018421636458055,
         0.2727885497353888,
         0.000046550355108367394
        ],
        [
         "100636768966578192",
         "83BK05RY3MX5OGVGXXXX",
         "RETIREMENT",
         73,
         "200-399k",
         "Strategic Spenders",
         "$50-$100K",
         "Branch Assist",
         "Division 8",
         "PCPG",
         "SYRACUSE",
         "NY",
         10.56,
         3,
         "LIFE_INSURANCE",
         0.7336613204298862,
         0.00008442334025892909,
         0.0000010360020716577716,
         0.11884209566982998,
         0.7336613204298862,
         0.09296796855916155,
         0.054419793482903205,
         0.000023362515888471893
        ],
        [
         "100658269393234144",
         "20208ac72304946245c7",
         "LIFE_INSURANCE",
         75,
         "1-2.99m",
         "Next Chapters",
         "<$25K",
         "Branch Assist",
         "Division 8",
         "PCPG",
         "SYRACUSE",
         "NY",
         51.71,
         6,
         "RETIREMENT",
         0.6369366973193312,
         0.0036561512962371523,
         0.0000024741755891599476,
         0.1308979938624453,
         0.0028386100434686272,
         0.1424697282705258,
         0.08319834503240277,
         0.6369366973193312
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "cont_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "axa_party_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "psn_age",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "client_seg",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "client_seg_1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "aum_band",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "channel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "division_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "branch_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "business_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "business_state_cod",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "client_tenure",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "pred_class_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "pred_product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "pred_prob",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "prob_0",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "prob_1",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "prob_2",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "prob_3",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "prob_4",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "prob_5",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "prob_6",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction summary:\n",
      "RETIREMENT          130923\n",
      "LIFE_INSURANCE       57470\n",
      "OTHER                29847\n",
      "NETWORK_PRODUCTS     20517\n",
      "INVESTMENT            6011\n",
      "DISABILITY             945\n",
      "HEALTH                  15\n",
      "Name: pred_product, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# PREDICTIONS FOR BRANCH OFFICE 83 - MOST RECENT BUSINESS_MONTH\n",
    "# ============================================\n",
    "\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# ------------- 1) Load filtered data: branchoffice_code='83' and most recent business_month -------------\n",
    "df_pred_raw = spark.table(\"dl_tenants_daas.us_wealth_management.wealth_management_client_metrics\")\n",
    "\n",
    "# Apply product_category transformation (same as training)\n",
    "df_pred_raw = df_pred_raw.withColumn(\n",
    "    \"product_category\",\n",
    "    F.when(F.col(\"prod_lob\") == \"LIFE\", \"LIFE_INSURANCE\")\n",
    "    .when(F.col(\"sub_product_level_1\").isin(\"VLI\", \"WL\", \"UL/IUL\", \"TERM\", \"PROTECTIVE PRODUCT\"), \"LIFE_INSURANCE\")\n",
    "    .when(F.col(\"sub_product_level_2\").like(\"%LIFE%\"), \"LIFE_INSURANCE\")\n",
    "    .when(F.col(\"sub_product_level_2\").isin(\n",
    "        \"VARIABLE UNIVERSAL LIFE\", \"WHOLE LIFE\", \"UNIVERSAL LIFE\",\n",
    "        \"INDEX UNIVERSAL LIFE\", \"TERM PRODUCT\", \"VARIABLE LIFE\",\n",
    "        \"SURVIVORSHIP WHOLE LIFE\", \"MONY PROTECTIVE PRODUCT\"\n",
    "    ), \"LIFE_INSURANCE\")\n",
    "    .when(F.col(\"prod_lob\").isin(\"GROUP RETIREMENT\", \"INDIVIDUAL RETIREMENT\"), \"RETIREMENT\")\n",
    "    .when(F.col(\"sub_product_level_1\").isin(\n",
    "        \"EQUIVEST\", \"RETIREMENT 401K\", \"ACCUMULATOR\",\n",
    "        \"RETIREMENT CORNERSTONE\", \"SCS\", \"INVESTMENT EDGE\"\n",
    "    ), \"RETIREMENT\")\n",
    "    .when(\n",
    "        (F.col(\"sub_product_level_2\").like(\"%403B%\")) |\n",
    "        (F.col(\"sub_product_level_2\").like(\"%401%\")) |\n",
    "        (F.col(\"sub_product_level_2\").like(\"%IRA%\")) |\n",
    "        (F.col(\"sub_product_level_2\").like(\"%SEP%\")),\n",
    "        \"RETIREMENT\"\n",
    "    )\n",
    "    .when(F.col(\"prod_lob\") == \"BROKER DEALER\", \"INVESTMENT\")\n",
    "    .when(F.col(\"sub_product_level_1\").isin(\n",
    "        \"INVESTMENT PRODUCT - DIRECT\", \"INVESTMENT PRODUCT - BROKERAGE\",\n",
    "        \"INVESTMENT PRODUCT - ADVISORY\", \"DIRECT\", \"BROKERAGE\",\n",
    "        \"ADVISORY\", \"CASH SOLICITOR\"\n",
    "    ), \"INVESTMENT\")\n",
    "    .when(\n",
    "        (F.col(\"sub_product_level_2\").like(\"%Investment%\")) |\n",
    "        (F.col(\"sub_product_level_2\").like(\"%Brokerage%\")) |\n",
    "        (F.col(\"sub_product_level_2\").like(\"%Advisory%\")),\n",
    "        \"INVESTMENT\"\n",
    "    )\n",
    "    .when(F.col(\"prod_lob\") == \"NETWORK\", \"NETWORK_PRODUCTS\")\n",
    "    .when(\n",
    "        (F.col(\"sub_product_level_1\") == \"NETWORK PRODUCTS\") |\n",
    "        (F.col(\"sub_product_level_2\") == \"NETWORK PRODUCTS\"),\n",
    "        \"NETWORK_PRODUCTS\"\n",
    "    )\n",
    "    .when(\n",
    "        (F.col(\"prod_lob\") == \"OTHERS\") & (F.col(\"sub_product_level_1\") == \"HAS\"),\n",
    "        \"DISABILITY\"\n",
    "    )\n",
    "    .when(F.col(\"sub_product_level_2\") == \"HAS - DISABILITY\", \"DISABILITY\")\n",
    "    .when(F.col(\"prod_lob\") == \"OTHERS\", \"HEALTH\")\n",
    "    .when(F.col(\"sub_product_level_2\") == \"GROUP HEALTH PRODUCTS\", \"HEALTH\")\n",
    "    .otherwise(\"OTHER\")\n",
    ")\n",
    "\n",
    "# Filter for branchoffice_code = '83'\n",
    "df_pred_raw = df_pred_raw.filter(F.col(\"branchoffice_code\") == \"83\")\n",
    "\n",
    "# Get most recent business_month\n",
    "max_business_month = df_pred_raw.select(F.max(\"business_month\").alias(\"max_month\")).collect()[0][\"max_month\"]\n",
    "print(f\"Most recent business_month: {max_business_month}\")\n",
    "\n",
    "# Filter for most recent business_month\n",
    "df_pred_raw = df_pred_raw.filter(F.col(\"business_month\") == max_business_month)\n",
    "\n",
    "print(f\"Rows for prediction (branchoffice_code='83', business_month={max_business_month}): {df_pred_raw.count()}\")\n",
    "\n",
    "# ------------- 2) Prepare events data (similar to training) -------------\n",
    "df_pred_events = df_pred_raw.select(\n",
    "    \"cont_id\", \"product_category\", \"register_date\",\n",
    "    \"acct_val_amt\",\"face_amt\",\"cash_val_amt\",\"wc_total_assets\",\n",
    "    \"wc_assetmix_stocks\",\"wc_assetmix_bonds\",\"wc_assetmix_mutual_funds\",\n",
    "    \"wc_assetmix_annuity\",\"wc_assetmix_deposits\",\"wc_assetmix_other_assets\",\n",
    "    \"psn_age\",\"client_seg\",\"client_seg_1\",\"aum_band\",\"channel\",\"agent_segment\",\n",
    "    \"branchoffice_code\",\"policy_status\", \"business_month\"\n",
    ").filter(\n",
    "    (F.col(\"cont_id\").isNotNull()) &\n",
    "    (F.col(\"register_date\").isNotNull()) &\n",
    "    (F.col(\"product_category\").isNotNull())\n",
    ")\n",
    "\n",
    "# Keep only Active policies\n",
    "df_pred_events = df_pred_events.filter(F.col(\"policy_status\") == \"Active\")\n",
    "\n",
    "# Order events per user\n",
    "df_pred_events = df_pred_events.withColumn(\"register_ts\", F.to_timestamp(\"register_date\"))\n",
    "w_pred = Window.partitionBy(\"cont_id\").orderBy(\"register_ts\")\n",
    "df_pred_events = df_pred_events.withColumn(\"event_idx\", F.row_number().over(w_pred))\n",
    "\n",
    "# ------------- 3) Build sequences for prediction (need full history up to prediction point) -------------\n",
    "# For prediction, we need to get the full history for each client up to the most recent business_month\n",
    "# We'll use ALL historical events (not just from the filtered month) to build sequences\n",
    "\n",
    "# Get all historical events for clients in branchoffice_code='83' (for sequence building)\n",
    "df_all_hist = spark.table(\"dl_tenants_daas.us_wealth_management.wealth_management_client_metrics\")\n",
    "df_all_hist = df_all_hist.withColumn(\n",
    "    \"product_category\",\n",
    "    F.when(F.col(\"prod_lob\") == \"LIFE\", \"LIFE_INSURANCE\")\n",
    "    .when(F.col(\"sub_product_level_1\").isin(\"VLI\", \"WL\", \"UL/IUL\", \"TERM\", \"PROTECTIVE PRODUCT\"), \"LIFE_INSURANCE\")\n",
    "    .when(F.col(\"sub_product_level_2\").like(\"%LIFE%\"), \"LIFE_INSURANCE\")\n",
    "    .when(F.col(\"sub_product_level_2\").isin(\n",
    "        \"VARIABLE UNIVERSAL LIFE\", \"WHOLE LIFE\", \"UNIVERSAL LIFE\",\n",
    "        \"INDEX UNIVERSAL LIFE\", \"TERM PRODUCT\", \"VARIABLE LIFE\",\n",
    "        \"SURVIVORSHIP WHOLE LIFE\", \"MONY PROTECTIVE PRODUCT\"\n",
    "    ), \"LIFE_INSURANCE\")\n",
    "    .when(F.col(\"prod_lob\").isin(\"GROUP RETIREMENT\", \"INDIVIDUAL RETIREMENT\"), \"RETIREMENT\")\n",
    "    .when(F.col(\"sub_product_level_1\").isin(\n",
    "        \"EQUIVEST\", \"RETIREMENT 401K\", \"ACCUMULATOR\",\n",
    "        \"RETIREMENT CORNERSTONE\", \"SCS\", \"INVESTMENT EDGE\"\n",
    "    ), \"RETIREMENT\")\n",
    "    .when(\n",
    "        (F.col(\"sub_product_level_2\").like(\"%403B%\")) |\n",
    "        (F.col(\"sub_product_level_2\").like(\"%401%\")) |\n",
    "        (F.col(\"sub_product_level_2\").like(\"%IRA%\")) |\n",
    "        (F.col(\"sub_product_level_2\").like(\"%SEP%\")),\n",
    "        \"RETIREMENT\"\n",
    "    )\n",
    "    .when(F.col(\"prod_lob\") == \"BROKER DEALER\", \"INVESTMENT\")\n",
    "    .when(F.col(\"sub_product_level_1\").isin(\n",
    "        \"INVESTMENT PRODUCT - DIRECT\", \"INVESTMENT PRODUCT - BROKERAGE\",\n",
    "        \"INVESTMENT PRODUCT - ADVISORY\", \"DIRECT\", \"BROKERAGE\",\n",
    "        \"ADVISORY\", \"CASH SOLICITOR\"\n",
    "    ), \"INVESTMENT\")\n",
    "    .when(\n",
    "        (F.col(\"sub_product_level_2\").like(\"%Investment%\")) |\n",
    "        (F.col(\"sub_product_level_2\").like(\"%Brokerage%\")) |\n",
    "        (F.col(\"sub_product_level_2\").like(\"%Advisory%\")),\n",
    "        \"INVESTMENT\"\n",
    "    )\n",
    "    .when(F.col(\"prod_lob\") == \"NETWORK\", \"NETWORK_PRODUCTS\")\n",
    "    .when(\n",
    "        (F.col(\"sub_product_level_1\") == \"NETWORK PRODUCTS\") |\n",
    "        (F.col(\"sub_product_level_2\") == \"NETWORK PRODUCTS\"),\n",
    "        \"NETWORK_PRODUCTS\"\n",
    "    )\n",
    "    .when(\n",
    "        (F.col(\"prod_lob\") == \"OTHERS\") & (F.col(\"sub_product_level_1\") == \"HAS\"),\n",
    "        \"DISABILITY\"\n",
    "    )\n",
    "    .when(F.col(\"sub_product_level_2\") == \"HAS - DISABILITY\", \"DISABILITY\")\n",
    "    .when(F.col(\"prod_lob\") == \"OTHERS\", \"HEALTH\")\n",
    "    .when(F.col(\"sub_product_level_2\") == \"GROUP HEALTH PRODUCTS\", \"HEALTH\")\n",
    "    .otherwise(\"OTHER\")\n",
    ")\n",
    "\n",
    "# Get list of cont_ids we need to predict for\n",
    "cont_ids_to_predict = [r[\"cont_id\"] for r in df_pred_events.select(\"cont_id\").distinct().collect()]\n",
    "\n",
    "# Filter historical data for these clients and events up to (and including) the prediction month\n",
    "df_all_hist = df_all_hist.filter(\n",
    "    (F.col(\"cont_id\").isin(cont_ids_to_predict)) &\n",
    "    (F.col(\"branchoffice_code\") == \"83\") &\n",
    "    (F.col(\"cont_id\").isNotNull()) &\n",
    "    (F.col(\"register_date\").isNotNull()) &\n",
    "    (F.col(\"product_category\").isNotNull()) &\n",
    "    (F.col(\"policy_status\") == \"Active\")\n",
    ")\n",
    "\n",
    "# Filter events up to and including the prediction business_month\n",
    "df_all_hist = df_all_hist.filter(F.col(\"business_month\") <= max_business_month)\n",
    "\n",
    "# Order events per user\n",
    "df_all_hist = df_all_hist.withColumn(\"register_ts\", F.to_timestamp(\"register_date\"))\n",
    "w_hist = Window.partitionBy(\"cont_id\").orderBy(\"register_ts\")\n",
    "df_all_hist = df_all_hist.withColumn(\"event_idx\", F.row_number().over(w_hist))\n",
    "\n",
    "print(f\"Historical events for prediction clients: {df_all_hist.count()}\")\n",
    "\n",
    "# ------------- 4) Build sequences for prediction (one prediction per client) -------------\n",
    "# Create RDD of (cont_id, (event_idx, product_category))\n",
    "rdd_pred = df_all_hist.select(\"cont_id\",\"event_idx\",\"product_category\").rdd.map(\n",
    "    lambda r: (r[\"cont_id\"], (int(r[\"event_idx\"]), r[\"product_category\"]))\n",
    ")\n",
    "\n",
    "# Group by cont_id\n",
    "grouped_pred = rdd_pred.groupByKey().mapValues(\n",
    "    lambda evs: [p for _, p in sorted(list(evs), key=lambda x: x[0])]\n",
    ")\n",
    "\n",
    "# Remove consecutive duplicates\n",
    "def dedupe_consecutive(seq):\n",
    "    if not seq:\n",
    "        return []\n",
    "    out = [seq[0]]\n",
    "    for x in seq[1:]:\n",
    "        if x != out[-1]:\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "grouped_pred = grouped_pred.mapValues(dedupe_consecutive).filter(lambda kv: len(kv[1]) >= 1)  # At least 1 event\n",
    "\n",
    "print(f\"Users with sequences for prediction: {grouped_pred.count()}\")\n",
    "\n",
    "# ------------- 5) Create prediction examples (one per client, using full history) -------------\n",
    "def make_prediction_examples(kv):\n",
    "    cont_id, seq = kv\n",
    "    # Map to ids using the same prod2id mapping from training\n",
    "    seq_ids = [prod2id.get(x, 0) for x in seq]\n",
    "    if len(seq_ids) == 0:\n",
    "        return []\n",
    "    # Use full history (up to MAX_SEQ_LEN) for prediction\n",
    "    history = seq_ids[-MAX_SEQ_LEN:] if len(seq_ids) > MAX_SEQ_LEN else seq_ids\n",
    "    return [(str(cont_id), history)]\n",
    "\n",
    "pred_examples_rdd = grouped_pred.flatMap(make_prediction_examples)\n",
    "\n",
    "# Convert to DataFrame\n",
    "pred_schema = StructType([\n",
    "    StructField(\"cont_id\", StringType(), True),\n",
    "    StructField(\"hist_seq\", ArrayType(IntegerType()), True),\n",
    "])\n",
    "pred_examples_df = spark.createDataFrame(pred_examples_rdd, pred_schema).cache()\n",
    "\n",
    "print(f\"Prediction examples: {pred_examples_df.count()}\")\n",
    "\n",
    "# ------------- 6) Create features for prediction (same as training) -------------\n",
    "def hist_to_features_row_pred(x):\n",
    "    cont_id, hist = x\n",
    "    seq_len = len(hist)\n",
    "    last_1 = hist[-1] if seq_len >= 1 else 0\n",
    "    last_2 = hist[-2] if seq_len >= 2 else 0\n",
    "    unique_prior = len(set(hist))\n",
    "    num_switches = sum(1 for i in range(1, seq_len) if hist[i] != hist[i-1])\n",
    "    freq = Counter(hist)\n",
    "    freq_features = [freq.get(i, 0) for i in range(1, NUM_CLASSES+1)]\n",
    "    return (str(cont_id), hist, seq_len, last_1, last_2, unique_prior, num_switches, freq_features)\n",
    "\n",
    "rows_pred_rdd = pred_examples_rdd.map(hist_to_features_row_pred)\n",
    "\n",
    "from pyspark.sql.types import ArrayType, LongType\n",
    "feat_pred_schema = StructType([\n",
    "    StructField(\"cont_id\", StringType(), True),\n",
    "    StructField(\"hist_seq\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"seq_len\", IntegerType(), True),\n",
    "    StructField(\"last_1\", IntegerType(), True),\n",
    "    StructField(\"last_2\", IntegerType(), True),\n",
    "    StructField(\"unique_prior\", IntegerType(), True),\n",
    "    StructField(\"num_switches\", IntegerType(), True),\n",
    "    StructField(\"freq_list\", ArrayType(IntegerType()), True),\n",
    "])\n",
    "\n",
    "pred_feats_df = spark.createDataFrame(rows_pred_rdd, feat_pred_schema).cache()\n",
    "\n",
    "# Expand freq_list into separate columns\n",
    "for i in range(1, NUM_CLASSES+1):\n",
    "    pred_feats_df = pred_feats_df.withColumn(f\"freq_{i}\", F.col(\"freq_list\")[i-1])\n",
    "pred_feats_df = pred_feats_df.drop(\"freq_list\")\n",
    "\n",
    "# ------------- 7) Join with static features from most recent snapshot -------------\n",
    "w2_pred = Window.partitionBy(\"cont_id\").orderBy(F.col(\"register_ts\").desc())\n",
    "client_snapshot_pred = (df_pred_events\n",
    "                       .withColumn(\"rn\", F.row_number().over(w2_pred))\n",
    "                       .filter(F.col(\"rn\") == 1)\n",
    "                       .select(\"cont_id\",\n",
    "                               \"acct_val_amt\",\"face_amt\",\"cash_val_amt\",\"wc_total_assets\",\n",
    "                               \"wc_assetmix_stocks\",\"wc_assetmix_bonds\",\"wc_assetmix_mutual_funds\",\n",
    "                               \"wc_assetmix_annuity\",\"wc_assetmix_deposits\",\"wc_assetmix_other_assets\",\n",
    "                               \"psn_age\",\"client_seg\",\"client_seg_1\",\"aum_band\",\"channel\",\"agent_segment\",\"branchoffice_code\", \"business_month\"))\n",
    "\n",
    "pred_full = pred_feats_df.join(client_snapshot_pred, on=\"cont_id\", how=\"inner\")\n",
    "\n",
    "print(f\"Prediction examples after join: {pred_full.count()}\")\n",
    "\n",
    "# ------------- 8) Fill missing values (same as training) -------------\n",
    "numeric_cols_pred = [c for c, t in pred_full.dtypes if t in (\"int\", \"double\", \"bigint\", \"float\") and c not in (\"seq_len\",\"last_1\",\"last_2\",\"unique_prior\",\"num_switches\")]\n",
    "fill_dict_pred = {c: 0 for c in numeric_cols_pred}\n",
    "\n",
    "categorical_cols = [\"client_seg\",\"client_seg_1\",\"aum_band\",\"channel\",\"agent_segment\",\"branchoffice_code\"]\n",
    "modes_pred = {}\n",
    "for c in categorical_cols:\n",
    "    try:\n",
    "        m = pred_full.groupBy(c).count().orderBy(F.desc(\"count\")).first()\n",
    "        modes_pred[c] = m[0] if m and m[0] is not None else \"UNKNOWN\"\n",
    "    except:\n",
    "        modes_pred[c] = \"UNKNOWN\"\n",
    "\n",
    "pred_full = pred_full.fillna(fill_dict_pred)\n",
    "for c in categorical_cols:\n",
    "    pred_full = pred_full.withColumn(c, F.when(F.col(c).isNull(), F.lit(modes_pred[c])).otherwise(F.col(c)))\n",
    "\n",
    "# ------------- 9) Convert hist_seq to fixed-length padded columns -------------\n",
    "def pad_history(hist):\n",
    "    arr = hist[-MAX_SEQ_LEN:] if hist is not None else []\n",
    "    pad_len = MAX_SEQ_LEN - len(arr)\n",
    "    return [0]*pad_len + arr\n",
    "\n",
    "pad_udf = F.udf(lambda x: pad_history(x), ArrayType(IntegerType()))\n",
    "pred_full = pred_full.withColumn(\"hist_padded\", pad_udf(F.col(\"hist_seq\")))\n",
    "\n",
    "for i in range(MAX_SEQ_LEN):\n",
    "    pred_full = pred_full.withColumn(f\"hist_{i}\", F.col(\"hist_padded\")[i])\n",
    "\n",
    "pred_full = pred_full.drop(\"hist_seq\", \"hist_padded\")\n",
    "\n",
    "# ------------- 10) Convert categorical strings to index (same as training) -------------\n",
    "# Use the same categorical mappings from training (we need to recreate them or use broadcast)\n",
    "# For now, we'll recreate the mappings from pred_full\n",
    "for c in categorical_cols:\n",
    "    vals = [r[0] for r in pred_full.select(c).distinct().collect()]\n",
    "    m = {v:i for i,v in enumerate(sorted([str(x) for x in vals]))}\n",
    "    b = spark.sparkContext.broadcast(m)\n",
    "    pred_full = pred_full.withColumn(c + \"_idx\", F.coalesce(F.col(c).cast(\"string\"), F.lit(\"UNKNOWN\")))\n",
    "    pred_full = pred_full.withColumn(c + \"_idx\", F.udf(lambda s: int(b.value.get(str(s), 0)), IntegerType())(F.col(c + \"_idx\")))\n",
    "\n",
    "# ------------- 11) Convert to Pandas and prepare for prediction -------------\n",
    "pred_pd = pred_full.select([\"cont_id\", \"business_month\"] + model_feature_cols).toPandas()\n",
    "pred_pd.fillna(0, inplace=True)\n",
    "\n",
    "print(f\"Prediction data shape: {pred_pd.shape}\")\n",
    "\n",
    "# ------------- 12) Make predictions -------------\n",
    "pred_probs = model.predict(pred_pd[feature_cols_final])\n",
    "pred_class_ids = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "# Convert to product names using the same mapping from training\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "final_id2prod = {model_id: id2prod[original_id] for model_id, original_id in inv_label_map.items()}\n",
    "\n",
    "pred_pd[\"pred_class_id\"] = pred_class_ids\n",
    "pred_pd[\"pred_product\"] = pred_pd[\"pred_class_id\"].apply(lambda x: final_id2prod.get(x, \"UNKNOWN\"))\n",
    "\n",
    "# Add probability columns\n",
    "num_classes_pred = pred_probs.shape[1]\n",
    "for i in range(num_classes_pred):\n",
    "    pred_pd[f\"prob_{i}\"] = pred_probs[:, i]\n",
    "pred_pd[\"pred_prob\"] = pred_pd.apply(lambda r: r[f\"prob_{r['pred_class_id']}\"], axis=1)\n",
    "\n",
    "\n",
    "# ------------- 13) Add additional client information -------------\n",
    "from pyspark.sql.functions import to_date, current_date, datediff, round as spark_round\n",
    "\n",
    "axa_map_df_pred = (\n",
    "    df_pred_raw\n",
    "    .withColumn(\"client_tenure\", spark_round(datediff(current_date(), to_date(\"register_date\")) / 365.25, 2))\n",
    "    .select(\n",
    "        \"cont_id\", \"axa_party_id\", \"product_category\", \"psn_age\", \"client_seg\", \"client_seg_1\",\n",
    "        \"aum_band\", \"channel\", \"division_name\", \"branch_name\", \"business_city\", \"business_state_cod\", \"client_tenure\"\n",
    "    )\n",
    "    .dropDuplicates([\"cont_id\"])\n",
    ")\n",
    "\n",
    "axa_map_pd_pred = axa_map_df_pred.toPandas().drop_duplicates(\"cont_id\")\n",
    "\n",
    "# Ensure psn_age is present in both DataFrames and handle column suffixes if duplicated after merge\n",
    "final_predictions = pred_pd.merge(axa_map_pd_pred, on=\"cont_id\", how=\"left\", suffixes=(\"\", \"_dem\"))\n",
    "\n",
    "# If both pred_pd and axa_map_pd_pred have psn_age, pandas will add suffixes. Prefer the demographic version if present.\n",
    "if \"psn_age_dem\" in final_predictions.columns:\n",
    "    final_predictions[\"psn_age\"] = final_predictions[\"psn_age_dem\"]\n",
    "    final_predictions = final_predictions.drop(columns=[\"psn_age_dem\"])\n",
    "\n",
    "# Select final columns\n",
    "prob_cols_final = [c for c in final_predictions.columns if c.startswith(\"prob_\")]\n",
    "final_cols_list = [\n",
    "    \"cont_id\", \"axa_party_id\", \"product_category\", \"psn_age\", \"client_seg\", \"client_seg_1\",\n",
    "    \"aum_band\", \"channel\", \"division_name\", \"branch_name\", \"business_city\", \"business_state_cod\", \"client_tenure\",\n",
    "    \"pred_class_id\", \"pred_product\", \"pred_prob\"\n",
    "] + prob_cols_final\n",
    "\n",
    "final_predictions = final_predictions[final_cols_list]\n",
    "\n",
    "print(f\"\\nFinal predictions shape: {final_predictions.shape}\")\n",
    "print(f\"\\nFinal predictions sample:\")\n",
    "display(final_predictions.head(10))\n",
    "\n",
    "print(\"\\nPrediction summary:\")\n",
    "print(final_predictions[\"pred_product\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "99532a73-1e67-4725-a8fa-1024df6cf61c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### 13th step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bcd33e7-293a-4957-9753-a269f4e7d6e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.createDataFrame(final_predictions).write.mode(\"overwrite\").saveAsTable(\"eda_smartlist.us_wealth_management_smartlist.transformer4_202510_83\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23938116-076f-4758-bba5-7096c11f8776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'DISABILITY',\n",
       " 2: 'HEALTH',\n",
       " 3: 'INVESTMENT',\n",
       " 4: 'LIFE_INSURANCE',\n",
       " 5: 'NETWORK_PRODUCTS',\n",
       " 6: 'OTHER',\n",
       " 7: 'RETIREMENT'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_list\n",
    "prod2id\n",
    "label_map\n",
    "id2prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0d1805b-1a7f-42f9-be7e-893d9441c9f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DISABILITY': 1,\n",
       " 'HEALTH': 2,\n",
       " 'INVESTMENT': 3,\n",
       " 'LIFE_INSURANCE': 4,\n",
       " 'NETWORK_PRODUCTS': 5,\n",
       " 'OTHER': 6,\n",
       " 'RETIREMENT': 7}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d761c329-58d5-44c8-86d8-a48838ac6c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SHAP Analysis for LightGBM Model\n",
    "\n",
    "## Purpose\n",
    "SHAP (SHapley Additive exPlanations) is a powerful tool used to interpret the predictions of machine learning models. It provides insights into how each feature contributes to the model's predictions, allowing us to understand the model's behavior better. In the context of a LightGBM model, SHAP helps us identify feature importance and offers explanations for individual predictions.\n",
    "\n",
    "## Steps\n",
    "1. **Compute SHAP Values**: We will calculate the SHAP values for the LightGBM model, which quantify the contribution of each feature to the model's output.\n",
    "2. **Summary Plots**: We will generate summary plots to visualize the distribution of SHAP values across all predictions, highlighting the most important features.\n",
    "3. **Feature Importances**: Finally, we will display the feature importances derived from the SHAP values, providing a clear view of which features are driving the model's predictions.\n",
    "\n",
    "By following these steps, we can gain a deeper understanding of our LightGBM model and ensure that our predictions are interpretable and trustworthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26eb4cc2-c359-410e-840e-19df906b8c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SHAP ANALYSIS FOR CLIENT PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "Computing SHAP values for 100 prediction samples...\n",
      "Number of features: 39\n",
      "SHAP values format: Array shape (100, 39, 7), converted to list of 7 arrays\n",
      "Number of classes: 7\n",
      "\n",
      "============================================================\n",
      "OVERALL FEATURE IMPORTANCE\n",
      "============================================================\n",
      "\n",
      "Top 15 Most Important Features for Predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Feature</th><th>Mean_|SHAP_Value|</th><th>Rank</th></tr></thead><tbody><tr><td>hist_9</td><td>1.1724774650202778</td><td>1</td></tr><tr><td>branchoffice_code_idx</td><td>0.4643493689363995</td><td>2</td></tr><tr><td>psn_age</td><td>0.38814532693681525</td><td>3</td></tr><tr><td>face_amt</td><td>0.36151528420600065</td><td>4</td></tr><tr><td>cash_val_amt</td><td>0.3423985351019764</td><td>5</td></tr><tr><td>acct_val_amt</td><td>0.3226135872665702</td><td>6</td></tr><tr><td>last_1</td><td>0.23045749889244854</td><td>7</td></tr><tr><td>freq_4</td><td>0.1501410562922591</td><td>8</td></tr><tr><td>wc_assetmix_annuity</td><td>0.14153491029120627</td><td>9</td></tr><tr><td>agent_segment_idx</td><td>0.13511714753330775</td><td>10</td></tr><tr><td>hist_8</td><td>0.121426857644417</td><td>11</td></tr><tr><td>channel_idx</td><td>0.11316716556891616</td><td>12</td></tr><tr><td>freq_5</td><td>0.08300390070587606</td><td>13</td></tr><tr><td>wc_assetmix_other_assets</td><td>0.08134793891759988</td><td>14</td></tr><tr><td>freq_3</td><td>0.07510705994245881</td><td>15</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "hist_9",
         1.1724774650202778,
         1
        ],
        [
         "branchoffice_code_idx",
         0.4643493689363995,
         2
        ],
        [
         "psn_age",
         0.38814532693681525,
         3
        ],
        [
         "face_amt",
         0.36151528420600065,
         4
        ],
        [
         "cash_val_amt",
         0.3423985351019764,
         5
        ],
        [
         "acct_val_amt",
         0.3226135872665702,
         6
        ],
        [
         "last_1",
         0.23045749889244854,
         7
        ],
        [
         "freq_4",
         0.1501410562922591,
         8
        ],
        [
         "wc_assetmix_annuity",
         0.14153491029120627,
         9
        ],
        [
         "agent_segment_idx",
         0.13511714753330775,
         10
        ],
        [
         "hist_8",
         0.121426857644417,
         11
        ],
        [
         "channel_idx",
         0.11316716556891616,
         12
        ],
        [
         "freq_5",
         0.08300390070587606,
         13
        ],
        [
         "wc_assetmix_other_assets",
         0.08134793891759988,
         14
        ],
        [
         "freq_3",
         0.07510705994245881,
         15
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Feature",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Mean_|SHAP_Value|",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Rank",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING AGENT TALKING POINTS\n",
      "============================================================\n",
      "\n",
      "Generated talking points for 100 clients\n",
      "\n",
      "Sample talking points for first client:\n",
      "------------------------------------------------------------\n",
      "Based on your client profile and purchase history, we recommend LIFE_INSURANCE with 59.4% confidence.\n",
      "\n",
      "Key reasons for this recommendation:\n",
      "  1. Your recent purchase of RETIREMENT indicates strong alignment with LIFE_INSURANCE.\n",
      "  2. Your branchoffice code idx profile strongly supports LIFE_INSURANCE.\n",
      "  4. Your most recent product (RETIREMENT) shows a natural progression to LIFE_INSURANCE.\n",
      "  5. Your agent segment idx profile strongly supports LIFE_INSURANCE.\n",
      "\n",
      "Considerations:\n",
      "  - While age is a factor, LIFE_INSURANCE still offers significant value for your situation.\n",
      "\n",
      "Next Steps: Let's schedule a consultation to discuss how LIFE_INSURANCE can help you achieve your financial goals.\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "PREDICTION SUMMARY BY PRODUCT\n",
      "============================================================\n",
      "\n",
      "Predictions by Product:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Count</th><th>Avg_Confidence</th><th>Min_Confidence</th><th>Max_Confidence</th></tr></thead><tbody><tr><td>130923</td><td>0.746</td><td>0.224</td><td>0.982</td></tr><tr><td>57470</td><td>0.553</td><td>0.296</td><td>0.995</td></tr><tr><td>29847</td><td>0.467</td><td>0.225</td><td>0.968</td></tr><tr><td>20517</td><td>0.687</td><td>0.216</td><td>0.975</td></tr><tr><td>6011</td><td>0.498</td><td>0.229</td><td>0.801</td></tr><tr><td>945</td><td>0.449</td><td>0.22</td><td>0.969</td></tr><tr><td>15</td><td>0.336</td><td>0.234</td><td>0.56</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         130923,
         0.746,
         0.224,
         0.982
        ],
        [
         57470,
         0.553,
         0.296,
         0.995
        ],
        [
         29847,
         0.467,
         0.225,
         0.968
        ],
        [
         20517,
         0.687,
         0.216,
         0.975
        ],
        [
         6011,
         0.498,
         0.229,
         0.801
        ],
        [
         945,
         0.449,
         0.22,
         0.969
        ],
        [
         15,
         0.336,
         0.234,
         0.56
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Avg_Confidence",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Min_Confidence",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Max_Confidence",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE BY PREDICTED PRODUCT\n",
      "============================================================\n",
      "\n",
      "Top 10 Features for LIFE_INSURANCE predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Feature</th><th>Importance</th></tr></thead><tbody><tr><td>hist_9</td><td>1.3673149958937723</td></tr><tr><td>branchoffice_code_idx</td><td>1.144364867783504</td></tr><tr><td>face_amt</td><td>0.39401393368751686</td></tr><tr><td>cash_val_amt</td><td>0.26701732120165567</td></tr><tr><td>psn_age</td><td>0.2497408969340725</td></tr><tr><td>freq_4</td><td>0.24333331590390123</td></tr><tr><td>last_1</td><td>0.23229651698950685</td></tr><tr><td>acct_val_amt</td><td>0.09273797570209114</td></tr><tr><td>agent_segment_idx</td><td>0.06989557019289194</td></tr><tr><td>hist_8</td><td>0.053599152787927144</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "hist_9",
         1.3673149958937723
        ],
        [
         "branchoffice_code_idx",
         1.144364867783504
        ],
        [
         "face_amt",
         0.39401393368751686
        ],
        [
         "cash_val_amt",
         0.26701732120165567
        ],
        [
         "psn_age",
         0.2497408969340725
        ],
        [
         "freq_4",
         0.24333331590390123
        ],
        [
         "last_1",
         0.23229651698950685
        ],
        [
         "acct_val_amt",
         0.09273797570209114
        ],
        [
         "agent_segment_idx",
         0.06989557019289194
        ],
        [
         "hist_8",
         0.053599152787927144
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Feature",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Importance",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Features for OTHER predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Feature</th><th>Importance</th></tr></thead><tbody><tr><td>branchoffice_code_idx</td><td>1.48819435432409</td></tr><tr><td>psn_age</td><td>0.6258616834184567</td></tr><tr><td>acct_val_amt</td><td>0.5237801578634179</td></tr><tr><td>wc_assetmix_annuity</td><td>0.4428580929055625</td></tr><tr><td>hist_9</td><td>0.34649698531283885</td></tr><tr><td>face_amt</td><td>0.33001908652332707</td></tr><tr><td>agent_segment_idx</td><td>0.27277539203206774</td></tr><tr><td>cash_val_amt</td><td>0.2439482023317253</td></tr><tr><td>client_seg_idx</td><td>0.11514730053179106</td></tr><tr><td>freq_7</td><td>0.10349744366027674</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "branchoffice_code_idx",
         1.48819435432409
        ],
        [
         "psn_age",
         0.6258616834184567
        ],
        [
         "acct_val_amt",
         0.5237801578634179
        ],
        [
         "wc_assetmix_annuity",
         0.4428580929055625
        ],
        [
         "hist_9",
         0.34649698531283885
        ],
        [
         "face_amt",
         0.33001908652332707
        ],
        [
         "agent_segment_idx",
         0.27277539203206774
        ],
        [
         "cash_val_amt",
         0.2439482023317253
        ],
        [
         "client_seg_idx",
         0.11514730053179106
        ],
        [
         "freq_7",
         0.10349744366027674
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Feature",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Importance",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Features for NETWORK_PRODUCTS predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Feature</th><th>Importance</th></tr></thead><tbody><tr><td>acct_val_amt</td><td>0.8934665048365</td></tr><tr><td>hist_9</td><td>0.5726904210140203</td></tr><tr><td>aum_band_idx</td><td>0.34895760899295025</td></tr><tr><td>freq_5</td><td>0.29153381080744</td></tr><tr><td>face_amt</td><td>0.23541940817862844</td></tr><tr><td>branchoffice_code_idx</td><td>0.189382727854512</td></tr><tr><td>channel_idx</td><td>0.17840730639777594</td></tr><tr><td>cash_val_amt</td><td>0.14147060806480669</td></tr><tr><td>wc_assetmix_annuity</td><td>0.11016744211739678</td></tr><tr><td>psn_age</td><td>0.09552401925439133</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "acct_val_amt",
         0.8934665048365
        ],
        [
         "hist_9",
         0.5726904210140203
        ],
        [
         "aum_band_idx",
         0.34895760899295025
        ],
        [
         "freq_5",
         0.29153381080744
        ],
        [
         "face_amt",
         0.23541940817862844
        ],
        [
         "branchoffice_code_idx",
         0.189382727854512
        ],
        [
         "channel_idx",
         0.17840730639777594
        ],
        [
         "cash_val_amt",
         0.14147060806480669
        ],
        [
         "wc_assetmix_annuity",
         0.11016744211739678
        ],
        [
         "psn_age",
         0.09552401925439133
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Feature",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Importance",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Features for RETIREMENT predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Feature</th><th>Importance</th></tr></thead><tbody><tr><td>hist_9</td><td>2.9098624565816857</td></tr><tr><td>last_1</td><td>0.6448605613792959</td></tr><tr><td>face_amt</td><td>0.4425544106240939</td></tr><tr><td>wc_assetmix_annuity</td><td>0.24381998895460036</td></tr><tr><td>cash_val_amt</td><td>0.2113675185173826</td></tr><tr><td>acct_val_amt</td><td>0.11414204119560802</td></tr><tr><td>aum_band_idx</td><td>0.08408747639851706</td></tr><tr><td>freq_5</td><td>0.061218478382706616</td></tr><tr><td>wc_assetmix_mutual_funds</td><td>0.05750712151380439</td></tr><tr><td>psn_age</td><td>0.055755546491196434</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "hist_9",
         2.9098624565816857
        ],
        [
         "last_1",
         0.6448605613792959
        ],
        [
         "face_amt",
         0.4425544106240939
        ],
        [
         "wc_assetmix_annuity",
         0.24381998895460036
        ],
        [
         "cash_val_amt",
         0.2113675185173826
        ],
        [
         "acct_val_amt",
         0.11414204119560802
        ],
        [
         "aum_band_idx",
         0.08408747639851706
        ],
        [
         "freq_5",
         0.061218478382706616
        ],
        [
         "wc_assetmix_mutual_funds",
         0.05750712151380439
        ],
        [
         "psn_age",
         0.055755546491196434
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Feature",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Importance",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Features for INVESTMENT predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Feature</th><th>Importance</th></tr></thead><tbody><tr><td>hist_9</td><td>1.3909560806679553</td></tr><tr><td>agent_segment_idx</td><td>0.3820214273860083</td></tr><tr><td>cash_val_amt</td><td>0.3000088660009206</td></tr><tr><td>face_amt</td><td>0.26756562092552977</td></tr><tr><td>channel_idx</td><td>0.2614019279416352</td></tr><tr><td>acct_val_amt</td><td>0.24704340316158832</td></tr><tr><td>last_1</td><td>0.23297324694072374</td></tr><tr><td>branchoffice_code_idx</td><td>0.23268788438260646</td></tr><tr><td>freq_3</td><td>0.08098880150739951</td></tr><tr><td>aum_band_idx</td><td>0.07079409409105819</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "hist_9",
         1.3909560806679553
        ],
        [
         "agent_segment_idx",
         0.3820214273860083
        ],
        [
         "cash_val_amt",
         0.3000088660009206
        ],
        [
         "face_amt",
         0.26756562092552977
        ],
        [
         "channel_idx",
         0.2614019279416352
        ],
        [
         "acct_val_amt",
         0.24704340316158832
        ],
        [
         "last_1",
         0.23297324694072374
        ],
        [
         "branchoffice_code_idx",
         0.23268788438260646
        ],
        [
         "freq_3",
         0.08098880150739951
        ],
        [
         "aum_band_idx",
         0.07079409409105819
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Feature",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Importance",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Features for DISABILITY predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Feature</th><th>Importance</th></tr></thead><tbody><tr><td>psn_age</td><td>1.6430936980435464</td></tr><tr><td>freq_4</td><td>0.8800647718673434</td></tr><tr><td>hist_8</td><td>0.5729158384204837</td></tr><tr><td>acct_val_amt</td><td>0.5008553507217991</td></tr><tr><td>freq_3</td><td>0.4646815866214019</td></tr><tr><td>channel_idx</td><td>0.3255698881197243</td></tr><tr><td>wc_assetmix_other_assets</td><td>0.2902947344186935</td></tr><tr><td>hist_9</td><td>0.26604459716126</td></tr><tr><td>agent_segment_idx</td><td>0.2588461995372873</td></tr><tr><td>face_amt</td><td>0.2504221887461086</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "psn_age",
         1.6430936980435464
        ],
        [
         "freq_4",
         0.8800647718673434
        ],
        [
         "hist_8",
         0.5729158384204837
        ],
        [
         "acct_val_amt",
         0.5008553507217991
        ],
        [
         "freq_3",
         0.4646815866214019
        ],
        [
         "channel_idx",
         0.3255698881197243
        ],
        [
         "wc_assetmix_other_assets",
         0.2902947344186935
        ],
        [
         "hist_9",
         0.26604459716126
        ],
        [
         "agent_segment_idx",
         0.2588461995372873
        ],
        [
         "face_amt",
         0.2504221887461086
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Feature",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Importance",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL OUTPUT: PREDICTIONS WITH TALKING POINTS\n",
      "============================================================\n",
      "\n",
      "Final output shape: (245728, 24)\n",
      "\n",
      "Sample output (first 3 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>cont_id</th><th>axa_party_id</th><th>recommended_product</th><th>recommendation_confidence</th><th>talking_points</th><th>top_positive_features</th><th>top_negative_features</th><th>product_category</th><th>psn_age</th><th>client_seg</th><th>aum_band</th><th>channel</th><th>division_name</th><th>branch_name</th><th>business_city</th><th>business_state_cod</th><th>client_tenure</th><th>prob_0</th><th>prob_1</th><th>prob_2</th><th>prob_3</th><th>prob_4</th><th>prob_5</th><th>prob_6</th></tr></thead><tbody><tr><td>100236768405239730</td><td>00BK05RY3MBBYZETXXXX</td><td>LIFE_INSURANCE</td><td>0.5938047262505148</td><td>Based on your client profile and purchase history, we recommend LIFE_INSURANCE with 59.4% confidence.\n",
       "\n",
       "Key reasons for this recommendation:\n",
       "  1. Your recent purchase of RETIREMENT indicates strong alignment with LIFE_INSURANCE.\n",
       "  2. Your branchoffice code idx profile strongly supports LIFE_INSURANCE.\n",
       "  4. Your most recent product (RETIREMENT) shows a natural progression to LIFE_INSURANCE.\n",
       "  5. Your agent segment idx profile strongly supports LIFE_INSURANCE.\n",
       "\n",
       "Considerations:\n",
       "  - While age is a factor, LIFE_INSURANCE still offers significant value for your situation.\n",
       "\n",
       "Next Steps: Let's schedule a consultation to discuss how LIFE_INSURANCE can help you achieve your financial goals.</td><td>hist_9, branchoffice_code_idx, freq_4</td><td>face_amt, cash_val_amt</td><td>RETIREMENT</td><td>67.0</td><td>1-2.99m</td><td><$25K</td><td>Branch Assist</td><td>Division 8</td><td>PCPG</td><td>SYRACUSE</td><td>NY</td><td>35.47</td><td>0.0012894623356847466</td><td>1.6900128581065633E-6</td><td>0.304129020734926</td><td>0.5938047262505148</td><td>0.05471542317824895</td><td>0.046018349081623194</td><td>4.132840614402719E-5</td></tr><tr><td>100236775740980032</td><td>94YK0DQHXLXIDUTAXXXX</td><td>OTHER</td><td>0.3959357872039781</td><td>Based on your client profile and purchase history, we recommend OTHER with 39.6% confidence.\n",
       "\n",
       "Key reasons for this recommendation:\n",
       "  1. Your branchoffice code idx profile strongly supports OTHER.\n",
       "  2. Your cash val amt profile strongly supports OTHER.\n",
       "  3. Your acct val amt profile strongly supports OTHER.\n",
       "  4. Your asset profile indicates OTHER would be an excellent fit for your portfolio.\n",
       "  5. Your face amt profile strongly supports OTHER.\n",
       "\n",
       "Considerations:\n",
       "\n",
       "Next Steps: Let's schedule a consultation to discuss how OTHER can help you achieve your financial goals.</td><td>branchoffice_code_idx, cash_val_amt, acct_val_amt</td><td>channel_idx, wc_assetmix_stocks</td><td>LIFE_INSURANCE</td><td>60.0</td><td><200k</td><td>$50-$100K</td><td>Branch Assist</td><td>Division 8</td><td>PCPG</td><td>SYRACUSE</td><td>NY</td><td>10.58</td><td>0.017404929202043926</td><td>3.096362893112268E-5</td><td>0.212794693854749</td><td>0.004068445586196477</td><td>0.11514574458162888</td><td>0.3959357872039781</td><td>0.25461943594247255</td></tr><tr><td>100336777803286137</td><td>03BK05RY51WW64PHXXXX</td><td>NETWORK_PRODUCTS</td><td>0.898848040500579</td><td>Based on your client profile and purchase history, we recommend NETWORK_PRODUCTS with 89.9% confidence.\n",
       "\n",
       "Key reasons for this recommendation:\n",
       "  1. Your acct val amt profile strongly supports NETWORK_PRODUCTS.\n",
       "  2. Your recent purchase of LIFE_INSURANCE indicates strong alignment with NETWORK_PRODUCTS.\n",
       "  3. Your asset profile indicates NETWORK_PRODUCTS would be an excellent fit for your portfolio.\n",
       "  5. Your face amt profile strongly supports NETWORK_PRODUCTS.\n",
       "\n",
       "Considerations:\n",
       "\n",
       "Next Steps: Let's schedule a consultation to discuss how NETWORK_PRODUCTS can help you achieve your financial goals.</td><td>acct_val_amt, hist_9, aum_band_idx</td><td>channel_idx, branchoffice_code_idx</td><td>LIFE_INSURANCE</td><td>73.0</td><td><200k</td><td><$25K</td><td>Branch Assist</td><td>Division 8</td><td>PCPG</td><td>SYRACUSE</td><td>NY</td><td>24.43</td><td>0.0017949733570290158</td><td>1.816864919724236E-6</td><td>0.04189879744226347</td><td>0.0020453718716081848</td><td>0.898848040500579</td><td>0.004626477713615976</td><td>0.05078452224998479</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "100236768405239730",
         "00BK05RY3MBBYZETXXXX",
         "LIFE_INSURANCE",
         0.5938047262505148,
         "Based on your client profile and purchase history, we recommend LIFE_INSURANCE with 59.4% confidence.\n\nKey reasons for this recommendation:\n  1. Your recent purchase of RETIREMENT indicates strong alignment with LIFE_INSURANCE.\n  2. Your branchoffice code idx profile strongly supports LIFE_INSURANCE.\n  4. Your most recent product (RETIREMENT) shows a natural progression to LIFE_INSURANCE.\n  5. Your agent segment idx profile strongly supports LIFE_INSURANCE.\n\nConsiderations:\n  - While age is a factor, LIFE_INSURANCE still offers significant value for your situation.\n\nNext Steps: Let's schedule a consultation to discuss how LIFE_INSURANCE can help you achieve your financial goals.",
         "hist_9, branchoffice_code_idx, freq_4",
         "face_amt, cash_val_amt",
         "RETIREMENT",
         67,
         "1-2.99m",
         "<$25K",
         "Branch Assist",
         "Division 8",
         "PCPG",
         "SYRACUSE",
         "NY",
         35.47,
         0.0012894623356847466,
         0.0000016900128581065633,
         0.304129020734926,
         0.5938047262505148,
         0.05471542317824895,
         0.046018349081623194,
         0.00004132840614402719
        ],
        [
         "100236775740980032",
         "94YK0DQHXLXIDUTAXXXX",
         "OTHER",
         0.3959357872039781,
         "Based on your client profile and purchase history, we recommend OTHER with 39.6% confidence.\n\nKey reasons for this recommendation:\n  1. Your branchoffice code idx profile strongly supports OTHER.\n  2. Your cash val amt profile strongly supports OTHER.\n  3. Your acct val amt profile strongly supports OTHER.\n  4. Your asset profile indicates OTHER would be an excellent fit for your portfolio.\n  5. Your face amt profile strongly supports OTHER.\n\nConsiderations:\n\nNext Steps: Let's schedule a consultation to discuss how OTHER can help you achieve your financial goals.",
         "branchoffice_code_idx, cash_val_amt, acct_val_amt",
         "channel_idx, wc_assetmix_stocks",
         "LIFE_INSURANCE",
         60,
         "<200k",
         "$50-$100K",
         "Branch Assist",
         "Division 8",
         "PCPG",
         "SYRACUSE",
         "NY",
         10.58,
         0.017404929202043926,
         0.00003096362893112268,
         0.212794693854749,
         0.004068445586196477,
         0.11514574458162888,
         0.3959357872039781,
         0.25461943594247255
        ],
        [
         "100336777803286137",
         "03BK05RY51WW64PHXXXX",
         "NETWORK_PRODUCTS",
         0.898848040500579,
         "Based on your client profile and purchase history, we recommend NETWORK_PRODUCTS with 89.9% confidence.\n\nKey reasons for this recommendation:\n  1. Your acct val amt profile strongly supports NETWORK_PRODUCTS.\n  2. Your recent purchase of LIFE_INSURANCE indicates strong alignment with NETWORK_PRODUCTS.\n  3. Your asset profile indicates NETWORK_PRODUCTS would be an excellent fit for your portfolio.\n  5. Your face amt profile strongly supports NETWORK_PRODUCTS.\n\nConsiderations:\n\nNext Steps: Let's schedule a consultation to discuss how NETWORK_PRODUCTS can help you achieve your financial goals.",
         "acct_val_amt, hist_9, aum_band_idx",
         "channel_idx, branchoffice_code_idx",
         "LIFE_INSURANCE",
         73,
         "<200k",
         "<$25K",
         "Branch Assist",
         "Division 8",
         "PCPG",
         "SYRACUSE",
         "NY",
         24.43,
         0.0017949733570290158,
         0.000001816864919724236,
         0.04189879744226347,
         0.0020453718716081848,
         0.898848040500579,
         0.004626477713615976,
         0.05078452224998479
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "cont_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "axa_party_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "recommended_product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "recommendation_confidence",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "talking_points",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "top_positive_features",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "top_negative_features",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "psn_age",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "client_seg",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "aum_band",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "channel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "division_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "branch_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "business_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "business_state_cod",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "client_tenure",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "prob_0",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "prob_1",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "prob_2",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "prob_3",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "prob_4",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "prob_5",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "prob_6",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "AGENT SUMMARY REPORT\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Recommendations Summary by Product:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Client_Count</th><th>Avg_Confidence</th><th>Min_Confidence</th><th>Max_Confidence</th></tr></thead><tbody><tr><td>130923</td><td>0.746</td><td>0.224</td><td>0.982</td></tr><tr><td>57470</td><td>0.553</td><td>0.296</td><td>0.995</td></tr><tr><td>29847</td><td>0.467</td><td>0.225</td><td>0.968</td></tr><tr><td>20517</td><td>0.687</td><td>0.216</td><td>0.975</td></tr><tr><td>6011</td><td>0.498</td><td>0.229</td><td>0.801</td></tr><tr><td>945</td><td>0.449</td><td>0.22</td><td>0.969</td></tr><tr><td>15</td><td>0.336</td><td>0.234</td><td>0.56</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         130923,
         0.746,
         0.224,
         0.982
        ],
        [
         57470,
         0.553,
         0.296,
         0.995
        ],
        [
         29847,
         0.467,
         0.225,
         0.968
        ],
        [
         20517,
         0.687,
         0.216,
         0.975
        ],
        [
         6011,
         0.498,
         0.229,
         0.801
        ],
        [
         945,
         0.449,
         0.22,
         0.969
        ],
        [
         15,
         0.336,
         0.234,
         0.56
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Client_Count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Avg_Confidence",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Min_Confidence",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Max_Confidence",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ High Confidence Recommendations (>70%): 105281 clients\n",
      "\n",
      "Top 5 High Confidence Clients:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>cont_id</th><th>recommended_product</th><th>recommendation_confidence</th></tr></thead><tbody><tr><td>100336777803286137</td><td>NETWORK_PRODUCTS</td><td>0.898848040500579</td></tr><tr><td>100367742412104944</td><td>RETIREMENT</td><td>0.8291345205905127</td></tr><tr><td>100436778268667989</td><td>RETIREMENT</td><td>0.8748926140368007</td></tr><tr><td>100636768966578192</td><td>LIFE_INSURANCE</td><td>0.7336613204298862</td></tr><tr><td>100836767553266158</td><td>RETIREMENT</td><td>0.7319233646857863</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "100336777803286137",
         "NETWORK_PRODUCTS",
         0.898848040500579
        ],
        [
         "100367742412104944",
         "RETIREMENT",
         0.8291345205905127
        ],
        [
         "100436778268667989",
         "RETIREMENT",
         0.8748926140368007
        ],
        [
         "100636768966578192",
         "LIFE_INSURANCE",
         0.7336613204298862
        ],
        [
         "100836767553266158",
         "RETIREMENT",
         0.7319233646857863
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "cont_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "recommended_product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "recommendation_confidence",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘¥ Recommendations by Client Segment:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>client_seg</th><th>recommended_product</th><th>count</th></tr></thead><tbody><tr><td>1-2.99m</td><td>RETIREMENT</td><td>33095</td></tr><tr><td>400-999k</td><td>RETIREMENT</td><td>29192</td></tr><tr><td><200k</td><td>RETIREMENT</td><td>23332</td></tr><tr><td>400-999k</td><td>LIFE_INSURANCE</td><td>16540</td></tr><tr><td>200-399k</td><td>RETIREMENT</td><td>14915</td></tr><tr><td>1-2.99m</td><td>LIFE_INSURANCE</td><td>14838</td></tr><tr><td><200k</td><td>OTHER</td><td>10650</td></tr><tr><td><200k</td><td>LIFE_INSURANCE</td><td>10186</td></tr><tr><td>5m+</td><td>RETIREMENT</td><td>9214</td></tr><tr><td>3-4.99m</td><td>RETIREMENT</td><td>8546</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "1-2.99m",
         "RETIREMENT",
         33095
        ],
        [
         "400-999k",
         "RETIREMENT",
         29192
        ],
        [
         "<200k",
         "RETIREMENT",
         23332
        ],
        [
         "400-999k",
         "LIFE_INSURANCE",
         16540
        ],
        [
         "200-399k",
         "RETIREMENT",
         14915
        ],
        [
         "1-2.99m",
         "LIFE_INSURANCE",
         14838
        ],
        [
         "<200k",
         "OTHER",
         10650
        ],
        [
         "<200k",
         "LIFE_INSURANCE",
         10186
        ],
        [
         "5m+",
         "RETIREMENT",
         9214
        ],
        [
         "3-4.99m",
         "RETIREMENT",
         8546
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "client_seg",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "recommended_product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘¤ Recommendations by Age Group:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age_group</th><th>recommended_product</th><th>count</th></tr></thead><tbody><tr><td>65+</td><td>RETIREMENT</td><td>94610</td></tr><tr><td>65+</td><td>LIFE_INSURANCE</td><td>27216</td></tr><tr><td>50-65</td><td>RETIREMENT</td><td>24951</td></tr><tr><td>50-65</td><td>LIFE_INSURANCE</td><td>18492</td></tr><tr><td>50-65</td><td>OTHER</td><td>15311</td></tr><tr><td>35-50</td><td>OTHER</td><td>11076</td></tr><tr><td>35-50</td><td>LIFE_INSURANCE</td><td>10636</td></tr><tr><td>35-50</td><td>RETIREMENT</td><td>9407</td></tr><tr><td>65+</td><td>NETWORK_PRODUCTS</td><td>8295</td></tr><tr><td>50-65</td><td>NETWORK_PRODUCTS</td><td>7798</td></tr><tr><td>35-50</td><td>NETWORK_PRODUCTS</td><td>3957</td></tr><tr><td>65+</td><td>INVESTMENT</td><td>2892</td></tr><tr><td><35</td><td>OTHER</td><td>2772</td></tr><tr><td>50-65</td><td>INVESTMENT</td><td>2455</td></tr><tr><td><35</td><td>RETIREMENT</td><td>1750</td></tr><tr><td><35</td><td>LIFE_INSURANCE</td><td>1073</td></tr><tr><td>50-65</td><td>DISABILITY</td><td>745</td></tr><tr><td>65+</td><td>OTHER</td><td>645</td></tr><tr><td>35-50</td><td>INVESTMENT</td><td>526</td></tr><tr><td><35</td><td>NETWORK_PRODUCTS</td><td>373</td></tr><tr><td>65+</td><td>DISABILITY</td><td>177</td></tr><tr><td><35</td><td>INVESTMENT</td><td>104</td></tr><tr><td><35</td><td>DISABILITY</td><td>18</td></tr><tr><td>35-50</td><td>HEALTH</td><td>8</td></tr><tr><td><35</td><td>HEALTH</td><td>5</td></tr><tr><td>35-50</td><td>DISABILITY</td><td>4</td></tr><tr><td>50-65</td><td>HEALTH</td><td>2</td></tr><tr><td>65+</td><td>HEALTH</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "65+",
         "RETIREMENT",
         94610
        ],
        [
         "65+",
         "LIFE_INSURANCE",
         27216
        ],
        [
         "50-65",
         "RETIREMENT",
         24951
        ],
        [
         "50-65",
         "LIFE_INSURANCE",
         18492
        ],
        [
         "50-65",
         "OTHER",
         15311
        ],
        [
         "35-50",
         "OTHER",
         11076
        ],
        [
         "35-50",
         "LIFE_INSURANCE",
         10636
        ],
        [
         "35-50",
         "RETIREMENT",
         9407
        ],
        [
         "65+",
         "NETWORK_PRODUCTS",
         8295
        ],
        [
         "50-65",
         "NETWORK_PRODUCTS",
         7798
        ],
        [
         "35-50",
         "NETWORK_PRODUCTS",
         3957
        ],
        [
         "65+",
         "INVESTMENT",
         2892
        ],
        [
         "<35",
         "OTHER",
         2772
        ],
        [
         "50-65",
         "INVESTMENT",
         2455
        ],
        [
         "<35",
         "RETIREMENT",
         1750
        ],
        [
         "<35",
         "LIFE_INSURANCE",
         1073
        ],
        [
         "50-65",
         "DISABILITY",
         745
        ],
        [
         "65+",
         "OTHER",
         645
        ],
        [
         "35-50",
         "INVESTMENT",
         526
        ],
        [
         "<35",
         "NETWORK_PRODUCTS",
         373
        ],
        [
         "65+",
         "DISABILITY",
         177
        ],
        [
         "<35",
         "INVESTMENT",
         104
        ],
        [
         "<35",
         "DISABILITY",
         18
        ],
        [
         "35-50",
         "HEALTH",
         8
        ],
        [
         "<35",
         "HEALTH",
         5
        ],
        [
         "35-50",
         "DISABILITY",
         4
        ],
        [
         "50-65",
         "HEALTH",
         2
        ],
        [
         "65+",
         "HEALTH",
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "age_group",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "recommended_product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE!\n",
      "============================================================\n",
      "\n",
      "âœ… Use 'agent_pitch_data' DataFrame to access predictions with talking points.\n",
      "ðŸ“‹ Total clients with recommendations: 245728\n",
      "ðŸ“¦ Products recommended: 7\n",
      "ðŸ“ˆ Average confidence: 65.5%\n",
      "\n",
      "ðŸ’¡ Each row in 'agent_pitch_data' contains:\n",
      "   - Client information (cont_id, demographics)\n",
      "   - Recommended product and confidence score\n",
      "   - Personalized talking points based on SHAP analysis\n",
      "   - Top contributing features for the recommendation\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SHAP ANALYSIS FOR PREDICTIONS + AGENT TALKING POINTS\n",
    "# ============================================\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SHAP ANALYSIS FOR CLIENT PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use prediction data for SHAP analysis (sample if too large)\n",
    "shap_sample_size = min(100, len(pred_pd))\n",
    "shap_pred_data = pred_pd[feature_cols_final].iloc[:shap_sample_size].copy()\n",
    "shap_pred_indices = pred_pd.iloc[:shap_sample_size].index\n",
    "\n",
    "# Get corresponding final_predictions data for demographic info\n",
    "if len(final_predictions) >= shap_sample_size:\n",
    "    shap_final_preds = final_predictions.iloc[:shap_sample_size].copy()\n",
    "else:\n",
    "    shap_final_preds = final_predictions.copy()\n",
    "\n",
    "print(f\"\\nComputing SHAP values for {len(shap_pred_data)} prediction samples...\")\n",
    "print(f\"Number of features: {len(feature_cols_final)}\")\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calculate SHAP values for predictions\n",
    "shap_values_pred = explainer.shap_values(shap_pred_data)\n",
    "\n",
    "# Handle different SHAP output formats\n",
    "if isinstance(shap_values_pred, list):\n",
    "    # List of arrays (one per class)\n",
    "    num_classes = len(shap_values_pred)\n",
    "    print(f\"SHAP values format: List of {num_classes} arrays\")\n",
    "    print(f\"Each array shape: {shap_values_pred[0].shape}\")\n",
    "else:\n",
    "    # Single array - convert to list format\n",
    "    shap_array = np.array(shap_values_pred)\n",
    "    if len(shap_array.shape) == 3:\n",
    "        # Shape is (n_samples, n_features, n_classes) - need to transpose\n",
    "        num_classes = shap_array.shape[2]\n",
    "        shap_values_pred = [shap_array[:, :, i] for i in range(num_classes)]\n",
    "        print(f\"SHAP values format: Array shape {shap_array.shape}, converted to list of {num_classes} arrays\")\n",
    "    else:\n",
    "        num_classes = 1\n",
    "        shap_values_pred = [shap_array]\n",
    "        print(f\"SHAP values format: Single array shape {shap_array.shape}\")\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# ------------- 1) Overall Feature Importance -------------\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OVERALL FEATURE IMPORTANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "feature_importance_pred = {}\n",
    "for i, feat in enumerate(feature_cols_final):\n",
    "    # Calculate mean absolute SHAP value across all classes\n",
    "    importance = np.mean([np.mean(np.abs(sv[:, i])) for sv in shap_values_pred])\n",
    "    feature_importance_pred[feat] = importance\n",
    "\n",
    "feature_importance_sorted = sorted(feature_importance_pred.items(), key=lambda x: x[1], reverse=True)\n",
    "shap_importance_df = pd.DataFrame(feature_importance_sorted, columns=['Feature', 'Mean_|SHAP_Value|'])\n",
    "shap_importance_df['Rank'] = range(1, len(shap_importance_df) + 1)\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features for Predictions:\")\n",
    "display(shap_importance_df.head(15))\n",
    "\n",
    "# ------------- 2) Generate Talking Points for Each Client -------------\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING AGENT TALKING POINTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get product name mapping\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "final_id2prod = {model_id: id2prod[original_id] for model_id, original_id in inv_label_map.items()}\n",
    "\n",
    "# Create talking points DataFrame\n",
    "talking_points_list = []\n",
    "\n",
    "for idx in range(len(shap_pred_data)):\n",
    "    actual_idx = shap_pred_indices[idx]\n",
    "    client_id = pred_pd.iloc[actual_idx]['cont_id']\n",
    "    pred_class = pred_pd.iloc[actual_idx]['pred_class_id']\n",
    "    pred_product = pred_pd.iloc[actual_idx]['pred_product']\n",
    "    pred_prob = pred_pd.iloc[actual_idx]['pred_prob']\n",
    "    \n",
    "    # Get demographic data if available\n",
    "    client_demo = None\n",
    "    if idx < len(shap_final_preds):\n",
    "        client_demo = shap_final_preds.iloc[idx]\n",
    "    \n",
    "    # Get SHAP values for the predicted class\n",
    "    # Ensure pred_class is within valid range\n",
    "    if pred_class >= len(shap_values_pred):\n",
    "        pred_class = len(shap_values_pred) - 1\n",
    "    shap_vals = shap_values_pred[pred_class][idx]\n",
    "    \n",
    "    # Get top contributing features (positive and negative)\n",
    "    feature_contributions = []\n",
    "    for i, feat in enumerate(feature_cols_final):\n",
    "        contrib = shap_vals[i]\n",
    "        feature_contributions.append({\n",
    "            'feature': feat,\n",
    "            'shap_value': contrib,\n",
    "            'feature_value': shap_pred_data.iloc[idx][feat]\n",
    "        })\n",
    "    \n",
    "    # Sort by absolute SHAP value\n",
    "    feature_contributions.sort(key=lambda x: abs(x['shap_value']), reverse=True)\n",
    "    \n",
    "    # Get top 5 positive and top 3 negative contributors\n",
    "    top_positive = [f for f in feature_contributions if f['shap_value'] > 0][:5]\n",
    "    top_negative = [f for f in feature_contributions if f['shap_value'] < 0][:3]\n",
    "    \n",
    "    # Build talking points\n",
    "    talking_points = []\n",
    "    \n",
    "    # Main recommendation\n",
    "    talking_points.append(f\"Based on your client profile and purchase history, we recommend {pred_product} with {pred_prob*100:.1f}% confidence.\")\n",
    "    \n",
    "    # Positive drivers\n",
    "    if top_positive:\n",
    "        talking_points.append(\"\\nKey reasons for this recommendation:\")\n",
    "        for i, contrib in enumerate(top_positive, 1):\n",
    "            feat = contrib['feature']\n",
    "            val = contrib['feature_value']\n",
    "            shap_val = contrib['shap_value']\n",
    "            \n",
    "            # Interpret feature values\n",
    "            if feat.startswith('hist_'):\n",
    "                pos = int(feat.split('_')[1])\n",
    "                if val > 0:\n",
    "                    product_name = id2prod.get(int(val), f\"Product {val}\")\n",
    "                    talking_points.append(f\"  {i}. Your recent purchase of {product_name} indicates strong alignment with {pred_product}.\")\n",
    "            \n",
    "            elif feat == 'last_1':\n",
    "                if val > 0:\n",
    "                    product_name = id2prod.get(int(val), f\"Product {val}\")\n",
    "                    talking_points.append(f\"  {i}. Your most recent product ({product_name}) shows a natural progression to {pred_product}.\")\n",
    "            \n",
    "            elif feat == 'last_2':\n",
    "                if val > 0:\n",
    "                    product_name = id2prod.get(int(val), f\"Product {val}\")\n",
    "                    talking_points.append(f\"  {i}. Your purchase history including {product_name} suggests {pred_product} would complement your portfolio.\")\n",
    "            \n",
    "            elif feat.startswith('freq_'):\n",
    "                prod_id = int(feat.split('_')[1])\n",
    "                if val > 0:\n",
    "                    product_name = id2prod.get(prod_id, f\"Product {prod_id}\")\n",
    "                    talking_points.append(f\"  {i}. Your frequent engagement with {product_name} indicates readiness for {pred_product}.\")\n",
    "            \n",
    "            elif feat == 'psn_age':\n",
    "                age_val = int(val) if val > 0 else (int(client_demo['psn_age']) if client_demo is not None and 'psn_age' in client_demo and pd.notna(client_demo['psn_age']) else None)\n",
    "                if age_val:\n",
    "                    talking_points.append(f\"  {i}. At age {age_val}, {pred_product} aligns well with your life stage and financial planning needs.\")\n",
    "                else:\n",
    "                    talking_points.append(f\"  {i}. Your age profile indicates {pred_product} would be an excellent fit for your financial goals.\")\n",
    "            \n",
    "            elif feat == 'client_tenure' or 'tenure' in feat.lower():\n",
    "                tenure_val = val if val > 0 else (client_demo['client_tenure'] if client_demo is not None and 'client_tenure' in client_demo and pd.notna(client_demo['client_tenure']) else None)\n",
    "                if tenure_val:\n",
    "                    talking_points.append(f\"  {i}. With {tenure_val:.1f} years as our client, {pred_product} represents a natural next step in your relationship with us.\")\n",
    "                else:\n",
    "                    talking_points.append(f\"  {i}. As a valued client, {pred_product} represents a natural next step in your relationship with us.\")\n",
    "            \n",
    "            elif 'aum' in feat.lower() or 'asset' in feat.lower():\n",
    "                talking_points.append(f\"  {i}. Your asset profile indicates {pred_product} would be an excellent fit for your portfolio.\")\n",
    "            \n",
    "            elif 'channel' in feat.lower():\n",
    "                talking_points.append(f\"  {i}. Clients in your channel typically benefit from {pred_product}.\")\n",
    "            \n",
    "            else:\n",
    "                talking_points.append(f\"  {i}. Your {feat.replace('_', ' ')} profile strongly supports {pred_product}.\")\n",
    "    \n",
    "    # Address potential concerns (negative contributors)\n",
    "    if top_negative:\n",
    "        talking_points.append(\"\\nConsiderations:\")\n",
    "        for contrib in top_negative:\n",
    "            feat = contrib['feature']\n",
    "            # Only mention if it's a significant concern\n",
    "            if abs(contrib['shap_value']) > 0.1:\n",
    "                if 'age' in feat.lower():\n",
    "                    talking_points.append(f\"  - While age is a factor, {pred_product} still offers significant value for your situation.\")\n",
    "                elif 'freq' in feat.lower():\n",
    "                    talking_points.append(f\"  - Even with limited prior engagement in this category, {pred_product} presents a strong opportunity.\")\n",
    "    \n",
    "    # Add call to action\n",
    "    talking_points.append(f\"\\nNext Steps: Let's schedule a consultation to discuss how {pred_product} can help you achieve your financial goals.\")\n",
    "    \n",
    "    talking_points_list.append({\n",
    "        'cont_id': client_id,\n",
    "        'predicted_product': pred_product,\n",
    "        'confidence': f\"{pred_prob*100:.1f}%\",\n",
    "        'talking_points': '\\n'.join(talking_points),\n",
    "        'top_positive_features': ', '.join([f['feature'] for f in top_positive[:3]]),\n",
    "        'top_negative_features': ', '.join([f['feature'] for f in top_negative[:2]])\n",
    "    })\n",
    "\n",
    "# Create talking points DataFrame\n",
    "talking_points_df = pd.DataFrame(talking_points_list)\n",
    "\n",
    "print(f\"\\nGenerated talking points for {len(talking_points_df)} clients\")\n",
    "print(\"\\nSample talking points for first client:\")\n",
    "print(\"-\" * 60)\n",
    "print(talking_points_df.iloc[0]['talking_points'])\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# ------------- 3) Summary Statistics by Predicted Product -------------\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREDICTION SUMMARY BY PRODUCT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pred_summary = final_predictions.groupby('pred_product').agg({\n",
    "    'cont_id': 'count',\n",
    "    'pred_prob': ['mean', 'min', 'max']\n",
    "}).round(3)\n",
    "pred_summary.columns = ['Count', 'Avg_Confidence', 'Min_Confidence', 'Max_Confidence']\n",
    "pred_summary = pred_summary.sort_values('Count', ascending=False)\n",
    "print(\"\\nPredictions by Product:\")\n",
    "display(pred_summary)\n",
    "\n",
    "# ------------- 4) Feature Importance by Predicted Product -------------\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE IMPORTANCE BY PREDICTED PRODUCT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for product in final_predictions['pred_product'].unique():\n",
    "    product_mask = pred_pd['pred_product'] == product\n",
    "    if product_mask.sum() > 0:\n",
    "        # Get indices in pred_pd for this product\n",
    "        product_pred_indices = pred_pd[product_mask].index[:min(50, product_mask.sum())]\n",
    "        \n",
    "        # Map to shap_pred_data indices (only include those in our SHAP sample)\n",
    "        shap_pred_indices_list = shap_pred_indices.tolist() if hasattr(shap_pred_indices, 'tolist') else list(shap_pred_indices)\n",
    "        product_shap_indices = []\n",
    "        for pred_idx in product_pred_indices:\n",
    "            try:\n",
    "                shap_idx = shap_pred_indices_list.index(pred_idx)\n",
    "                product_shap_indices.append(shap_idx)\n",
    "            except ValueError:\n",
    "                continue  # Skip if not in SHAP sample\n",
    "        \n",
    "        if len(product_shap_indices) > 0:\n",
    "            # Get SHAP values for this product's predicted class\n",
    "            product_pred_class = pred_pd.iloc[product_pred_indices[0]]['pred_class_id']\n",
    "            # Ensure product_pred_class is within valid range\n",
    "            if product_pred_class >= len(shap_values_pred):\n",
    "                product_pred_class = len(shap_values_pred) - 1\n",
    "            \n",
    "            product_shap_vals = shap_values_pred[product_pred_class][product_shap_indices]\n",
    "            \n",
    "            # Calculate feature importance for this product\n",
    "            product_feat_importance = {}\n",
    "            for i, feat in enumerate(feature_cols_final):\n",
    "                product_feat_importance[feat] = np.mean(np.abs(product_shap_vals[:, i]))\n",
    "            \n",
    "            product_feat_sorted = sorted(product_feat_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "            product_feat_df = pd.DataFrame(product_feat_sorted[:10], columns=['Feature', 'Importance'])\n",
    "            \n",
    "            print(f\"\\nTop 10 Features for {product} predictions:\")\n",
    "            display(product_feat_df)\n",
    "\n",
    "# ------------- 5) Create Final Output with Talking Points -------------\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL OUTPUT: PREDICTIONS WITH TALKING POINTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Merge talking points with final predictions\n",
    "final_with_talking_points = final_predictions.merge(\n",
    "    talking_points_df[['cont_id', 'talking_points', 'top_positive_features', 'top_negative_features']],\n",
    "    left_on='cont_id',\n",
    "    right_on='cont_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Reorder columns\n",
    "output_cols = [\n",
    "    'cont_id', 'axa_party_id', 'predicted_product', 'pred_product', 'pred_prob',\n",
    "    'confidence', 'talking_points', 'top_positive_features', 'top_negative_features',\n",
    "    'product_category', 'psn_age', 'client_seg', 'aum_band', 'channel', \n",
    "    'division_name', 'branch_name', 'business_city', 'business_state_cod', 'client_tenure'\n",
    "] + [c for c in final_with_talking_points.columns if c.startswith('prob_')]\n",
    "\n",
    "# Select available columns\n",
    "available_cols = [c for c in output_cols if c in final_with_talking_points.columns]\n",
    "final_with_talking_points = final_with_talking_points[available_cols]\n",
    "\n",
    "# Rename for clarity\n",
    "final_with_talking_points = final_with_talking_points.rename(columns={\n",
    "    'pred_product': 'recommended_product',\n",
    "    'pred_prob': 'recommendation_confidence'\n",
    "})\n",
    "\n",
    "print(f\"\\nFinal output shape: {final_with_talking_points.shape}\")\n",
    "print(\"\\nSample output (first 3 rows):\")\n",
    "display(final_with_talking_points.head(3))\n",
    "\n",
    "# Save to variable for easy access\n",
    "agent_pitch_data = final_with_talking_points.copy()\n",
    "\n",
    "# ------------- 6) Create Agent Summary Report -------------\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AGENT SUMMARY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Summary by product\n",
    "product_summary = agent_pitch_data.groupby('recommended_product').agg({\n",
    "    'cont_id': 'count',\n",
    "    'recommendation_confidence': ['mean', 'min', 'max']\n",
    "}).round(3)\n",
    "product_summary.columns = ['Client_Count', 'Avg_Confidence', 'Min_Confidence', 'Max_Confidence']\n",
    "product_summary = product_summary.sort_values('Client_Count', ascending=False)\n",
    "\n",
    "print(\"\\nðŸ“Š Recommendations Summary by Product:\")\n",
    "display(product_summary)\n",
    "\n",
    "# High confidence recommendations (>70%)\n",
    "high_confidence = agent_pitch_data[agent_pitch_data['recommendation_confidence'] > 0.7]\n",
    "print(f\"\\nðŸŽ¯ High Confidence Recommendations (>70%): {len(high_confidence)} clients\")\n",
    "if len(high_confidence) > 0:\n",
    "    print(\"\\nTop 5 High Confidence Clients:\")\n",
    "    high_conf_display = high_confidence[['cont_id', 'recommended_product', 'recommendation_confidence']].head(5)\n",
    "    display(high_conf_display)\n",
    "\n",
    "# Recommendations by client segment\n",
    "if 'client_seg' in agent_pitch_data.columns:\n",
    "    seg_summary = agent_pitch_data.groupby(['client_seg', 'recommended_product']).size().reset_index(name='count')\n",
    "    seg_summary = seg_summary.sort_values('count', ascending=False)\n",
    "    print(\"\\nðŸ‘¥ Recommendations by Client Segment:\")\n",
    "    display(seg_summary.head(10))\n",
    "\n",
    "# Recommendations by age group\n",
    "if 'psn_age' in agent_pitch_data.columns:\n",
    "    agent_pitch_data['age_group'] = pd.cut(\n",
    "        agent_pitch_data['psn_age'], \n",
    "        bins=[0, 35, 50, 65, 100], \n",
    "        labels=['<35', '35-50', '50-65', '65+']\n",
    "    )\n",
    "    age_summary = agent_pitch_data.groupby(['age_group', 'recommended_product']).size().reset_index(name='count')\n",
    "    age_summary = age_summary.sort_values('count', ascending=False)\n",
    "    print(\"\\nðŸ‘¤ Recommendations by Age Group:\")\n",
    "    display(age_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nâœ… Use 'agent_pitch_data' DataFrame to access predictions with talking points.\")\n",
    "print(f\"ðŸ“‹ Total clients with recommendations: {len(agent_pitch_data)}\")\n",
    "print(f\"ðŸ“¦ Products recommended: {agent_pitch_data['recommended_product'].nunique()}\")\n",
    "print(f\"ðŸ“ˆ Average confidence: {agent_pitch_data['recommendation_confidence'].mean()*100:.1f}%\")\n",
    "print(f\"\\nðŸ’¡ Each row in 'agent_pitch_data' contains:\")\n",
    "print(\"   - Client information (cont_id, demographics)\")\n",
    "print(\"   - Recommended product and confidence score\")\n",
    "print(\"   - Personalized talking points based on SHAP analysis\")\n",
    "print(\"   - Top contributing features for the recommendation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da019230-e8db-4ec7-98b2-f7337c80765d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¬ Generating Enhanced Talking Points...\n",
      "================================================================================\n",
      "Generated enhanced talking points for 100 records\n",
      "\n",
      "ðŸ“‹ Sample Enhanced Explanation:\n",
      "================================================================================\n",
      "Client: 00BK05RY3MBBYZETXXXX\n",
      "Product: Life_Insurance\n",
      "Score: 0.594\n",
      "\n",
      "Top Reasons (Enhanced):\n",
      "1. ðŸ”¥ Recent purchase history: RETIREMENT (strong engagement pattern) â†’ Build on existing relationship\n",
      "2. ðŸ”¥ Face amount: $4,517 (significant coverage) â†’ Review coverage adequacy\n",
      "3. ðŸ”¥ Most recent product: RETIREMENT (active client engagement) â†’ Natural product progression\n",
      "4. ðŸ”¥ Cash value: $0 (accumulated value) â†’ Optimize cash value growth\n",
      "5. ðŸ”¥ Age 67 (retirement planning window) â†’ Focus on retirement readiness\n",
      "\n",
      "Enhanced talking points complete\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Sample df_explanations DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>cont_id</th><th>axa_party_id</th><th>product</th><th>score</th><th>pred_class_id</th><th>top_feature_1</th><th>top_feature_1_value</th><th>top_feature_1_shap</th><th>top_feature_2</th><th>top_feature_2_value</th><th>top_feature_2_shap</th><th>top_feature_3</th><th>top_feature_3_value</th><th>top_feature_3_shap</th><th>top_feature_4</th><th>top_feature_4_value</th><th>top_feature_4_shap</th><th>top_feature_5</th><th>top_feature_5_value</th><th>top_feature_5_shap</th><th>psn_age</th><th>channel</th><th>aum_band</th><th>client_tenure</th><th>client_seg</th><th>client_seg_1</th><th>enhanced_talking_point_1</th><th>enhanced_talking_point_2</th><th>enhanced_talking_point_3</th><th>enhanced_talking_point_4</th><th>enhanced_talking_point_5</th></tr></thead><tbody><tr><td>100236768405239730</td><td>00BK05RY3MBBYZETXXXX</td><td>LIFE_INSURANCE</td><td>0.5938047262505148</td><td>3</td><td>hist_9</td><td>7.0</td><td>1.3411791428462072</td><td>face_amt</td><td>4517.27978515625</td><td>-0.4251241366279902</td><td>last_1</td><td>7.0</td><td>0.23965723118047746</td><td>cash_val_amt</td><td>0.0</td><td>-0.22075836214563097</td><td>psn_age</td><td>67.0</td><td>-0.16332863727903824</td><td>67.0</td><td>Branch Assist</td><td><$25K</td><td>35.47</td><td>1-2.99m</td><td>Next Chapters</td><td>ðŸ”¥ Recent purchase history: RETIREMENT (strong engagement pattern) â†’ Build on existing relationship</td><td>ðŸ”¥ Face amount: $4,517 (significant coverage) â†’ Review coverage adequacy</td><td>ðŸ”¥ Most recent product: RETIREMENT (active client engagement) â†’ Natural product progression</td><td>ðŸ”¥ Cash value: $0 (accumulated value) â†’ Optimize cash value growth</td><td>ðŸ”¥ Age 67 (retirement planning window) â†’ Focus on retirement readiness</td></tr><tr><td>100236775740980032</td><td>94YK0DQHXLXIDUTAXXXX</td><td>OTHER</td><td>0.3959357872039781</td><td>5</td><td>cash_val_amt</td><td>47800.171875</td><td>0.5789888431986175</td><td>acct_val_amt</td><td>51325.578125</td><td>0.5272540486392654</td><td>wc_assetmix_annuity</td><td>671.0</td><td>0.5206662779149053</td><td>face_amt</td><td>298000.0</td><td>0.4754566866527263</td><td>psn_age</td><td>60.0</td><td>0.3246531523221521</td><td>60.0</td><td>Branch Assist</td><td>$50-$100K</td><td>10.58</td><td><200k</td><td>Strategic Spenders</td><td>ðŸ”¥ Cash value: $47,800 (accumulated value) â†’ Optimize cash value growth</td><td>ðŸ”¥ Account value of $51,326 (strong capacity) â†’ Discuss portfolio diversification</td><td>ðŸ”¥ Annuity allocation: $671 (income-focused strategy) â†’ Expand retirement income solutions</td><td>ðŸ”¥ Face amount: $298,000 (significant coverage) â†’ Review coverage adequacy</td><td>ðŸ”¥ Age 60 (retirement planning window) â†’ Focus on retirement readiness</td></tr><tr><td>100336777803286137</td><td>03BK05RY51WW64PHXXXX</td><td>NETWORK_PRODUCTS</td><td>0.898848040500579</td><td>4</td><td>acct_val_amt</td><td>0.0</td><td>0.8768672456971227</td><td>hist_9</td><td>4.0</td><td>0.5233633015079674</td><td>aum_band_idx</td><td><$25K</td><td>0.35297361316761905</td><td>channel_idx</td><td>Branch Assist</td><td>-0.23703658541371186</td><td>face_amt</td><td>200000.0</td><td>0.22461856950272793</td><td>73.0</td><td>Branch Assist</td><td><$25K</td><td>24.43</td><td><200k</td><td>Strategic Spenders</td><td>ðŸ”¥ Account value of $0 (moderate capacity) â†’ Discuss portfolio diversification</td><td>ðŸ”¥ Recent purchase history: LIFE_INSURANCE (strong engagement pattern) â†’ Build on existing relationship</td><td></td><td>ðŸ”¥ Channel: Branch Assist (direct channel) â†’ Personal outreach approach</td><td>ðŸ”¥ Face amount: $200,000 (significant coverage) â†’ Review coverage adequacy</td></tr><tr><td>100367602181183095</td><td>96YK0DQHXI9402H9XXXX</td><td>RETIREMENT</td><td>0.46700793050411765</td><td>6</td><td>hist_9</td><td>4.0</td><td>2.8230836501594156</td><td>last_1</td><td>4.0</td><td>0.6138390601269282</td><td>acct_val_amt</td><td>18038.609375</td><td>-0.26808574997177576</td><td>aum_band_idx</td><td><$25K</td><td>-0.1657958964377084</td><td>cash_val_amt</td><td>18038.609375</td><td>-0.11763838378402688</td><td>71.0</td><td>Branch Assist</td><td><$25K</td><td>22.35</td><td>1-2.99m</td><td>Next Chapters</td><td>ðŸ”¥ Recent purchase history: LIFE_INSURANCE (strong engagement pattern) â†’ Build on existing relationship</td><td>ðŸ”¥ Most recent product: LIFE_INSURANCE (active client engagement) â†’ Natural product progression</td><td>ðŸ”¥ Account value of $18,039 (moderate capacity) â†’ Discuss portfolio diversification</td><td></td><td>ðŸ”¥ Cash value: $18,039 (accumulated value) â†’ Optimize cash value growth</td></tr><tr><td>100367742412104944</td><td>31YK0DQHXNK0ROV0XXXX</td><td>RETIREMENT</td><td>0.8291345205905127</td><td>6</td><td>hist_9</td><td>4.0</td><td>2.943542025912494</td><td>last_1</td><td>4.0</td><td>0.6517243776306855</td><td>face_amt</td><td>10000.0</td><td>0.45765472270309354</td><td>cash_val_amt</td><td>7550.0</td><td>-0.22681370011333263</td><td>wc_assetmix_annuity</td><td>70846.0</td><td>0.1894961497849251</td><td>74.0</td><td>Branch Assist</td><td><$25K</td><td>64.22</td><td>1-2.99m</td><td>Next Chapters</td><td>ðŸ”¥ Recent purchase history: LIFE_INSURANCE (strong engagement pattern) â†’ Build on existing relationship</td><td>ðŸ”¥ Most recent product: LIFE_INSURANCE (active client engagement) â†’ Natural product progression</td><td>ðŸ”¥ Face amount: $10,000 (significant coverage) â†’ Review coverage adequacy</td><td>ðŸ”¥ Cash value: $7,550 (accumulated value) â†’ Optimize cash value growth</td><td>ðŸ”¥ Annuity allocation: $70,846 (income-focused strategy) â†’ Expand retirement income solutions</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "100236768405239730",
         "00BK05RY3MBBYZETXXXX",
         "LIFE_INSURANCE",
         0.5938047262505148,
         3,
         "hist_9",
         7,
         1.3411791428462072,
         "face_amt",
         4517.27978515625,
         -0.4251241366279902,
         "last_1",
         "7.0",
         0.23965723118047746,
         "cash_val_amt",
         "0.0",
         -0.22075836214563097,
         "psn_age",
         "67.0",
         -0.16332863727903824,
         67,
         "Branch Assist",
         "<$25K",
         35.47,
         "1-2.99m",
         "Next Chapters",
         "ðŸ”¥ Recent purchase history: RETIREMENT (strong engagement pattern) â†’ Build on existing relationship",
         "ðŸ”¥ Face amount: $4,517 (significant coverage) â†’ Review coverage adequacy",
         "ðŸ”¥ Most recent product: RETIREMENT (active client engagement) â†’ Natural product progression",
         "ðŸ”¥ Cash value: $0 (accumulated value) â†’ Optimize cash value growth",
         "ðŸ”¥ Age 67 (retirement planning window) â†’ Focus on retirement readiness"
        ],
        [
         "100236775740980032",
         "94YK0DQHXLXIDUTAXXXX",
         "OTHER",
         0.3959357872039781,
         5,
         "cash_val_amt",
         47800.171875,
         0.5789888431986175,
         "acct_val_amt",
         51325.578125,
         0.5272540486392654,
         "wc_assetmix_annuity",
         "671.0",
         0.5206662779149053,
         "face_amt",
         "298000.0",
         0.4754566866527263,
         "psn_age",
         "60.0",
         0.3246531523221521,
         60,
         "Branch Assist",
         "$50-$100K",
         10.58,
         "<200k",
         "Strategic Spenders",
         "ðŸ”¥ Cash value: $47,800 (accumulated value) â†’ Optimize cash value growth",
         "ðŸ”¥ Account value of $51,326 (strong capacity) â†’ Discuss portfolio diversification",
         "ðŸ”¥ Annuity allocation: $671 (income-focused strategy) â†’ Expand retirement income solutions",
         "ðŸ”¥ Face amount: $298,000 (significant coverage) â†’ Review coverage adequacy",
         "ðŸ”¥ Age 60 (retirement planning window) â†’ Focus on retirement readiness"
        ],
        [
         "100336777803286137",
         "03BK05RY51WW64PHXXXX",
         "NETWORK_PRODUCTS",
         0.898848040500579,
         4,
         "acct_val_amt",
         0,
         0.8768672456971227,
         "hist_9",
         4,
         0.5233633015079674,
         "aum_band_idx",
         "<$25K",
         0.35297361316761905,
         "channel_idx",
         "Branch Assist",
         -0.23703658541371186,
         "face_amt",
         "200000.0",
         0.22461856950272793,
         73,
         "Branch Assist",
         "<$25K",
         24.43,
         "<200k",
         "Strategic Spenders",
         "ðŸ”¥ Account value of $0 (moderate capacity) â†’ Discuss portfolio diversification",
         "ðŸ”¥ Recent purchase history: LIFE_INSURANCE (strong engagement pattern) â†’ Build on existing relationship",
         "",
         "ðŸ”¥ Channel: Branch Assist (direct channel) â†’ Personal outreach approach",
         "ðŸ”¥ Face amount: $200,000 (significant coverage) â†’ Review coverage adequacy"
        ],
        [
         "100367602181183095",
         "96YK0DQHXI9402H9XXXX",
         "RETIREMENT",
         0.46700793050411765,
         6,
         "hist_9",
         4,
         2.8230836501594156,
         "last_1",
         4,
         0.6138390601269282,
         "acct_val_amt",
         "18038.609375",
         -0.26808574997177576,
         "aum_band_idx",
         "<$25K",
         -0.1657958964377084,
         "cash_val_amt",
         "18038.609375",
         -0.11763838378402688,
         71,
         "Branch Assist",
         "<$25K",
         22.35,
         "1-2.99m",
         "Next Chapters",
         "ðŸ”¥ Recent purchase history: LIFE_INSURANCE (strong engagement pattern) â†’ Build on existing relationship",
         "ðŸ”¥ Most recent product: LIFE_INSURANCE (active client engagement) â†’ Natural product progression",
         "ðŸ”¥ Account value of $18,039 (moderate capacity) â†’ Discuss portfolio diversification",
         "",
         "ðŸ”¥ Cash value: $18,039 (accumulated value) â†’ Optimize cash value growth"
        ],
        [
         "100367742412104944",
         "31YK0DQHXNK0ROV0XXXX",
         "RETIREMENT",
         0.8291345205905127,
         6,
         "hist_9",
         4,
         2.943542025912494,
         "last_1",
         4,
         0.6517243776306855,
         "face_amt",
         "10000.0",
         0.45765472270309354,
         "cash_val_amt",
         "7550.0",
         -0.22681370011333263,
         "wc_assetmix_annuity",
         "70846.0",
         0.1894961497849251,
         74,
         "Branch Assist",
         "<$25K",
         64.22,
         "1-2.99m",
         "Next Chapters",
         "ðŸ”¥ Recent purchase history: LIFE_INSURANCE (strong engagement pattern) â†’ Build on existing relationship",
         "ðŸ”¥ Most recent product: LIFE_INSURANCE (active client engagement) â†’ Natural product progression",
         "ðŸ”¥ Face amount: $10,000 (significant coverage) â†’ Review coverage adequacy",
         "ðŸ”¥ Cash value: $7,550 (accumulated value) â†’ Optimize cash value growth",
         "ðŸ”¥ Annuity allocation: $70,846 (income-focused strategy) â†’ Expand retirement income solutions"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "cont_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "axa_party_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "pred_class_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "top_feature_1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "top_feature_1_value",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "top_feature_1_shap",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "top_feature_2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "top_feature_2_value",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "top_feature_2_shap",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "top_feature_3",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "top_feature_3_value",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "top_feature_3_shap",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "top_feature_4",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "top_feature_4_value",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "top_feature_4_shap",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "top_feature_5",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "top_feature_5_value",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "top_feature_5_shap",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "psn_age",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "channel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "aum_band",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "client_tenure",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "client_seg",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "client_seg_1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "enhanced_talking_point_1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "enhanced_talking_point_2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "enhanced_talking_point_3",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "enhanced_talking_point_4",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "enhanced_talking_point_5",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================\n",
    "# ENHANCED TALKING POINTS GENERATION\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"ðŸ’¬ Generating Enhanced Talking Points...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# First, create df_explanations DataFrame from SHAP analysis results\n",
    "# This will contain top features, their values, and SHAP contributions for each client\n",
    "\n",
    "df_explanations_list = []\n",
    "\n",
    "for idx in range(len(shap_pred_data)):\n",
    "    actual_idx = shap_pred_indices[idx]\n",
    "    client_id = pred_pd.iloc[actual_idx]['cont_id']\n",
    "    pred_class = pred_pd.iloc[actual_idx]['pred_class_id']\n",
    "    pred_product = pred_pd.iloc[actual_idx]['pred_product']\n",
    "    pred_prob = pred_pd.iloc[actual_idx]['pred_prob']\n",
    "    \n",
    "    # Get demographic data if available\n",
    "    client_demo = None\n",
    "    if idx < len(shap_final_preds):\n",
    "        client_demo = shap_final_preds.iloc[idx]\n",
    "    \n",
    "    # Get SHAP values for the predicted class\n",
    "    if pred_class >= len(shap_values_pred):\n",
    "        pred_class = len(shap_values_pred) - 1\n",
    "    shap_vals = shap_values_pred[pred_class][idx]\n",
    "    \n",
    "    # Get feature contributions\n",
    "    feature_contributions = []\n",
    "    for i, feat in enumerate(feature_cols_final):\n",
    "        contrib = shap_vals[i]\n",
    "        feat_val = shap_pred_data.iloc[idx][feat]\n",
    "        \n",
    "        # Get actual feature value (try to get from client_demo if available for better context)\n",
    "        actual_value = feat_val\n",
    "        \n",
    "        # Skip index features or map them to actual values\n",
    "        if feat.endswith('_idx'):\n",
    "            # Try to get the actual categorical value\n",
    "            base_feat = feat.replace('_idx', '')\n",
    "            if client_demo is not None and base_feat in client_demo:\n",
    "                actual_value = client_demo[base_feat]\n",
    "            else:\n",
    "                # Skip index features if we can't map them\n",
    "                continue\n",
    "        \n",
    "        if client_demo is not None:\n",
    "            # Map feature names to actual column names\n",
    "            feat_mapping = {\n",
    "                'acct_val_amt': 'acct_val_amt',\n",
    "                'wc_total_assets': 'wc_total_assets',\n",
    "                'psn_age': 'psn_age',\n",
    "                'channel': 'channel',\n",
    "                'client_seg': 'client_seg',\n",
    "                'client_seg_1': 'client_seg_1',\n",
    "                'aum_band': 'aum_band',\n",
    "                'wc_assetmix_stocks': 'wc_assetmix_stocks',\n",
    "                'wc_assetmix_annuity': 'wc_assetmix_annuity',\n",
    "                'face_amt': 'face_amt',\n",
    "                'cash_val_amt': 'cash_val_amt',\n",
    "            }\n",
    "            \n",
    "            # Check if we can get a better value from client_demo\n",
    "            if feat in feat_mapping:\n",
    "                col_name = feat_mapping[feat]\n",
    "                if col_name in client_demo:\n",
    "                    try:\n",
    "                        demo_val = client_demo[col_name]\n",
    "                        if pd.notna(demo_val):\n",
    "                            actual_value = demo_val\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        # Skip zero values for frequency and history features (they're not meaningful)\n",
    "        if feat.startswith('freq_') and (actual_value == 0 or actual_value == 0.0):\n",
    "            continue\n",
    "        \n",
    "        feature_contributions.append({\n",
    "            'feature': feat,\n",
    "            'shap_value': contrib,\n",
    "            'feature_value': actual_value\n",
    "        })\n",
    "    \n",
    "    # Sort by absolute SHAP value\n",
    "    feature_contributions.sort(key=lambda x: abs(x['shap_value']), reverse=True)\n",
    "    \n",
    "    # Get top 5 features\n",
    "    top_features = feature_contributions[:5]\n",
    "    \n",
    "    # Create row for df_explanations\n",
    "    row = {\n",
    "        'cont_id': client_id,\n",
    "        'axa_party_id': client_demo.get('axa_party_id', 'N/A') if client_demo is not None else 'N/A',\n",
    "        'product': pred_product,\n",
    "        'score': pred_prob,\n",
    "        'pred_class_id': pred_class\n",
    "    }\n",
    "    \n",
    "    # Add top 5 features\n",
    "    for rank in range(1, 6):\n",
    "        if rank <= len(top_features):\n",
    "            feat_info = top_features[rank - 1]\n",
    "            row[f'top_feature_{rank}'] = feat_info['feature']\n",
    "            row[f'top_feature_{rank}_value'] = feat_info['feature_value']\n",
    "            row[f'top_feature_{rank}_shap'] = feat_info['shap_value']\n",
    "        else:\n",
    "            row[f'top_feature_{rank}'] = None\n",
    "            row[f'top_feature_{rank}_value'] = None\n",
    "            row[f'top_feature_{rank}_shap'] = None\n",
    "    \n",
    "    # Add additional context fields from client_demo if available (only numeric or simple string types)\n",
    "    if client_demo is not None:\n",
    "        for field in ['psn_age', 'wc_total_assets', 'acct_val_amt', 'wc_assetmix_stocks', \n",
    "                      'channel', 'aum_band', 'client_tenure', 'client_seg', 'client_seg_1']:\n",
    "            if field in client_demo:\n",
    "                val = client_demo[field]\n",
    "                # Convert to appropriate type to avoid Arrow conversion issues\n",
    "                if pd.notna(val):\n",
    "                    if field in ['psn_age', 'wc_total_assets', 'acct_val_amt', 'wc_assetmix_stocks', 'client_tenure']:\n",
    "                        try:\n",
    "                            row[field] = float(val) if val != '' else None\n",
    "                        except:\n",
    "                            row[field] = None\n",
    "                    else:\n",
    "                        # For categorical fields, keep as string but ensure it's a simple string\n",
    "                        row[field] = str(val) if val is not None else None\n",
    "                else:\n",
    "                    row[field] = None\n",
    "    \n",
    "    df_explanations_list.append(row)\n",
    "\n",
    "df_explanations = pd.DataFrame(df_explanations_list)\n",
    "\n",
    "# Enhanced talking point templates with context and actions\n",
    "ENHANCED_TEMPLATES = {\n",
    "    'acct_val_amt': {\n",
    "        'base': 'Account value of ${value:,.0f}',\n",
    "        'context': lambda v: 'strong capacity' if v > 50000 else 'good capacity' if v > 25000 else 'moderate capacity',\n",
    "        'action': 'Discuss portfolio diversification'\n",
    "    },\n",
    "    'wc_total_assets': {\n",
    "        'base': 'Total assets of ${value:,.0f}',\n",
    "        'context': lambda v: 'high net worth client' if v > 100000 else 'substantial assets',\n",
    "        'action': 'Explore comprehensive wealth planning'\n",
    "    },\n",
    "    'aum_segment': {\n",
    "        'base': 'AUM tier: {value}',\n",
    "        'context': lambda v: 'premium client segment' if v == 'HIGH' else 'core client segment',\n",
    "        'action': lambda v: 'White-glove service approach' if v == 'HIGH' else 'Standard advisory approach'\n",
    "    },\n",
    "    'wc_assetmix_stocks': {\n",
    "        'base': 'Stock allocation: ${value:,.0f}',\n",
    "        'context': 'equity-focused portfolio',\n",
    "        'action': 'Position growth products'\n",
    "    },\n",
    "    'aggressive_investor': {\n",
    "        'base': 'Aggressive risk profile',\n",
    "        'context': 'high-growth orientation',\n",
    "        'action': 'Emphasize equity and growth opportunities'\n",
    "    },\n",
    "    'conservative_investor': {\n",
    "        'base': 'Conservative risk profile',\n",
    "        'context': 'capital preservation focus',\n",
    "        'action': 'Highlight stability and guaranteed products'\n",
    "    },\n",
    "    'client_age': {\n",
    "        'base': 'Age {value:.0f}',\n",
    "        'context': lambda v: 'retirement planning window' if v > 55 else 'wealth accumulation phase' if v > 40 else 'early career',\n",
    "        'action': lambda v: 'Focus on retirement readiness' if v > 55 else 'Emphasize long-term growth'\n",
    "    },\n",
    "    'psn_age': {\n",
    "        'base': 'Age {value:.0f}',\n",
    "        'context': lambda v: 'retirement planning window' if v > 55 else 'wealth accumulation phase' if v > 40 else 'early career',\n",
    "        'action': lambda v: 'Focus on retirement readiness' if v > 55 else 'Emphasize long-term growth'\n",
    "    },\n",
    "    'retirement_planning_trigger': {\n",
    "        'base': 'Retirement planning phase',\n",
    "        'context': 'active retirement preparation',\n",
    "        'action': 'Lead with retirement income solutions'\n",
    "    },\n",
    "    'snp_close_lead_6': {\n",
    "        'base': 'S&P 6-month trend: {value:+.1f}%',\n",
    "        'context': lambda v: 'positive market momentum' if v > 0 else 'market correction opportunity',\n",
    "        'action': lambda v: 'Act on current strength' if v > 0 else 'Position for recovery'\n",
    "    },\n",
    "    'channel': {\n",
    "        'base': 'Channel: {value}',\n",
    "        'context': lambda v: 'advisor relationship' if 'Advisor' in str(v) else 'direct channel',\n",
    "        'action': lambda v: 'Leverage advisor trust' if 'Advisor' in str(v) else 'Personal outreach approach'\n",
    "    },\n",
    "    'channel_idx': {\n",
    "        'base': 'Channel: {value}',\n",
    "        'context': lambda v: 'advisor relationship' if 'Advisor' in str(v) else 'direct channel',\n",
    "        'action': lambda v: 'Leverage advisor trust' if 'Advisor' in str(v) else 'Personal outreach approach'\n",
    "    },\n",
    "    'client_tenure_years': {\n",
    "        'base': '{value:.0f} years with us',\n",
    "        'context': lambda v: 'long-standing relationship' if v > 10 else 'established client' if v > 5 else 'newer client',\n",
    "        'action': lambda v: 'Deepen existing relationship' if v > 5 else 'Build trust and engagement'\n",
    "    },\n",
    "    'client_tenure': {\n",
    "        'base': '{value:.1f} years with us',\n",
    "        'context': lambda v: 'long-standing relationship' if v > 10 else 'established client' if v > 5 else 'newer client',\n",
    "        'action': lambda v: 'Deepen existing relationship' if v > 5 else 'Build trust and engagement'\n",
    "    },\n",
    "    'hist_': {\n",
    "        'base': 'Recent purchase history: {value}',\n",
    "        'context': 'strong engagement pattern',\n",
    "        'action': 'Build on existing relationship'\n",
    "    },\n",
    "    'last_1': {\n",
    "        'base': 'Most recent product: {value}',\n",
    "        'context': 'active client engagement',\n",
    "        'action': 'Natural product progression'\n",
    "    },\n",
    "    'last_2': {\n",
    "        'base': 'Previous product: {value}',\n",
    "        'context': 'diverse product portfolio',\n",
    "        'action': 'Complement existing holdings'\n",
    "    },\n",
    "    'freq_': {\n",
    "        'base': 'Purchase frequency: {value}',\n",
    "        'context': 'consistent engagement',\n",
    "        'action': 'Leverage loyalty'\n",
    "    },\n",
    "    'wc_assetmix_annuity': {\n",
    "        'base': 'Annuity allocation: ${value:,.0f}',\n",
    "        'context': 'income-focused strategy',\n",
    "        'action': 'Expand retirement income solutions'\n",
    "    },\n",
    "    'face_amt': {\n",
    "        'base': 'Face amount: ${value:,.0f}',\n",
    "        'context': 'significant coverage',\n",
    "        'action': 'Review coverage adequacy'\n",
    "    },\n",
    "    'cash_val_amt': {\n",
    "        'base': 'Cash value: ${value:,.0f}',\n",
    "        'context': 'accumulated value',\n",
    "        'action': 'Optimize cash value growth'\n",
    "    },\n",
    "    'branchoffice_code': {\n",
    "        'base': 'Branch office: {value}',\n",
    "        'context': 'local market presence',\n",
    "        'action': 'Leverage local expertise'\n",
    "    },\n",
    "    'agent_segment': {\n",
    "        'base': 'Agent segment: {value}',\n",
    "        'context': 'specialized advisory',\n",
    "        'action': 'Align with segment expertise'\n",
    "    },\n",
    "    'client_seg': {\n",
    "        'base': 'Client segment: {value}',\n",
    "        'context': 'targeted service tier',\n",
    "        'action': 'Customize approach to segment'\n",
    "    },\n",
    "    'client_seg_1': {\n",
    "        'base': 'Client segment level 1: {value}',\n",
    "        'context': 'refined segmentation',\n",
    "        'action': 'Tailor recommendations'\n",
    "    }\n",
    "}\n",
    "\n",
    "def generate_enhanced_talking_point(feature_name, feature_value, shap_value, id2prod=None):\n",
    "    \"\"\"Generate enhanced talking point with context and action\"\"\"\n",
    "    # Impact indicator\n",
    "    impact = \"ðŸ”¥\" if abs(shap_value) > 0.1 else \"â­\" if abs(shap_value) > 0.05 else \"\"\n",
    "    \n",
    "    # Handle special cases for history features\n",
    "    if feature_name.startswith('hist_'):\n",
    "        # Try to map to product name\n",
    "        if id2prod is not None:\n",
    "            try:\n",
    "                prod_id = int(float(feature_value)) if feature_value not in [None, ''] else 0\n",
    "                if prod_id > 0 and prod_id in id2prod:\n",
    "                    feature_value = id2prod[prod_id]\n",
    "                elif prod_id == 0:\n",
    "                    return None  # Skip zero/empty history\n",
    "            except:\n",
    "                pass\n",
    "        template_dict = ENHANCED_TEMPLATES.get('hist_', None)\n",
    "    elif feature_name.startswith('freq_'):\n",
    "        # Skip zero frequencies\n",
    "        try:\n",
    "            freq_val = float(feature_value) if feature_value not in [None, ''] else 0\n",
    "            if freq_val == 0:\n",
    "                return None\n",
    "        except:\n",
    "            pass\n",
    "        template_dict = ENHANCED_TEMPLATES.get('freq_', None)\n",
    "    elif feature_name in ['last_1', 'last_2']:\n",
    "        # Try to map product ID to name\n",
    "        if id2prod is not None:\n",
    "            try:\n",
    "                prod_id = int(float(feature_value)) if feature_value not in [None, ''] else 0\n",
    "                if prod_id > 0 and prod_id in id2prod:\n",
    "                    feature_value = id2prod[prod_id]\n",
    "                elif prod_id == 0:\n",
    "                    return None  # Skip zero/empty\n",
    "            except:\n",
    "                pass\n",
    "        template_dict = ENHANCED_TEMPLATES.get(feature_name, None)\n",
    "    else:\n",
    "        # Find matching template\n",
    "        template_dict = None\n",
    "        for template_key in ENHANCED_TEMPLATES:\n",
    "            if template_key in feature_name or feature_name.startswith(template_key):\n",
    "                template_dict = ENHANCED_TEMPLATES[template_key]\n",
    "                break\n",
    "    \n",
    "    if not template_dict:\n",
    "        # Skip index features and other non-meaningful features\n",
    "        if feature_name.endswith('_idx') or feature_name in ['seq_len', 'unique_prior', 'num_switches']:\n",
    "            return None\n",
    "        # Fallback to simple format\n",
    "        return f\"{impact} {feature_name}: {feature_value} (impact: {shap_value:+.3f})\"\n",
    "    \n",
    "    # Format base message\n",
    "    try:\n",
    "        template = template_dict['base']\n",
    "        if '{value' in template:\n",
    "            # Handle None, NaN, or empty values\n",
    "            if feature_value is None or feature_value == '' or (isinstance(feature_value, float) and np.isnan(feature_value)):\n",
    "                return None\n",
    "            \n",
    "            # Convert to numeric if needed\n",
    "            if isinstance(feature_value, str) and feature_value not in ['N/A', 'UNKNOWN']:\n",
    "                try:\n",
    "                    feature_value = float(feature_value)\n",
    "                except:\n",
    "                    # Keep as string if conversion fails\n",
    "                    pass\n",
    "            \n",
    "            message = template.format(value=feature_value)\n",
    "        else:\n",
    "            message = template\n",
    "    except Exception as e:\n",
    "        # If formatting fails, skip this feature\n",
    "        return None\n",
    "    \n",
    "    # Add context\n",
    "    context = template_dict.get('context')\n",
    "    if context:\n",
    "        if callable(context):\n",
    "            try:\n",
    "                context_str = context(feature_value)\n",
    "            except:\n",
    "                context_str = None\n",
    "        else:\n",
    "            context_str = context\n",
    "        \n",
    "        if context_str:\n",
    "            message += f\" ({context_str})\"\n",
    "    \n",
    "    # Add action\n",
    "    action = template_dict.get('action')\n",
    "    if action:\n",
    "        if callable(action):\n",
    "            try:\n",
    "                action_str = action(feature_value)\n",
    "            except:\n",
    "                action_str = None\n",
    "        else:\n",
    "            action_str = action\n",
    "        \n",
    "        if action_str:\n",
    "            message += f\" â†’ {action_str}\"\n",
    "    \n",
    "    return f\"{impact} {message}\"\n",
    "\n",
    "# Generate enhanced talking points and add as new columns\n",
    "if len(df_explanations) > 0:\n",
    "    for i in range(len(df_explanations)):\n",
    "        valid_talking_points = []\n",
    "        for rank in range(1, 6):\n",
    "            feature = df_explanations.iloc[i][f'top_feature_{rank}']\n",
    "            value = df_explanations.iloc[i][f'top_feature_{rank}_value']\n",
    "            shap_val = df_explanations.iloc[i][f'top_feature_{rank}_shap']\n",
    "            \n",
    "            if pd.notna(feature) and pd.notna(shap_val):\n",
    "                enhanced_tp = generate_enhanced_talking_point(feature, value, shap_val, id2prod)\n",
    "                if enhanced_tp is not None:  # Only add non-None talking points\n",
    "                    valid_talking_points.append(enhanced_tp)\n",
    "                    df_explanations.loc[i, f'enhanced_talking_point_{rank}'] = enhanced_tp\n",
    "                else:\n",
    "                    df_explanations.loc[i, f'enhanced_talking_point_{rank}'] = None\n",
    "            else:\n",
    "                df_explanations.loc[i, f'enhanced_talking_point_{rank}'] = None\n",
    "        \n",
    "        # If we have fewer than 5 valid talking points, pad with None\n",
    "        for rank in range(len(valid_talking_points) + 1, 6):\n",
    "            if f'enhanced_talking_point_{rank}' not in df_explanations.columns:\n",
    "                df_explanations.loc[i, f'enhanced_talking_point_{rank}'] = None\n",
    "    \n",
    "    print(f\"Generated enhanced talking points for {len(df_explanations):,} records\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nðŸ“‹ Sample Enhanced Explanation:\")\n",
    "    print(\"=\" * 80)\n",
    "    if len(df_explanations) > 0:\n",
    "        sample = df_explanations.iloc[0]\n",
    "        print(f\"Client: {sample.get('axa_party_id', sample.get('cont_id', 'N/A'))}\")\n",
    "        print(f\"Product: {sample['product'].title()}\")\n",
    "        print(f\"Score: {sample['score']:.3f}\")\n",
    "        print(f\"\\nTop Reasons (Enhanced):\")\n",
    "        for i in range(1, 6):\n",
    "            tp = sample.get(f'enhanced_talking_point_{i}')\n",
    "            if pd.notna(tp):\n",
    "                print(f\"{i}. {tp}\")\n",
    "    else:\n",
    "        print(\"No samples available\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No explanations to enhance\")\n",
    "\n",
    "print(\"\\nEnhanced talking points complete\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display sample of df_explanations\n",
    "print(\"\\nðŸ“Š Sample df_explanations DataFrame:\")\n",
    "# Convert problematic string columns to ensure Arrow compatibility\n",
    "df_explanations_display = df_explanations.copy()\n",
    "\n",
    "# Clean up the display DataFrame - convert object columns to proper types\n",
    "for col in df_explanations_display.columns:\n",
    "    if df_explanations_display[col].dtype == 'object':\n",
    "        # For string columns, replace NaN/None with empty string for display\n",
    "        df_explanations_display[col] = df_explanations_display[col].fillna('').astype(str)\n",
    "        df_explanations_display[col] = df_explanations_display[col].replace('nan', '').replace('None', '')\n",
    "\n",
    "display(df_explanations_display.head(5))\n",
    "\n",
    "# Note: If you need to convert to Spark DataFrame, handle string columns carefully:\n",
    "# Option 1: Convert only specific columns that are safe\n",
    "# Option 2: Use verifySchema=False and let Spark infer types\n",
    "# Option 3: Explicitly define schema for problematic columns\n",
    "# Example:\n",
    "# from pyspark.sql.types import StringType, DoubleType, IntegerType\n",
    "# schema = StructType([\n",
    "#     StructField(\"cont_id\", StringType(), True),\n",
    "#     StructField(\"aum_band\", StringType(), True),  # Keep as string\n",
    "#     ...\n",
    "# ])\n",
    "# spark_df = spark.createDataFrame(df_explanations, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1705f76c-d469-4062-aee4-52932d8a9d00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVING MODEL ARTIFACTS\n",
      "================================================================================\n",
      "Artifacts will be saved to: /Workspace/Users/rajesh.patil@equitable.com/Final_model_files\n",
      "  - prod2id: 7 products\n",
      "  - num_classes: 7\n",
      "  - feature_cols: 39 features\n",
      "\n",
      "âœ“ Created directory: /Workspace/Users/rajesh.patil@equitable.com/Final_model_files\n",
      "âœ“ Saved artifacts to: /Workspace/Users/rajesh.patil@equitable.com/Final_model_files/artifacts.pkl\n",
      "\n",
      "================================================================================\n",
      "ARTIFACTS SAVED SUCCESSFULLY\n",
      "================================================================================\n",
      "\n",
      "Saved artifacts:\n",
      "  2. artifacts.pkl - Contains:\n",
      "     - prod2id: dict with 7 items\n",
      "     - id2prod: dict with 7 items\n",
      "     - label_map: dict with 7 items\n",
      "     - num_classes: int = 7\n",
      "     - feature_cols: list with 39 items\n",
      "     - max_seq_len: int = 10\n",
      "\n",
      "ðŸ“ Artifacts location: /Workspace/Users/rajesh.patil@equitable.com/Final_model_files\n",
      "\n",
      "âœ… You can now proceed to register the model in the next cell!\n"
     ]
    }
   ],
   "source": [
    "# # STEP 0: Save Model Artifacts from Training Notebook\n",
    "# # ============================================================================\n",
    "# # Run this cell AFTER training your model in the diagnostics notebook\n",
    "# # This saves all required artifacts to your working directory\n",
    "\n",
    "# import os\n",
    "# import pickle\n",
    "# import lightgbm as lgb\n",
    "\n",
    "# # ============================================================================\n",
    "# # CONFIGURATION - Choose where to save artifacts\n",
    "# # ============================================================================\n",
    "\n",
    "# # Option 1: Save in your working directory (current notebook directory) - RECOMMENDED\n",
    "# WORKING_DIR = os.getcwd()  # Current working directory\n",
    "# ARTIFACTS_PATH = \"/Workspace/Users/rajesh.patil@equitable.com/Final_model_files\"\n",
    "\n",
    "# # Option 2: Save in a specific Databricks path (uncomment if preferred)\n",
    "# # ARTIFACTS_PATH = \"/dbfs/FileStore/model_artifacts\"\n",
    "\n",
    "# # Option 3: Save in Unity Catalog mount point (uncomment if preferred)\n",
    "# # ARTIFACTS_PATH = \"/dbfs/mnt/unity-catalog/models/wealth_management_product_recommendation/v1\"\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"SAVING MODEL ARTIFACTS\")\n",
    "# print(\"=\" * 80)\n",
    "# print(f\"Artifacts will be saved to: {ARTIFACTS_PATH}\")\n",
    "\n",
    "# # ============================================================================\n",
    "# # STEP 1: Collect Artifacts from Training Notebook\n",
    "# # ============================================================================\n",
    "# # After running your training notebook, you should have these variables:\n",
    "# # Make sure you've run the training cells and have these variables available\n",
    "\n",
    "# # Check if variables exist (they should be from your training notebook)\n",
    "# try:\n",
    "#     # These should be available from your diagnostics notebook after training\n",
    "#     # If running in the same notebook session, they'll be available\n",
    "#     # If running separately, you'll need to load them first\n",
    "    \n",
    "#     artifacts = {\n",
    "#         'prod2id': prod2id,  # Product to ID mapping (from Cell 4)\n",
    "#         'id2prod': id2prod,  # ID to product mapping (from Cell 4)\n",
    "#         'label_map': label_map,  # Label mapping (from Cell 4)\n",
    "#         'num_classes': NUM_CLASSES,  # Number of classes (from Cell 4)\n",
    "#         'feature_cols': model_feature_cols,  # Feature columns list (from Cell 4)\n",
    "#         'max_seq_len': MAX_SEQ_LEN,  # Maximum sequence length (from Cell 4)\n",
    "#         'categorical_mappings': None  # Optional - can be None or a dict\n",
    "#     }\n",
    "    \n",
    "#     print(f\"  - prod2id: {len(artifacts['prod2id'])} products\")\n",
    "#     print(f\"  - num_classes: {artifacts['num_classes']}\")\n",
    "#     print(f\"  - feature_cols: {len(artifacts['feature_cols'])} features\")\n",
    "    \n",
    "# except NameError as e:\n",
    "#     print(f\"\\nâœ— Missing required variable: {e}\")\n",
    "#     print(\"\\nPlease ensure you've run the training cells from diagnostics notebook first.\")\n",
    "#     print(\"\\nRequired variables from diagnostics notebook:\")\n",
    "#     print(\"  - model: trained LightGBM model (from Cell 5)\")\n",
    "#     print(\"  - prod2id: product to ID mapping dictionary (from Cell 4)\")\n",
    "#     print(\"  - id2prod: ID to product mapping dictionary (from Cell 4)\")\n",
    "#     print(\"  - label_map: label mapping dictionary (from Cell 4)\")\n",
    "#     print(\"  - NUM_CLASSES: number of classes (from Cell 4)\")\n",
    "#     print(\"  - model_feature_cols: list of feature column names (from Cell 4)\")\n",
    "#     print(\"  - MAX_SEQ_LEN: maximum sequence length, usually 10 (from Cell 4)\")\n",
    "#     print(\"\\nIf running in a different notebook, you can also manually create artifacts:\")\n",
    "#     print(\"\"\"\n",
    "#     artifacts = {\n",
    "#         'prod2id': your_prod2id_dict,\n",
    "#         'id2prod': your_id2prod_dict,\n",
    "#         'label_map': your_label_map_dict,\n",
    "#         'num_classes': your_num_classes,\n",
    "#         'feature_cols': your_model_feature_cols_list,\n",
    "#         'max_seq_len': 10,\n",
    "#         'categorical_mappings': None  # Optional\n",
    "#     }\n",
    "#     \"\"\")\n",
    "#     raise\n",
    "\n",
    "# # ============================================================================\n",
    "# # STEP 2: Save Artifacts\n",
    "# # ============================================================================\n",
    "\n",
    "# # Create directory if it doesn't exist\n",
    "# os.makedirs(ARTIFACTS_PATH, exist_ok=True)\n",
    "# print(f\"\\nâœ“ Created directory: {ARTIFACTS_PATH}\")\n",
    "\n",
    "# # Save other artifacts (excluding model, which is saved separately)\n",
    "# artifacts_to_save = {\n",
    "#     'prod2id': artifacts['prod2id'],\n",
    "#     'id2prod': artifacts['id2prod'],\n",
    "#     'label_map': artifacts['label_map'],\n",
    "#     'num_classes': artifacts['num_classes'],\n",
    "#     'categorical_mappings': artifacts.get('categorical_mappings'),\n",
    "#     'feature_cols': artifacts['feature_cols'],\n",
    "#     'max_seq_len': artifacts['max_seq_len']\n",
    "# }\n",
    "\n",
    "# artifacts_file = os.path.join(ARTIFACTS_PATH, \"artifacts.pkl\")\n",
    "# with open(artifacts_file, 'wb') as f:\n",
    "#     pickle.dump(artifacts_to_save, f)\n",
    "# print(f\"âœ“ Saved artifacts to: {artifacts_file}\")\n",
    "\n",
    "# # Display what was saved\n",
    "# print(\"\\n\" + \"=\" * 80)\n",
    "# print(\"ARTIFACTS SAVED SUCCESSFULLY\")\n",
    "# print(\"=\" * 80)\n",
    "# print(f\"\\nSaved artifacts:\")\n",
    "# print(f\"  2. artifacts.pkl - Contains:\")\n",
    "# for key, value in artifacts_to_save.items():\n",
    "#     if value is not None:\n",
    "#         if isinstance(value, dict):\n",
    "#             print(f\"     - {key}: dict with {len(value)} items\")\n",
    "#         elif isinstance(value, list):\n",
    "#             print(f\"     - {key}: list with {len(value)} items\")\n",
    "#         else:\n",
    "#             print(f\"     - {key}: {type(value).__name__} = {value}\")\n",
    "\n",
    "# print(f\"\\nðŸ“ Artifacts location: {ARTIFACTS_PATH}\")\n",
    "# print(f\"\\nâœ… You can now proceed to register the model in the next cell!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone)final_diagnostics_copy4-transformer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
